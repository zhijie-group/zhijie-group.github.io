+++
title = "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding"
date = 2025-12-18
authors = ["Chenkai Xu*", "Yijie Jin*", "Jiajun Li", "Yi Tu", "Guoping Long", "Dandan Tu", "Tianqi Hou", "Junchi Yan", "Zhijie Deng †"]
author = "Chenkai Xu*, Yijie Jin*, Jiajun Li, Yi Tu, Guoping Long, Dandan Tu, Tianqi Hou, Junchi Yan, Zhijie Deng †"
ShowReadingTime = true
draft = false
summary = "We introduce the Lookahead Parallel Decoding (LoPA) algorithm for diffusion large language models (dLLMs) inference, enabling up to 10.1 tokens per forward pass."

[cover]
    image = "blogs/LoPA/image/figure4.png"
    alt = "LoPA Performance"
    caption = "TPF Scaling and Accuracy Performance on GSM8K and HumanEval+."
+++

[**Github: https://github.com/zhijie-group/LoPA**](https://github.com/zhijie-group/LoPA)

We introduce the Lookahead Parallel Decoding (LoPA) algorithm for diffusion large language models (dLLMs) inference. LoPA enables up to 10.1 tokens per forward pass (TPF) for state-of-the-art dLLMs—without compromising predictive performance. This represents an unprecedented degree of parallelism, a capability unmatched by previous dLLM decoding methods. Under multi-device deployment, our specialized system LoPA-Dist achieves a single-sample throughput of 1073.9 tokens per second.

<img src="image/figure1.png" alt="Illustration of diffusion LLM inference challenges" style="width:100%; margin: 20px auto;">
<div style="text-align: center; color: gray; font-size: 0.9em; margin-bottom: 20px;">Figure 1: Illustration of diffusion LLM inference challenges.</div>

## Background

DLLMs show significant potential for high-speed inference, yet current confidence-driven decoding strategies are constrained by limited parallelism—typically achieving only 1-3 TPF on math and coding tasks [2, 3]. Our investigation identifies a key insight: during dLLM inference, the degree of parallelism fluctuates sharply with the prediction confidence, which is heavily influenced by the Token Filling Order (TFO). Consequently, standard strategies that greedily prioritize currently high-confidence positions may lead to suboptimal trajectories. To address this, we propose Lookahead Parallel Decoding (LoPA), a training-free, plug-and-play algorithm designed to actively explore superior TFOs to unlock higher parallelism.

## Methodology

This section first explains the foundational Confidence-Driven Sampling used in regular dLLM inference [2, 3, 4] and then elaborates on LoPA.

<img src="image/figure2.png" alt="The architecture of standard confidence-driven sampling" style="width:100%; margin: 20px auto;">
<div style="text-align: center; color: gray; font-size: 0.9em; margin-bottom: 20px;">Figure 2: The architecture of standard confidence-driven sampling.</div>

### Preliminary: Confidence-Driven Sampling for dLLMs

Confidence-driven sampling is a prevalent paradigm for current dLLMs to boost parallelism, adopted in models such as Fast-dLLM [2], D2F [3], and SDAR [4]. Specifically, given a sequence $x_t$ with a set of masked positions $M_t$, the dLLM model $p_\theta$ outputs a predictive distribution $p_\theta(\cdot | x_t)$. A candidate sequence $\hat{x}_0 \sim p_\theta(\cdot | x_t)$ is sampled, and a confidence function, $\text{Conf}(\cdot)$, assigns a score to each position $i \in M_t$. The set of positions to fill, $I_{fill}$, is then determined as:

$$
I_{fill} = \begin{cases} \{i \in M_t \mid \text{Conf}(i) > \tau\} & \text{if } \{i \in M_t \mid \text{Conf}(i) > \tau\} \neq \emptyset \\ \{\arg\max_{i \in M_t} \text{Conf}(i)\} & \text{otherwise} \end{cases}
$$

The algorithm then accepts the predictions according to $I_{fill}$ and moves to the next iteration.

## LoPA

<img src="image/figure3.png" alt="Overview of LoPA" style="width:100%; margin: 20px auto;">
<div style="text-align: center; color: gray; font-size: 0.9em; margin-bottom: 20px;">Figure 3: Overview of the LoPA algorithm and TFO exploration.</div>

As shown in Figure 3, in every decoding iteration, LoPA looks ahead at multiple TFOs, yielding multiple sampling branches, and then identifies the branch with superior future parallel decoding potential.

### Look ahead Multiple TFOs in Parallel

LoPA operates by generating multiple branches. First, it constructs an Anchor Branch ($B_0$) using the standard confidence-driven strategy (filling positions in $I_{fill}$).

LoPA is designed to explore one step further than this anchor branch. To ensure effective and reliable exploration, we prioritize sampling tokens with higher confidence, a strategy that has been proved in Fast-dLLM [2] to yield more stable predictions. Specifically, in addition to $B_0$, we generate $k$ competitive Lookahead Branches. We identify the top-$k$ positions from the anchor branch's unfilled set $M_{B_0}$ that possess the highest confidence scores. For each identified position, we sample it independently to create a distinct branch. This results in a set of $k$ new branches $\{B_1, \dots, B_k\}$, each with its own sequence $x_{B_j}$ and unfilled set $M_{B_j}$.

### Branch Confidence-based Verification

Inspired by DeepConf [5], we design a branch confidence metric to guide the selection among candidate decoding paths. Formally, the confidence of a branch $B_j$ is defined as the average prediction confidence over its remaining unfilled positions $M_{B_j}$:

$$
C(B_j) = \frac{1}{|M_{B_j}|} \sum_{i \in M_{B_j}} \text{Conf}(i)
$$

A higher branch confidence indicates that more unfilled positions are likely to be accepted in the very next decoding step. This directly increases the number of tokens filled per iteration, thereby enhancing the overall parallelism. Beyond this mean confidence, branch confidence can also be quantified by other methods [5], such as applying a sliding window to assess local quality or averaging confidence over the least confident segment to identify weak links.

This verification mechanism offers distinct advantages. First, all candidate branches (Anchor + Lookahead) can be packed and verified within a single forward pass, with custom attention masks ensuring independent computation for each branch. Second, the logits computed during branch evaluation are directly reused in the next decoding step, eliminating the need for additional forward passes.

### Application: integration with D2F

LoPA integrates seamlessly with D2F [3], the first open-source diffusion language model whose inference throughput surpasses that of autoregressive (AR) models. Our application of LoPA to D2F incorporates two key enhancements:

* **Parallel Exploration in a Decoding Window:** We treat all active blocks in D2F's pipeline as a single window where LoPA's branch exploration and lookahead verification operate. Replacing the original block-level causal attention with a full attention mechanism within this window reduces implementation complexity and enhances computational performance.
* **System Integration and Performance:** On the D2F-Dream model, LoPA achieves a TPF of up to 10.1. To leverage this parallelism, we developed a specialized multi-device inference system where LoPA achieves a throughput of 1073.86 tokens per second.

## Results

### Scaling Analysis of Branch Count

We analyzed the impact of the competitive branch count (k) on TPF and quality using D2F models fine-tuned on Dream[6] and DiffuCoder[7]. Results show TPF consistently improves with k; however, excessive k introduces fluctuations, attributed to the model prioritizing future confidence over local optimality. These results point to an optimal trade-off, where a carefully chosen k can maximize TPF while preserving quality.

<img src="image/figure4.png" alt="TPF Scaling and Accuracy Performance" style="width:100%; margin: 20px auto;">
<div style="text-align: center; color: gray; font-size: 0.9em; margin-bottom: 20px;">Figure 4: TPF Scaling and Accuracy Performance on GSM8K and HumanEval+.</div>

As shown in the Figure 4, on GSM8K, LoPA scales the TPF of D2F-Dream to 10.1 while maintaining a score (73.8) superior to the Dream baseline (72.6). on HumanEval+, LoPA scales the TPF of D2F-DiffuCoder to 8.3 with marginal performance degradation, demonstrating a clear speed-accuracy trade-off.

Tables 1 and 2 below confirm this efficacy across multiple benchmarks.

<table>
    <caption>Table 1. Accuracy-preserving parallelism scaling of Dream on multiple benchmarks across multiple branches.</caption>
    <thead>
        <tr>
            <th rowspan="2">Model</th>
            <th rowspan="2">Decoding algo</th>
            <th colspan="2">MBPP 3-shot</th>
            <th colspan="2">Math 4-shot</th>
            <th colspan="2">HumanEval 0-shot</th>
            <th colspan="2">GSM8K 4-shot</th>
        </tr>
        <tr>
            <th>TPF</th>
            <th>Score</th>
            <th>TPF</th>
            <th>Score</th>
            <th>TPF</th>
            <th>Score</th>
            <th>TPF</th>
            <th>Score</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Dream</td>
            <td>vanilla</td>
            <td>1</td>
            <td>56.2</td>
            <td>1</td>
            <td>33.7</td>
            <td>1</td>
            <td>55.5</td>
            <td>1</td>
            <td>72.6</td>
        </tr>
        <tr>
            <td>Dream</td>
            <td>Fast-dLLM</td>
            <td>1.9</td>
            <td>55.6</td>
            <td>1.9</td>
            <td>37.6</td>
            <td>1.8</td>
            <td>55.5</td>
            <td>2.1</td>
            <td>72.6</td>
        </tr>
        <tr>
            <td>Dream</td>
            <td>LoPA</td>
            <td>3.3</td>
            <td>54.8</td>
            <td>3.4</td>
            <td>37.0</td>
            <td>2.9</td>
            <td>53</td>
            <td>3.1</td>
            <td>73.3</td>
        </tr>
        <tr>
            <td>D2F-Dream</td>
            <td>vanilla</td>
            <td>2.3</td>
            <td>53.8</td>
            <td>2.6</td>
            <td>36.8</td>
            <td>2.5</td>
            <td>56.1</td>
            <td>3.1</td>
            <td>78.5</td>
        </tr>
        <tr>
            <td>D2F-Dream</td>
            <td>LoPA</td>
            <td><b>5.4</b></td>
            <td><b>56.0</b></td>
            <td><b>8.0</b></td>
            <td><b>35.2</b></td>
            <td><b>6.3</b></td>
            <td><b>56.1</b></td>
            <td><b>10.1</b></td>
            <td><b>73.8</b></td>
        </tr>
    </tbody>
</table>

<table>
    <caption>Table 2. Accuracy-preserving parallelism scaling of DiffuCoder on MBPP+ and HumanEval+ benchmarks.</caption>
    <thead>
        <tr>
            <th rowspan="2">Model</th>
            <th rowspan="2">Decoding algo</th>
            <th colspan="2">MBPP++ 0-shot</th>
            <th colspan="2">HumanEval++ 0-shot</th>
        </tr>
        <tr>
            <th>TPF</th>
            <th>Score</th>
            <th>TPF</th>
            <th>Score</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>DiffuCoder</td>
            <td>vanilla</td>
            <td>1</td>
            <td>61.9</td>
            <td>1</td>
            <td>65.2</td>
        </tr>
        <tr>
            <td>D2F-Diffucoder</td>
            <td>vanilla</td>
            <td>2.2</td>
            <td>61.9</td>
            <td>2.2</td>
            <td>65.9</td>
        </tr>
        <tr>
            <td>D2F-Diffucoder</td>
            <td>LoPA</td>
            <td><b>6.7</b></td>
            <td>61.6</td>
            <td><b>8.3</b></td>
            <td>64.0</td>
        </tr>
    </tbody>
</table>

<img src="image/figure5.png" alt="Additional Results" style="width:100%; margin: 20px auto;">
<div style="text-align: center; color: gray; font-size: 0.9em; margin-bottom: 20px;">Figure 5: Additional experimental results and analysis.</div>

## System Throughput and Scalability

To fully exploit LoPA’s parallelism, we designed LoPA-Dist, a distributed inference system utilizing Branch Parallelism (BP).

The system distributes candidate branches across multiple GPUs for concurrent processing. We provide two specialized implementations:

* **LoPA-Dist-NV (CUDA):** Optimized for low latency using static KV cache and a two-phase update protocol (Pre-Write and Commit-Winner-Cache) to ensure consistency.
* **LoPA-Dist-Ascend (Ascend 910C):** Optimized for high throughput using hybrid parallelism and graph compilation to fuse element-wise operations.

As shown in Table 3, this design achieves near-linear scalability. On the Ascend platform, LoPA-Dist achieves a peak throughput of 1073.86 tokens/s.

<table>
    <caption>Table 3. System performance of D2F-Dream under guaranteed inference speed.</caption>
    <thead>
        <tr>
            <th rowspan="2">Model</th>
            <th rowspan="2">Platform</th>
            <th colspan="5">MBPP</th>
            <th colspan="5">GSM8K</th>
        </tr>
        <tr>
            <th>Avg TPS</th>
            <th>Max TPS</th>
            <th>TPF</th>
            <th>Latency</th>
            <th>PU</th>
            <th>Avg TPS</th>
            <th>Max TPS</th>
            <th>TPF</th>
            <th>Latency</th>
            <th>PU</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="2">D2F-Dream-Base</td>
            <td>LoPA-Dist-NV</td>
            <td>630.28</td>
            <td>1472.37</td>
            <td>15.69</td>
            <td>0.84</td>
            <td>0.69</td>
            <td>566.97</td>
            <td>1305.86</td>
            <td>13.31</td>
            <td>0.93</td>
            <td>0.74</td>
        </tr>
        <tr>
            <td>LoPA-Dist-Ascend</td>
            <td>1073.86</td>
            <td>2400.12</td>
            <td>11.92</td>
            <td>0.78</td>
            <td>0.72</td>
            <td>856.46</td>
            <td>2751.61</td>
            <td>9.34</td>
            <td>0.75</td>
            <td>0.76</td>
        </tr>
        <tr>
            <td rowspan="2">D2F-Dream-Instruct</td>
            <td>LoPA-Dist-NV</td>
            <td>543.32</td>
            <td>1531.64</td>
            <td>9.45</td>
            <td>0.16</td>
            <td>0.87</td>
            <td>536.71</td>
            <td>1141.71</td>
            <td>11.41</td>
            <td>0.29</td>
            <td>0.76</td>
        </tr>
        <tr>
            <td>LoPA-Dist-Ascend</td>
            <td>896.21</td>
            <td>2586.73</td>
            <td>8.64</td>
            <td>0.11</td>
            <td>0.74</td>
            <td>897.10</td>
            <td>1868.16</td>
            <td>9.30</td>
            <td>0.21</td>
            <td>0.76</td>
        </tr>
    </tbody>
</table>

## Future Works

We will explore adapting LoPA to SDAR and other confidence-driven diffusion language models to further demonstrate its generalizability and effectiveness across diverse model architectures.

## Reference

[1] Nie, Shen, et al. "Large language diffusion models." arXiv preprint arXiv:2502.09992 (2025).

[2] Wu, Chengyue, et al. "Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding." arXiv preprint arXiv:2505.22618 (2025).

[3] Wang, Xu, et al. "Diffusion llms can do faster-than-ar inference via discrete diffusion forcing." arXiv preprint arXiv:2508.09192 (2025).

[4] Cheng, Shuang, et al. "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation." arXiv preprint arXiv:2510.06303 (2025).

[5] Fu, Yichao, et al. "Deep think with confidence." arXiv preprint arXiv:2508.15260 (2025).

[6] Ye, Jiacheng, et al. "Dream 7b: Diffusion large language models." arXiv preprint arXiv:2508.15487 (2025).

[7] Gong, Shansan, et al. "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation." arXiv preprint arXiv:2506.20639 (2025).