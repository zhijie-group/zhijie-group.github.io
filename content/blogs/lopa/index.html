+++
title = "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding"
date = 2025-12-18
authors = ["Chenkai Xu*", "Yijie Jin*", "Jiajun Li", "Yi Tu", "Guoping Long", "Dandan Tu", "Mingcong Song", "Hongjie Si", "Tianqi Hou", "Junchi Yan", "Zhijie Deng †"]
author = "Chenkai Xu*, Yijie Jin*, Jiajun Li, Yi Tu, Guoping Long, Dandan Tu, Mingcong Song, Hongjie Si, Tianqi Hou, Junchi Yan, Zhijie Deng †"
ShowReadingTime = true
draft = false
summary = "We introduce the Lookahead Parallel Decoding (LoPA) algorithm for diffusion large language models (dLLMs) inference, enabling up to 10.1 tokens per forward pass."

[cover]
image = "blogs/lopa/image/figure3.png"
alt = "LoPA Performance"
caption = "Overview of Lookahead Parallel Decoding (LoPA)."
+++

<p><strong>Github:</strong> <a href="https://github.com/zhijie-group/LoPA">https://github.com/zhijie-group/LoPA</a></p>

<p>We introduce the Lookahead Parallel Decoding (LoPA) algorithm for diffusion large language models (dLLMs) inference. LoPA enables up to 10.1 tokens per forward pass (TPF) for state-of-the-art dLLMs—without compromising predictive performance. This represents an unprecedented degree of parallelism, a capability unmatched by previous dLLM decoding methods. Under multi-device deployment, our specialized system LoPA-Dist achieves a single-sample throughput of 1073.9 tokens per second.</p>

<div style="text-align: center; margin: 30px 0;">
<img src="image/figure1.png" alt="Illustration of diffusion LLM inference challenges" style="display: block; margin: 0 auto; max-width: 85%; height: auto; border: 1px solid #eee; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.05);">
<div style="color: gray; font-size: 0.9em; margin-top: 10px; padding: 0 10%;">Figure 1. Throughput performance of LoPA under guaranteed inference speed. LoPA accelerates the single-sample throughput for D2F-Dream to up to 1073.9 and 774.1 tokens/s on MBPP and GSM8K respectively, significantly outperforming baselines.</div>
</div>

<h2>Background</h2>

<p>DLLMs show significant potential for high-speed inference, yet current confidence-driven decoding strategies are constrained by limited parallelism—typically achieving only 1-3 TPF on math and coding tasks [2, 3]. Our investigation identifies a key insight: during dLLM inference, the degree of parallelism fluctuates sharply with the prediction confidence, which is heavily influenced by the Token Filling Order (TFO). Consequently, standard strategies that greedily prioritize currently high-confidence positions may lead to suboptimal trajectories. To address this, we propose Lookahead Parallel Decoding (LoPA), a training-free, plug-and-play algorithm designed to actively explore superior TFOs to unlock higher parallelism.</p>

<h2>Methodology</h2>

<p>This section first explains the foundational Confidence-Driven Sampling used in regular dLLM inference [2, 3, 4] and then elaborates on LoPA.</p>

<div style="text-align: center; margin: 30px 0;">
<img src="image/figure2.png" alt="The architecture of standard confidence-driven sampling" style="display: block; margin: 0 auto; max-width: 85%; height: auto; border: 1px solid #eee; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.05);">
<div style="color: gray; font-size: 0.9em; margin-top: 10px; padding: 0 10%;">Figure 2. Scaling analysis of LoPA on D2F-Dream with varying branch counts. The results illustrate that LoPA effectively scales the TPF of D2F to a peak exceeding 10, thereby significantly reducing the total number of decoding steps.</div>
</div>

<h3>Preliminary: Confidence-Driven Sampling for dLLMs</h3>

<p>Confidence-driven sampling is a prevalent paradigm for current dLLMs to boost parallelism, adopted in models such as Fast-dLLM [2], D2F [3], and SDAR [4]. Specifically, given a sequence $x_t$ with a set of masked positions $M_t$, the dLLM model $p_{\theta}$ outputs a predictive distribution $p_{\theta}(\cdot \mid x_t)$. A candidate sequence $\hat{x}_0 \sim p_{\theta}(\cdot \mid x_t)$ is sampled, and a confidence function, $\text{Conf}(\cdot)$, assigns a score to each position $i \in M_t$. The set of positions to fill, $I_{fill}$, is then determined as:</p>

<p>
$$I_{fill} = \begin{cases} \{i \in M_t \mid \text{Conf}(i) > \tau\} & \text{if } \{i \in M_t \mid \text{Conf}(i) > \tau\} \neq \emptyset \\ \{\arg\max_{i \in M_t} \text{Conf}(i)\} & \text{otherwise} \end{cases}$$
</p>

<p>The algorithm then accepts the predictions according to $I_{fill}$ and moves to the next iteration.</p>

<h2>LoPA</h2>

<div style="text-align: center; margin: 30px 0;">
<img src="image/figure3.png" alt="Overview of LoPA" style="display: block; margin: 0 auto; max-width: 85%; height: auto; border: 1px solid #eee; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.05);">
<div style="color: gray; font-size: 0.9em; margin-top: 10px; padding: 0 10%;">Figure 3. Overview of Lookahead Parallel Decoding (LoPA). In each iteration, LoPA generates a anchor branch alongside multiple lookahead branches (e.g., B1, . . . , Bk ) by independently sampling high-confidence positions from the baseline’s unfilled set. A branch confidence verification mechanism then evaluates all branches in parallel within a single forward pass, selecting the optimal path to maximize future parallelism.</div>
</div>

<p>As shown in Figure 3, in every decoding iteration, LoPA looks ahead at multiple TFOs, yielding multiple sampling branches, and then identifies the branch with superior future parallel decoding potential.</p>

<h3>Look ahead Multiple TFOs in Parallel</h3>

<p>LoPA operates by generating multiple branches. First, it constructs an Anchor Branch ($B_0$) using the standard confidence-driven strategy (filling positions in $I_{fill}$).</p>

<p>LoPA is designed to explore one step further than this anchor branch. To ensure effective and reliable exploration, we prioritize sampling tokens with higher confidence, a strategy that has been proved in Fast-dLLM [2] to yield more stable predictions. Specifically, in addition to $B_0$, we generate $k$ competitive Lookahead Branches. We identify the top-$k$ positions from the anchor branch's unfilled set $M_{B_0}$ that possess the highest confidence scores. For each identified position, we sample it independently to create a distinct branch. This results in a set of $k$ new branches $\{B_1, \dots, B_k\}$, each with its own sequence $x_{B_j}$ and unfilled set $M_{B_j}$.</p>

<h3>Branch Confidence-based Verification</h3>

<p>Inspired by DeepConf [5], we design a branch confidence metric to guide the selection among candidate decoding paths. Formally, the confidence of a branch $B_j$ is defined as the average prediction confidence over its remaining unfilled positions $M_{B_j}$:</p>

<p>
$$C(B_j) = \frac{1}{|M_{B_j}|} \sum_{i \in M_{B_j}} \text{Conf}(i)$$
</p>

<p>A higher branch confidence indicates that more unfilled positions are likely to be accepted in the very next decoding step. This directly increases the number of tokens filled per iteration, thereby enhancing the overall parallelism. Beyond this mean confidence, branch confidence can also be quantified by other methods [5], such as applying a sliding window to assess local quality or averaging confidence over the least confident segment to identify weak links.</p>

<p>This verification mechanism offers distinct advantages. First, all candidate branches (Anchor + Lookahead) can be packed and verified within a single forward pass, with custom attention masks ensuring independent computation for each branch. Second, the logits computed during branch evaluation are directly reused in the next decoding step, eliminating the need for additional forward passes.</p>

<h3>Application: integration with D2F</h3>

<p>LoPA integrates seamlessly with D2F [3], the first open-source diffusion language model whose inference throughput surpasses that of autoregressive (AR) models. Our application of LoPA to D2F incorporates two key enhancements:</p>

<ul>
<li><strong>Parallel Exploration in a Decoding Window:</strong> We treat all active blocks in D2F's pipeline as a single window where LoPA's branch exploration and lookahead verification operate. Replacing the original block-level causal attention with a full attention mechanism within this window reduces implementation complexity and enhances computational performance.</li>
<li><strong>System Integration and Performance:</strong> On the D2F-Dream model, LoPA achieves a TPF of up to 10.1. To leverage this parallelism, we developed a specialized multi-device inference system where LoPA achieves a throughput of 1073.86 tokens per second.</li>
</ul>

<h2>Results</h2>

<h3>Scaling Analysis of Branch Count</h3>
<p>We analyzed the impact of the competitive branch count (k) on TPF and quality using D2F models fine-tuned on Dream[6] and DiffuCoder[7]. Results show TPF consistently improves with k; however, excessive k introduces fluctuations, attributed to the model prioritizing future confidence over local optimality. These results point to an optimal trade-off, where a carefully chosen k can maximize TPF while preserving quality.</p>

<div style="text-align: center; margin: 30px 0;">
<img src="image/figure4.png" alt="TPF Scaling and Accuracy Performance" style="display: block; margin: 0 auto; max-width: 85%; height: auto; border: 1px solid #eee; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.05);">
<div style="color: gray; font-size: 0.9em; margin-top: 10px; padding: 0 10%;">Figure 4. Scaling Curves of LoPA. LoPA scales the TPF for D2F-Dream and D2F-DiffuCoder to up to 10.1 and 8.3 on GSM8k and HumanEval+ respectively, with comparable performance.</div>
</div>

<p>As shown in the Figure 4, on GSM8K, LoPA scales the TPF of D2F-Dream to 10.1 while maintaining a score (73.8) superior to the Dream baseline (72.6). on HumanEval+, LoPA scales the TPF of D2F-DiffuCoder to 8.3 with marginal performance degradation, demonstrating a clear speed-accuracy trade-off.</p>

<p>Tables 1 and 2 below confirm this efficacy across multiple benchmarks.</p>

<div style="text-align: center; overflow-x: auto; margin: 20px 0;">
<table style="margin: 0 auto; text-align: center; border-collapse: collapse; display: inline-table; width: auto !important;">
<caption>Table 1. Accuracy-preserving parallelism scaling of Dream on multiple benchmarks across multiple branches. TPF denotes Tokens Per Forward pass. LoPA significantly scales the TPF of D2F-Dream while maintaining or exceeding baseline scores.</caption>
<thead>
<tr style="border-bottom: 2px solid #ddd;">
<th rowspan="2" style="padding: 10px;">Model</th>
<th rowspan="2" style="padding: 10px;">Decoding algo</th>
<th colspan="2" style="padding: 10px;">MBPP 3-shot</th>
<th colspan="2" style="padding: 10px;">Math 4-shot</th>
<th colspan="2" style="padding: 10px;">HumanEval 0-shot</th>
<th colspan="2" style="padding: 10px;">GSM8K 4-shot</th>
</tr>
<tr style="border-bottom: 2px solid #ddd;">
<th style="padding: 10px;">TPF</th>
<th style="padding: 10px;">Score</th>
<th style="padding: 10px;">TPF</th>
<th style="padding: 10px;">Score</th>
<th style="padding: 10px;">TPF</th>
<th style="padding: 10px;">Score</th>
<th style="padding: 10px;">TPF</th>
<th style="padding: 10px;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="padding: 8px;">Dream</td>
<td style="padding: 8px;">vanilla</td>
<td style="padding: 8px;">1</td>
<td style="padding: 8px;"><b>56.2</b></td>
<td style="padding: 8px;">1</td>
<td style="padding: 8px;">33.7</td>
<td style="padding: 8px;">1</td>
<td style="padding: 8px;">55.5</td>
<td style="padding: 8px;">1</td>
<td style="padding: 8px;">72.6</td>
</tr>
<tr>
<td style="padding: 8px;">Dream</td>
<td style="padding: 8px;">Fast-dLLM</td>
<td style="padding: 8px;">1.9</td>
<td style="padding: 8px;">55.6</td>
<td style="padding: 8px;">1.9</td>
<td style="padding: 8px;"><b>37.6</b></td>
<td style="padding: 8px;">1.8</td>
<td style="padding: 8px;">55.5</td>
<td style="padding: 8px;">2.1</td>
<td style="padding: 8px;">72.6</td>
</tr>
<tr>
<td style="padding: 8px;">Dream</td>
<td style="padding: 8px;">LoPA</td>
<td style="padding: 8px;">3.3</td>
<td style="padding: 8px;">54.8</td>
<td style="padding: 8px;">3.4</td>
<td style="padding: 8px;">37.0</td>
<td style="padding: 8px;">2.9</td>
<td style="padding: 8px;">53</td>
<td style="padding: 8px;">3.1</td>
<td style="padding: 8px;">73.3</td>
</tr>
<tr>
<td style="padding: 8px;">D2F-Dream</td>
<td style="padding: 8px;">vanilla</td>
<td style="padding: 8px;">2.3</td>
<td style="padding: 8px;">53.8</td>
<td style="padding: 8px;">2.6</td>
<td style="padding: 8px;">36.8</td>
<td style="padding: 8px;">2.5</td>
<td style="padding: 8px;"><b>56.1</b></td>
<td style="padding: 8px;">3.1</td>
<td style="padding: 8px;"><b>78.5</b></td>
</tr>
<tr style="border-bottom: 1px solid #ddd;">
<td style="padding: 8px;">D2F-Dream</td>
<td style="padding: 8px;">LoPA</td>
<td style="padding: 8px;"><b>5.4</b></td>
<td style="padding: 8px;">56.0</td>
<td style="padding: 8px;"><b>8.0</b></td>
<td style="padding: 8px;">35.2</td>
<td style="padding: 8px;"><b>6.3</b></td>
<td style="padding: 8px;"><b>56.1</b></td>
<td style="padding: 8px;"><b>10.1</b></td>
<td style="padding: 8px;">73.8</td>
</tr>
</tbody>
</table>
</div>

<div style="text-align: center; overflow-x: auto; margin: 20px 0;">
<table style="margin: 0 auto; text-align: center; border-collapse: collapse; display: inline-table; width: auto !important;">
<caption>Table 2. Accuracy-preserving parallelism scaling of DiffuCoder on MBPP+ and HumanEval+ benchmarks. LoPA boosts TPF by nearly 4× compared to the vanilla D2F baseline with minimal impact on generation quality.</caption>
<thead>
<tr style="border-bottom: 2px solid #ddd;">
<th rowspan="2" style="padding: 10px;">Model</th>
<th rowspan="2" style="padding: 10px;">Decoding algo</th>
<th colspan="2" style="padding: 10px;">MBPP++ 0-shot</th>
<th colspan="2" style="padding: 10px;">HumanEval++ 0-shot</th>
</tr>
<tr style="border-bottom: 2px solid #ddd;">
<th style="padding: 10px;">TPF</th>
<th style="padding: 10px;">Score</th>
<th style="padding: 10px;">TPF</th>
<th style="padding: 10px;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="padding: 8px;">DiffuCoder</td>
<td style="padding: 8px;">vanilla</td>
<td style="padding: 8px;">1</td>
<td style="padding: 8px;"><b>61.9</b></td>
<td style="padding: 8px;">1</td>
<td style="padding: 8px;">65.2</td>
</tr>
<tr>
<td style="padding: 8px;">D2F-Diffucoder</td>
<td style="padding: 8px;">vanilla</td>
<td style="padding: 8px;">2.2</td>
<td style="padding: 8px;"><b>61.9</b></td>
<td style="padding: 8px;">2.2</td>
<td style="padding: 8px;"><b>65.9</b></td>
</tr>
<tr style="border-bottom: 1px solid #ddd;">
<td style="padding: 8px;">D2F-Diffucoder</td>
<td style="padding: 8px;">LoPA</td>
<td style="padding: 8px;"><b>6.7</b></td>
<td style="padding: 8px;">61.6</td>
<td style="padding: 8px;"><b>8.3</b></td>
<td style="padding: 8px;">64.0</td>
</tr>
</tbody>
</table>
</div>

<div style="text-align: center; margin: 30px 0;">
<img src="image/figure5.png" alt="Additional Results" style="display: block; margin: 0 auto; max-width: 85%; height: auto; border: 1px solid #eee; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.05);">
<div style="color: gray; font-size: 0.9em; margin-top: 10px; padding: 0 10%;">Figure 5. Overview of LoPA Branch Parallel Distributed Inference System Design. A key distinction lies in the KV cache management protocol tailored for different backends: LoPA-Dist-NV utilizes a robust two-phase update mechanism to ensure consistency, whereas LoPA-Dist-Ascend adopts a streamlined single-phase update strategy for optimized serving efficiency.</div>
</div>

<h2>System Throughput and Scalability</h2>

<p>To fully exploit LoPA’s parallelism, we designed LoPA-Dist, a distributed inference system utilizing Branch Parallelism (BP).</p>

<p>The system distributes candidate branches across multiple GPUs for concurrent processing. We provide two specialized implementations:</p>
<ul>
<li>LoPA-Dist-NV (CUDA): Optimized for low latency using static KV cache and a two-phase update protocol (Pre-Write and Commit-Winner-Cache) to ensure consistency.</li>
<li>LoPA-Dist-Ascend (Ascend 910C): Optimized for high throughput using hybrid parallelism and graph compilation to fuse element-wise operations.</li>
</ul>

<p>As shown in Table 3, this design achieves near-linear scalability. On the Ascend platform, LoPA-Dist achieves a peak throughput of 1073.86 tokens/s.</p>

<div style="text-align: center; overflow-x: auto; margin: 20px 0;">
<table style="margin: 0 auto; text-align: center; border-collapse: collapse; display: inline-table; width: auto !important;">
<caption>Table 3. System performance of D2F-Dream under guaranteed inference speed. The results demonstrate that our system efficiently translates algorithmic parallelism (high TPF) into significant wall-clock acceleration, achieving high Parallelism Utilization (PU) and average throughputs exceeding 1000 tokens/s on the specialized LoPA-Dist-Ascend engine.</caption>
<thead>
<tr style="border-bottom: 2px solid #ddd;">
<th rowspan="2" style="padding: 10px;">Model</th>
<th rowspan="2" style="padding: 10px;">Platform</th>
<th colspan="4" style="padding: 10px;">MBPP</th>
<th colspan="4" style="padding: 10px;">GSM8K</th>
</tr>
<tr style="border-bottom: 2px solid #ddd;">
<th style="padding: 10px;">Avg TPS</th>
<th style="padding: 10px;">Max TPS</th>
<th style="padding: 10px;">TPF</th>
<th style="padding: 10px;">Latency</th>
<th style="padding: 10px;">Avg TPS</th>
<th style="padding: 10px;">Max TPS</th>
<th style="padding: 10px;">TPF</th>
<th style="padding: 10px;">Latency</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="2" style="padding: 8px;">D2F-Dream-Base</td>
<td style="padding: 8px;">LoPA-Dist-NV</td>
<td style="padding: 8px;">708.48</td>
<td style="padding: 8px;">1470.95</td>
<td style="padding: 8px;"><b>15.55</b></td>
<td style="padding: 8px;">0.74</td>
<td style="padding: 8px;">619.33</td>
<td style="padding: 8px;">1299.25</td>
<td style="padding: 8px;"><b>13.16</b></td>
<td style="padding: 8px;">0.85</td>
</tr>
<tr>
<td style="padding: 8px;">LoPA-Dist-Ascend</td>
<td style="padding: 8px;"><b>1073.86</b></td>
<td style="padding: 8px;"><b>2400.12</b></td>
<td style="padding: 8px;">11.92</td>
<td style="padding: 8px;"><b>0.78</b></td>
<td style="padding: 8px;"><b>856.46</b></td>
<td style="padding: 8px;"><b>2751.61</b></td>
<td style="padding: 8px;">9.34</td>
<td style="padding: 8px;"><b>0.75</b></td>
</tr>
<tr>
<td rowspan="2" style="padding: 8px;">D2F-Dream-Instruct</td>
<td style="padding: 8px;">LoPA-Dist-NV</td>
<td style="padding: 8px;">636.55</td>
<td style="padding: 8px;">1811.71</td>
<td style="padding: 8px;"><b>9.52</b></td>
<td style="padding: 8px;">0.14</td>
<td style="padding: 8px;">609.90</td>
<td style="padding: 8px;">1407.56</td>
<td style="padding: 8px;"><b>11.42</b></td>
<td style="padding: 8px;">0.26</td>
</tr>
<tr style="border-bottom: 1px solid #ddd;">
<td style="padding: 8px;">LoPA-Dist-Ascend</td>
<td style="padding: 8px;"><b>896.21</b></td>
<td style="padding: 8px;"><b>2586.73</b></td>
<td style="padding: 8px;">8.64</td>
<td style="padding: 8px;"><b>0.11</b></td>
<td style="padding: 8px;"><b>897.10</b></td>
<td style="padding: 8px;"><b>1868.16</b></td>
<td style="padding: 8px;">9.30</td>
<td style="padding: 8px;"><b>0.21</b></td>
</tr>
</tbody>
</table>
</div>

<h2>Future Works</h2>

<p>We are working on a new inference framework for dLLMs named Diffulex, which is flexible and easy to extend. Diffulex supports multiple decoding strategies including D2F, BlockDiffusion, and Fast-dLLM-v2, which is soon to be released. <strong>You can find the code <a href="https://github.com/zhijie-group/Diffulex">here</a>.</strong></p>

<p>We will explore adapting LoPA to SDAR and other confidence-driven diffusion language models to further demonstrate its generalizability and effectiveness across diverse model architectures.</p>

<h2>Reference</h2>
<p>[1] Nie, Shen, et al. "Large language diffusion models." arXiv preprint arXiv:2502.09992 (2025).</p>
<p>[2] Wu, Chengyue, et al. "Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding." arXiv preprint arXiv:2505.22618 (2025).</p>
<p>[3] Wang, Xu, et al. "Diffusion llms can do faster-than-ar inference via discrete diffusion forcing." arXiv preprint arXiv:2508.09192 (2025).</p>
<p>[4] Cheng, Shuang, et al. "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation." arXiv preprint arXiv:2510.06303 (2025).</p>
<p>[5] Fu, Yichao, et al. "Deep think with confidence." arXiv preprint arXiv:2508.15260 (2025).</p>
<p>[6] Ye, Jiacheng, et al. "Dream 7b: Diffusion large language models." arXiv preprint arXiv:2508.15487 (2025).</p>
<p>[7] Gong, Shansan, et al. "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation." arXiv preprint arXiv:2506.20639 (2025).</p>

<h3>BibTeX</h3>
<pre><code>@misc{xu2025lopascalingdllminference,
      title={LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding}, 
      author={Chenkai Xu and Yijie Jin and Jiajun Li and Yi Tu and Guoping Long and Dandan Tu and Tianqi Hou and Junchi Yan and Zhijie Deng},
      year={2025},
      eprint={2512.16229},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2512.16229}, 
}</code></pre>