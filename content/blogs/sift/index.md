+++
title = "üò≥7%+ accuracy leap on AIME24/25 for DeepSeek R1 via simply being more careful with the questions?! Attention should be shifted from reasoning capacities to reasoning fidelity!"
date = 2025-02-21T18:00:00-08:00
authors = ["Zihao Zeng", "Xuyao Huang", "Boxiu Li", "Zhijie Deng"]
author = "Zihao Zeng, Xuyao Huang, Boxiu Li, Zhijie Deng"
ShowReadingTime = true
draft = false
[socialIcons]
    [[socialIcons.icon]]
      name = "twitter"
      url = "https://x.com/sjtudenglab"
    [[socialIcons.icon]]
      name = "github"
      url = "https://github.com/zhijie-group/SIFT"
[cover]
      image = "fig6.png"
      alt = "Four core operations in SIFT"
      caption = "Four core operations in SIFT: (i) Sticker Generation (SG), (ii) Consensus Prediction (CP), (iii) Forward Optimization (FO), (iv) Inverse Generation (IG)."
+++

{{< socialBadges arxiv-index="xxxx.xxxxx" github="zhijie-group/SIFT" >}}

## Introduction

{{< justify >}}
Techniques including Chain-of-Thought (CoT) Prompting[^1][^2] and Self-Consistency[^3], as well as reasoning-enhanced models, e.g., OpenAI-o1[^4], DeepSeek-R1[^5], and KIMI-k1.5[^6], have all contributed to improvements in multi-step reasoning for solving hard problems.

Discussions in the community suggest that advanced reasoning capabilities in LLMs mainly stem from two factors:
1. foundational knowledge acquisition through massive pretraining on diverse data
2. strategic refinement via post-training interventions like supervised fine-tuning (SFT) or reinforcement learning (RL)

In recent weeks, much attention has been focused on reproducing R1 and improving reasoning abilities. However, through systematic exploration, we have found that various models ranging from the small Llama3.2-3B-Instruct to state-of-the-art ones like DeepSeek-R1 often suffer from the *misinterpretation of the original problem* during reasoning, leading to incorrect results.
{{< /justify >}}

{{< image src="fig1.png" alt="Factual Drift" width="60%" title="Qwen2.5 misinterprets the sentence 'Then Carla has to restart from the beginning' in the question. DeepSeek-R1 misinterprets by assuming the distribution of cubic residues, which is not mentioned in the question.">}}

{{< justify >}}
Therefore, many times, the failure to achieve correct reasoning results is not due to insufficient reasoning capability, but rather because the model is not reasoning on the correct problem. We refer to this type of failure as "**Factual Drift**."
It occurs when LLMs misinterpret, overlook, or hallucinate key contextual information during reasoning, thus leading to errors despite logically sound steps. For instance, in the phrase "_10 dollars per kilo_," models might misinterpret "_per_" as "_total_" instead of "for each," resulting in incorrect calculations.
{{< /justify >}}

{{< image src="fig2.png" alt="Error Distribution" width="60%" title="We conduct a study on Qwen2.5-7B-Instruct. We use Gsm8k dataset and analyze the routs for the reasoning errors by GLM-4-Plus.">}}

{{< justify >}}
Moreover, we observed a common phenomenon occurs during DeepSeek-R1's reasoning--**Self-Verification**, where the model revisiting the original problem, focusing on key information, and paraphrasing it.
{{< /justify >}}

{{< image src="fig3.png" alt="Self-Verification" width="60%">}}

{{< justify >}}
Inspired by that humans usually use sticky notes to externalize critical elements when handling complex tasks, we introduce **SIFT (<u>S</u>t<u>i</u>ck to the <u>F</u>ac<u>t</u>s)**, a post-training framework that explicitly grounds LLM reasoning in contextual facts using dynamically generated summaries called **Sticker**s.
{{< /justify >}}

{{< image src="fig4.png" alt="Sticky Note" width="48%" title="A sticky note. (Generated by Doubao)">}}
{{< image src="fig5.png" alt="Sticker" width="48%" title="An example of a query and its Sticker.">}}


## The SIFT Framework

{{< justify >}}
SIFT tackles factual drift through four core operations:  
{{< /justify >}}

{{< image src="fig6.png" alt="4 Operators" width="90%">}}

{{< justify >}}
1. Sticker Generation (SG): The model extracts key facts (e.g., conditions, core questions) from the query and makes a structured Sticker.                    
2. Consensus Prediction (CP): Predictions are generated from both the Sticker alone and the original query augmented with the Sticker. If they disagree, the Sticker will be refined.  
3. Forward Optimization (FO): The Sticker are adjusted to better align with the query‚Äôs semantics.  
4. Inverse Generation (IG): A new Sticker from the model‚Äôs prediction is inferred  to match its reasoning preferences.

And we can divide the SIFT approach into 3 stages:
1. Stage 1: Only SG and CP are used.
2. Stage 2: Building upon Stage 1, FO is used to optimize the Sticker.
3. Stage 3: The complete process outlined in Algorithm 1.
{{< /justify >}}

{{< image src="fig7.png" alt="Algorithms" width="50%">}}

{{< justify >}}
By iteratively refining the Sticker, SIFT ensures the model grounds its reasoning in accurate context.  
{{< /justify >}}


## Key Results

{{< justify >}}
Experiments across models (3B to 100B+ parameters) and benchmarks (GSM8K, MATH-500, AIME2024) demonstrate SIFT‚Äôs effectiveness:  
- DeepSeek-R1 achieves a high accuracy of **85.67%** on AIME2024 (vs. 78.33% baseline), setting a new SOTA in open-source models.  
- Llama3.2-3B-Instruct improves its accuracy by **8.80%** on MATH-500.  
- SIFT shows consistent gains across architectures (dense/MoE) and scales,  which proves its versatility.
{{< /justify >}}

{{< image src="fig8.png" alt="DeepSeek" width="90%">}}

{{< image src="fig9.png" alt="small" width="90%">}}

{{< justify >}}
More experimental details and results can be found in our paper (including iterative optimization, sample augmentation, and some ablation studies).
{{< /justify >}}


## Why Does SIFT Work?

{{< justify >}}
1. **Explicit Fact Highlights**ÔºöStickers externalize critical information, mimicking human sticky-note practices.  
2. **Bidirectional Alignment**: FO anchors Stickers to source semantics, while IG aligns them with the model‚Äôs reasoning style.  
3. **Consensus Validation**: It refers that disagreements between Sticker-only and augmented predictions can trigger refinement so as to ensure factual fidelity.  
4. **Read better to reason better**: SIFT helps models better understand a query, align more accurately with the true intent, and fully leverage their capabilities.
As shown in the figure below, iterative optimization progressively improves prediction alignment, reducing factual drift.  
{{< /justify >}}

{{< image src="fig10.png" alt="venn" width="90%" title="Venn diagrams illustrating the accuracy of predictions obtained from the ‚ÄúOnly Sticker‚Äù and ‚ÄúQuery + Sticker‚Äù representations at each stage. The percentages represent the accuracy where both methods correctly predict the same outcomes.">}}


## Conclusion

{{< justify >}}
SIFT provides a systematic solution to factual drift, enabling LLMs to ‚Äústick to the facts‚Äù without costly retraining. By bridging the gap between contextual understanding and reasoning, it pushes the boundaries of reliable AI problem-solving.  
{{< /justify >}}


[^1]: Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems.
[^2]: Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems.
[^3]: Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. The Eleventh International Conference on Learning Representations.
[^4]: Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. Openai o1 system card.
[^5]: Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
[^6]: Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms.
