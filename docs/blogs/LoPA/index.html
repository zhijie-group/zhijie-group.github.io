<!doctype html><html lang=en dir=auto><head>
<meta charset=utf-8><title>LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding | DENG Lab @ SJTU</title>
<link crossorigin=anonymous href=/assets/css/stylesheet.c0b173c8753dc0cb5a0e3b2c0cd5a4fa0869fbd82e03d31553bf8232b3914c61.css rel="preload stylesheet" as=style>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
<script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script>
<style>
    table { width: 100%; border-collapse: collapse; margin: 20px 0; font-size: 14px; }
    th, td { border: 1px solid #ddd; padding: 8px; text-align: center; }
    th { background-color: #f2f2f2; }
    .post-content img { display: block; margin: 20px auto; max-width: 80%; }
    .caption { text-align: center; font-size: 14px; color: gray; margin-bottom: 30px; }
</style>
</head>
<body id=top>
<main class=main>
<article class=post-single>
    <header class=post-header>
        <h1 class="post-title">LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding</h1>
        <div class=post-meta>December 18, 2025 · Chenkai Xu*, Yijie Jin*, et al.</div>
    </header>
    
    <div class="post-content">
        <p><a href="https://github.com/zhijie-group/LoPA">Github Link</a></p>
        <p>We introduce the Lookahead Parallel Decoding (LoPA) algorithm for diffusion large language models (dLLMs) inference. LoPA enables up to 10.1 tokens per forward pass (TPF) for state-of-the-art dLLMs—without compromising predictive performance. This represents an unprecedented degree of parallelism, a capability unmatched by previous dLLM decoding methods. Under multi-device deployment, our specialized system LoPA-Dist achieves a single-sample throughput of 1073.9 tokens per second.</p>

        <h2>Background</h2>
        <p>DLLMs show significant potential for high-speed inference, yet current confidence-driven decoding strategies are constrained by limited parallelism—typically achieving only 1-3 TPF on math and coding tasks [2, 3]. Our investigation identifies a key insight: during dLLM inference, the degree of parallelism fluctuates sharply with the prediction confidence, which is heavily influenced by the Token Filling Order (TFO). Consequently, standard strategies that greedily prioritize currently high-confidence positions may lead to suboptimal trajectories. To address this, we propose Lookahead Parallel Decoding (LoPA), a training-free, plug-and-play algorithm designed to actively explore superior TFOs to unlock higher parallelism.</p>

        <h2>Methodology</h2>
        <h3>Preliminary: Confidence-Driven Sampling for dLLMs</h3>
        <p>Confidence-driven sampling is a prevalent paradigm for current dLLMs to boost parallelism... (省略部分文字以保持长度，请在此处粘贴原文)</p>
        <p>$$I_{fill} = \begin{cases} \{i \in M_t \mid \text{Conf}(i) > \tau\} & \text{if } \{i \in M_t \mid \text{Conf}(i) > \tau\} \neq \emptyset \\ \{\arg\max_{i \in M_t} \text{Conf}(i)\} & \text{otherwise} \end{cases}$$</p>

        <h2>LoPA</h2>
        <img src="image/figure3.png" alt="Figure 3">
        <div class="caption">Figure 3: Overview of the LoPA algorithm and TFO exploration.</div>

        <p>As shown in Figure 3, in every decoding iteration, LoPA looks ahead at multiple TFOs, yielding multiple sampling branches, and then identifies the branch with superior future parallel decoding potential.</p>

        <h2>Results</h2>
        <img src="image/figure4.png" alt="Figure 4">
        <div class="caption">Figure 4: TPF Scaling and Accuracy Performance on GSM8K and HumanEval+.</div>

        <h3>Table 1: Accuracy-preserving parallelism scaling of Dream</h3>
        <table>
            <thead>
                <tr><th rowspan="2">Model</th><th rowspan="2">Decoding algo</th><th colspan="2">MBPP 3-shot</th><th colspan="2">Math 4-shot</th><th colspan="2">HumanEval 0-shot</th><th colspan="2">GSM8K 4-shot</th></tr>
                <tr><th>TPF</th><th>Score</th><th>TPF</th><th>Score</th><th>TPF</th><th>Score</th><th>TPF</th><th>Score</th></tr>
            </thead>
            <tbody>
                <tr><td>Dream</td><td>vanilla</td><td>1</td><td>56.2</td><td>1</td><td>33.7</td><td>1</td><td>55.5</td><td>1</td><td>72.6</td></tr>
                <tr><td>Dream</td><td>Fast-dLLM</td><td>1.9</td><td>55.6</td><td>1.9</td><td>37.6</td><td>1.8</td><td>55.5</td><td>2.1</td><td>72.6</td></tr>
                <tr><td>Dream</td><td>LoPA</td><td>3.3</td><td>54.8</td><td>3.4</td><td>37.0</td><td>2.9</td><td>53</td><td>3.1</td><td>73.3</td></tr>
                <tr><td>D2F-Dream</td><td>vanilla</td><td>2.3</td><td>53.8</td><td>2.6</td><td>36.8</td><td>2.5</td><td>56.1</td><td>3.1</td><td>78.5</td></tr>
                <tr><td>D2F-Dream</td><td>LoPA</td><td>5.4</td><td>56.0</td><td>8.0</td><td>35.2</td><td>6.3</td><td>56.1</td><td>10.1</td><td>73.8</td></tr>
            </tbody>
        </table>

        <h3>Table 3: System performance of D2F-Dream</h3>
        <table>
            <thead>
                <tr><th>Model</th><th>Platform</th><th>Avg TPS</th><th>Max TPS</th><th>TPF</th><th>Latency</th></tr>
            </thead>
            <tbody>
                <tr><td>D2F-Dream-Base</td><td>LoPA-Dist-NV</td><td>630.28</td><td>1472.37</td><td>15.69</td><td>0.84</td></tr>
                <tr><td>D2F-Dream-Base</td><td>LoPA-Dist-Ascend</td><td>1073.86</td><td>2400.12</td><td>11.92</td><td>0.78</td></tr>
            </tbody>
        </table>
    </div>
</article>
</main>
</body></html>
