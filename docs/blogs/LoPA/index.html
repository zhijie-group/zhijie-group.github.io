<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        header {
            text-align: center;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            color: #2c3e50;
        }
        h2 {
            margin-top: 40px;
            color: #2c3e50;
            border-bottom: 2px solid #eaeaea;
            padding-bottom: 10px;
        }
        h3 {
            margin-top: 30px;
            color: #34495e;
        }
        .authors {
            font-size: 1.1em;
            color: #555;
            margin-bottom: 10px;
        }
        .affiliations {
            font-size: 0.9em;
            color: #777;
            margin-bottom: 20px;
        }
        .links a {
            color: #0366d6;
            text-decoration: none;
            font-weight: bold;
        }
        .links a:hover {
            text-decoration: underline;
        }
        .abstract {
            font-style: italic;
            background-color: #fff;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            margin-bottom: 30px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9em;
            background-color: #fff;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }
        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: center;
        }
        th {
            background-color: #f4f4f4;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        .caption {
            font-size: 0.9em;
            color: #666;
            margin-top: 5px;
            margin-bottom: 20px;
            text-align: center;
        }
        .references {
            list-style-type: none;
            padding: 0;
        }
        .references li {
            margin-bottom: 10px;
            padding-left: 20px;
            text-indent: -20px;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
        }
        .img-placeholder {
            text-align: center;
            margin: 20px 0;
            color: #888;
            font-style: italic;
        }
    </style>
</head>
<body>

    <header>
        <h1>LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding</h1>
        <div class="authors">
            Chenkai Xu*, Yijie Jin*, Jiajun Li, Yi Tu, Guoping Long, Dandan Tu, Tianqi Hou, Junchi Yan, Zhijie Deng †
        </div>
        <div class="links">
            <a href="https://github.com/zhijie-group/LoPA">Github: https://github.com/zhijie-group/LoPA</a>
        </div>
    </header>

    <div class="abstract">
        <p>We introduce the Lookahead Parallel Decoding (LoPA) algorithm for diffusion large language models (dLLMs) inference. LoPA enables up to 10.1 tokens per forward pass (TPF) for state-of-the-art dLLMs—without compromising predictive performance. This represents an unprecedented degree of parallelism, a capability unmatched by previous dLLM decoding methods. Under multi-device deployment, our specialized system LoPA-Dist achieves a single-sample throughput of 1073.9 tokens per second.</p>
    </div>

    <h2>Background</h2>
    <p>DLLMs show significant potential for high-speed inference, yet current confidence-driven decoding strategies are constrained by limited parallelism—typically achieving only 1-3 TPF on math and coding tasks [2, 3]. Our investigation identifies a key insight: during dLLM inference, the degree of parallelism fluctuates sharply with the prediction confidence, which is heavily influenced by the Token Filling Order (TFO). Consequently, standard strategies that greedily prioritize currently high-confidence positions may lead to suboptimal trajectories. To address this, we propose Lookahead Parallel Decoding (LoPA), a training-free, plug-and-play algorithm designed to actively explore superior TFOs to unlock higher parallelism.</p>

    <h2>Methodology</h2>
    <p>This section first explains the foundational Confidence-Driven Sampling used in regular dLLM inference~[2, 3, 4] and then elaborates on LoPA.</p>

    <h3>Preliminary: Confidence-Driven Sampling for dLLMs</h3>
    <p>Confidence-driven sampling is a prevalent paradigm for current dLLMs to boost parallelism, adopted in models such as Fast-dLLM [2], D2F [3], and SDAR [4]. Specifically, given a sequence $$x_$$ with a set of masked positions $$M_$$, the dLLM model $$p_\thet$$ outputs a predictive distribution $$p_\theta(\cdot | x_t$$. A candidate sequence $$\hat{x}_0 \sim p_\theta(\cdot | x_t)$$ is sampled, and a confidence function, $$\text{Conf}(\cdot)$$, assigns a score to each position $$i \in M_$$. The set of positions to fill, $$I_{fill$$, is then determined as:</p>
    
    $$I_{fill} = \begin{cases} \{i \in M_t \mid \text{Conf}(i) > \tau\} & \text{if } \{i \in M_t \mid \text{Conf}(i) > \tau\} \neq \emptyset \\ \{\arg\max_{i \in M_t} \text{Conf}(i)\} & \text{otherwise} \end{cases}$$
    
    <p>The algorithm then accepts the predictions according to $$I_{fill$$ and moves to the next iteration.</p>

    <h3>LoPA</h3>
    
    <p>As shown in Figure 3, in every decoding iteration, LoPA looks ahead at multiple TFOs, yielding multiple sampling branches, and then identifies the branch with superior future parallel decoding potential.</p>

    <h3>Look ahead Multiple TFOs in Parallel</h3>
    <p>LoPA operates by generating multiple branches. First, it constructs an Anchor Branch ($$B_$$) using the standard confidence-driven strategy (filling positions in $$I_{fill$$).</p>
    <p>LoPA is designed to explore one step further than this anchor branch. To ensure effective and reliable exploration, we prioritize sampling tokens with higher confidence, a strategy that has been proved in Fast-dLLM [2] to yield more stable predictions. Specifically, in addition to $$B_0$$, we generate $$$$ competitive Lookahead Branches. We identify the top-$$$$ positions from the anchor branch's unfilled set $$M_{B_0$$ that possess the highest confidence scores. For each identified position, we sample it independently to create a distinct branch. This results in a set of $$$$ new branches $$\{B_1, \dots, B_k\$$, each with its own sequence $$x_{B_j$$ and unfilled set $$M_{B_j$$.</p>

    <h3>Branch Confidence-based Verification</h3>
    <p>Inspired by DeepConf [5], we design a branch confidence metric to guide the selection among candidate decoding paths. Formally, the confidence of a branch $$B_$$ is defined as the average prediction confidence over its remaining unfilled positions $$M_{B_j$$:</p>
    
    $$C(B_j) = \frac{1}{|M_{B_j}|} \sum_{i \in M_{B_j}} \text{Conf}(i)$$
    
    <p>A higher branch confidence indicates that more unfilled positions are likely to be accepted in the very next decoding step. This directly increases the number of tokens filled per iteration, thereby enhancing the overall parallelism. Beyond this mean confidence, branch confidence can also be quantified by other methods [5], such as applying a sliding window to assess local quality or averaging confidence over the least confident segment to identify weak links.</p>
    <p>This verification mechanism offers distinct advantages. First, all candidate branches (Anchor + Lookahead) can be packed and verified within a single forward pass, with custom attention masks ensuring independent computation for each branch. Second, the logits computed during branch evaluation are directly reused in the next decoding step, eliminating the need for additional forward passes.</p>

    <h3>Application: integration with D2F</h3>
    <p>LoPA integrates seamlessly with D2F [3], the first open-source diffusion language model whose inference throughput surpasses that of autoregressive (AR) models. Our application of LoPA to D2F incorporates two key enhancements:</p>
    <ul>
        <li><strong>Parallel Exploration in a Decoding Window:</strong> We treat all active blocks in D2F's pipeline as a single window where LoPA's branch exploration and lookahead verification operate. Replacing the original block-level causal attention with a full attention mechanism within this window reduces implementation complexity and enhances computational performance.</li>
        <li><strong>System Integration and Performance:</strong> On the D2F-Dream model, LoPA achieves a TPF of up to 10.1. To leverage this parallelism, we developed a specialized multi-device inference system where LoPA achieves a throughput of 1073.86 tokens per second.</li>
    </ul>

    <h2>Results</h2>
    <h3>Scaling Analysis of Branch Count</h3>
    <p>We analyzed the impact of the competitive branch count (k) on TPF and quality using D2F models fine-tuned on Dream[6] and DiffuCoder[7]. Results show TPF consistently improves with k; however, excessive k introduces fluctuations, attributed to the model prioritizing future confidence over local optimality. These results point to an optimal trade-off, where a carefully chosen k can maximize TPF while preserving quality.</p>
    
    

    <p>As shown in the Figure 4, on GSM8K, LoPA scales the TPF of D2F-Dream to 10.1 while maintaining a score (73.8) superior to the Dream baseline (72.6). on HumanEval+, LoPA scales the TPF of D2F-DiffuCoder to 8.3 with marginal performance degradation, demonstrating a clear speed-accuracy trade-off.</p>
    
    <p>Tables 1 and 2 below confirm this efficacy across multiple benchmarks.</p>

    <table>
        <thead>
            <tr>
                <th rowspan="2">Model</th>
                <th rowspan="2">Decoding algo</th>
                <th colspan="2">MBPP 3-shot</th>
                <th colspan="2">Math 4-shot</th>
                <th colspan="2">HumanEval 0-shot</th>
                <th colspan="2">GSM8K 4-shot</th>
            </tr>
            <tr>
                <th>TPF</th>
                <th>Score</th>
                <th>TPF</th>
                <th>Score</th>
                <th>TPF</th>
                <th>Score</th>
                <th>TPF</th>
                <th>Score</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Dream</td>
                <td>vanilla</td>
                <td>1</td>
                <td>56.2</td>
                <td>1</td>
                <td>33.7</td>
                <td>1</td>
                <td>55.5</td>
                <td>1</td>
                <td>72.6</td>
            </tr>
            <tr>
                <td></td>
                <td>Fast-dLLM</td>
                <td>1.9</td>
                <td>55.6</td>
                <td>1.9</td>
                <td>37.6</td>
                <td>1.8</td>
                <td>55.5</td>
                <td>2.1</td>
                <td>72.6</td>
            </tr>
            <tr>
                <td></td>
                <td>LoPA</td>
                <td>3.3</td>
                <td>54.8</td>
                <td>3.4</td>
                <td>37.0</td>
                <td>2.9</td>
                <td>53</td>
                <td>3.1</td>
                <td>73.3</td>
            </tr>
            <tr>
                <td>D2F-Dream</td>
                <td>vanilla</td>
                <td>2.3</td>
                <td>53.8</td>
                <td>2.6</td>
                <td>36.8</td>
                <td>2.5</td>
                <td>56.1</td>
                <td>3.1</td>
                <td>78.5</td>
            </tr>
            <tr>
                <td></td>
                <td>LoPA</td>
                <td>5.4</td>
                <td>56.0</td>
                <td>8.0</td>
                <td>35.2</td>
                <td>6.3</td>
                <td>56.1</td>
                <td>10.1</td>
                <td>73.8</td>
            </tr>
        </tbody>
    </table>
    <div class="caption">Table 1. Accuracy-preserving parallelism scaling of Dream on multiple benchmarks across multiple branches. TPF denotes Tokens Per Forward pass. LoPA significantly scales the TPF of D2F-Dream while maintaining or exceeding baseline scores.</div>

    <table>
        <thead>
            <tr>
                <th rowspan="2">Model</th>
                <th rowspan="2">Decoding algo</th>
                <th colspan="2">MBPP++ 0-shot</th>
                <th colspan="2">HumanEval++ 0-shot</th>
            </tr>
            <tr>
                <th>TPF</th>
                <th>Score</th>
                <th>TPF</th>
                <th>Score</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>DiffuCoder</td>
                <td>vanilla</td>
                <td>1</td>
                <td>61.9</td>
                <td>1</td>
                <td>65.2</td>
            </tr>
            <tr>
                <td>D2F-Diffucoder</td>
                <td>vanilla</td>
                <td>2.2</td>
                <td>61.9</td>
                <td>2.2</td>
                <td>65.9</td>
            </tr>
            <tr>
                <td></td>
                <td>LoPA</td>
                <td>6.7</td>
                <td>61.6</td>
                <td>8.3</td>
                <td>64.0</td>
            </tr>
        </tbody>
    </table>
    <div class="caption">Table 2. Accuracy-preserving parallelism scaling of DiffuCoder on MBPP+ and HumanEval+ benchmarks. LoPA boosts TPF by nearly 4× compared to the vanilla D2F baseline with minimal impact on generation quality.</div>

    <h2>System Throughput and Scalability</h2>
    <p>To fully exploit LoPA’s parallelism, we designed LoPA-Dist, a distributed inference system utilizing Branch Parallelism (BP).</p>
    
    

    <p>The system distributes candidate branches across multiple GPUs for concurrent processing. We provide two specialized implementations:</p>
    <ul>
        <li>LoPA-Dist-NV (CUDA): Optimized for low latency using static KV cache and a two-phase update protocol (Pre-Write and Commit-Winner-Cache) to ensure consistency.</li>
        <li>LoPA-Dist-Ascend (Ascend 910C): Optimized for high throughput using hybrid parallelism and graph compilation to fuse element-wise operations.</li>
    </ul>

    <p>As shown in Table 3, this design achieves near-linear scalability. On the Ascend platform, LoPA-Dist achieves a peak throughput of 1073.86 tokens/s.</p>

    <table>
        <thead>
            <tr>
                <th rowspan="2">Model</th>
                <th rowspan="2">Platform</th>
                <th colspan="5">MBPP</th>
                <th colspan="5">GSM8K</th>
            </tr>
            <tr>
                <th>Avg TPS</th>
                <th>Max TPS</th>
                <th>TPF</th>
                <th>Latency</th>
                <th>PU</th>
                <th>Avg TPS</th>
                <th>Max TPS</th>
                <th>TPF</th>
                <th>Latency</th>
                <th>PU</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>D2F-Dream-Base</td>
                <td>LoPA-Dist-NV</td>
                <td>630.28</td>
                <td>1472.37</td>
                <td>15.69</td>
                <td>0.84</td>
                <td>0.69</td>
                <td>566.97</td>
                <td>1305.86</td>
                <td>13.31</td>
                <td>0.93</td>
                <td>0.74</td>
            </tr>
            <tr>
                <td></td>
                <td>LoPA-Dist-Ascend</td>
                <td>1073.86</td>
                <td>2400.12</td>
                <td>11.92</td>
                <td>0.78</td>
                <td>0.72</td>
                <td>856.46</td>
                <td>2751.61</td>
                <td>9.34</td>
                <td>0.75</td>
                <td>0.76</td>
            </tr>
            <tr>
                <td>D2F-Dream-Instruct</td>
                <td>LoPA-Dist-NV</td>
                <td>543.32</td>
                <td>1531.64</td>
                <td>9.45</td>
                <td>0.16</td>
                <td>0.87</td>
                <td>536.71</td>
                <td>1141.71</td>
                <td>11.41</td>
                <td>0.29</td>
                <td>0.76</td>
            </tr>
            <tr>
                <td></td>
                <td>LoPA-Dist-Ascend</td>
                <td>896.21</td>
                <td>2586.73</td>
                <td>8.64</td>
                <td>0.11</td>
                <td>0.74</td>
                <td>897.10</td>
                <td>1868.16</td>
                <td>9.30</td>
                <td>0.21</td>
                <td>0.76</td>
            </tr>
        </tbody>
    </table>
    <div class="caption">Table 3. System performance of D2F-Dream under guaranteed inference speed. The results demonstrate that our system efficiently translates algorithmic parallelism (high TPF) into significant wall-clock acceleration, achieving high Parallelism Utilization (PU) and average throughputs exceeding 1000 tokens/s on the specialized LoPA-Dist-Ascend engine. algorithmic parallelism (high TPF) into significant wall-clock acceleration, achieving high Parallelism Utilization (PU) and average throughputs exceeding 1000 tokens/s on the specialized LoPA-Dist-Ascend engine.</div>

    <h2>Future Works</h2>
    <p>We will explore adapting LoPA to SDAR and other confidence-driven diffusion language models to further demonstrate its generalizability and effectiveness across diverse model architectures.</p>

    <h2>Reference</h2>
    <ul class="references">
        <li>[1] Nie, Shen, et al. "Large language diffusion models." arXiv preprint arXiv:2502.09992 (2025).</li>
        <li>[2] Wu, Chengyue, et al. "Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding." arXiv preprint arXiv:2505.22618 (2025).</li>
        <li>[3] Wang, Xu, et al. "Diffusion llms can do faster-than-ar inference via discrete diffusion forcing." arXiv preprint arXiv:2508.09192 (2025).</li>
        <li>[4] Cheng, Shuang, et al. "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation." arXiv preprint arXiv:2510.06303 (2025).</li>
        <li>[5] Fu, Yichao, et al. "Deep think with confidence." arXiv preprint arXiv:2508.15260 (2025).</li>
        <li>[6] Ye, Jiacheng, et al. "Dream 7b: Diffusion large language models." arXiv preprint arXiv:2508.15487 (2025).</li>
        <li>[7] Gong, Shansan, et al. "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation." arXiv preprint arXiv:2506.20639 (2025).</li>
    </ul>

</body>
</html>
