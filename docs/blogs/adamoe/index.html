<!doctype html><html lang=en dir=auto><head><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css rel=stylesheet><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AdaMoE: Token-Adaptive Routing with Null Experts for MoE | DENG Lab @ SJTU</title>
<meta name=keywords content><meta name=description content="
    
    
    
        
            
        
    
    
        
            
        
    



    LongCat-Flash[1] just showed a clean, large-scale deployment of token-adaptive MoE with zero-computation (identity) experts—-activating 18.6–31.3B parameters per token (~27B on average) inside a 560B MoE. Each layer mixes 512 FFN experts + 256 zero-compute experts, the router selects Top-12, and the average true-expert picks settle around ~8 thanks to a PID-style budget controller; device-level load balancing and ScMoE (shortcut-connected MoE) keep the system efficient. They present the model as a non-thinking foundation model with strong throughput/cost metrics."><meta name=author content="Zihao Zeng*, Yibo Miao*, Hongcheng Gao, Hao Zhang, Zhijie Deng†"><link rel=canonical href=https://zhijie-group.github.io/blogs/adamoe/><link crossorigin=anonymous href=/assets/css/stylesheet.c0b173c8753dc0cb5a0e3b2c0cd5a4fa0869fbd82e03d31553bf8232b3914c61.css integrity="sha256-wLFzyHU9wMtaDjssDNWk+ghp+9guA9MVU7+CMrORTGE=" rel="preload stylesheet" as=style><link rel=icon href=https://zhijie-group.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zhijie-group.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://zhijie-group.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://zhijie-group.github.io/apple-touch-icon.png><link rel=mask-icon href=https://zhijie-group.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://zhijie-group.github.io/blogs/adamoe/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(255, 255, 255);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-T4DXGLCH1D"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-T4DXGLCH1D")}</script><meta property="og:title" content="AdaMoE: Token-Adaptive Routing with Null Experts for MoE"><meta property="og:description" content="
    
    
    
        
            
        
    
    
        
            
        
    



    LongCat-Flash[1] just showed a clean, large-scale deployment of token-adaptive MoE with zero-computation (identity) experts—-activating 18.6–31.3B parameters per token (~27B on average) inside a 560B MoE. Each layer mixes 512 FFN experts + 256 zero-compute experts, the router selects Top-12, and the average true-expert picks settle around ~8 thanks to a PID-style budget controller; device-level load balancing and ScMoE (shortcut-connected MoE) keep the system efficient. They present the model as a non-thinking foundation model with strong throughput/cost metrics."><meta property="og:type" content="article"><meta property="og:url" content="https://zhijie-group.github.io/blogs/adamoe/"><meta property="og:image" content="https://zhijie-group.github.io/blogs/adamoe/image/vs.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2025-09-08T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-08T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zhijie-group.github.io/blogs/adamoe/image/vs.png"><meta name=twitter:title content="AdaMoE: Token-Adaptive Routing with Null Experts for MoE"><meta name=twitter:description content="
    
    
    
        
            
        
    
    
        
            
        
    



    LongCat-Flash[1] just showed a clean, large-scale deployment of token-adaptive MoE with zero-computation (identity) experts—-activating 18.6–31.3B parameters per token (~27B on average) inside a 560B MoE. Each layer mixes 512 FFN experts + 256 zero-compute experts, the router selects Top-12, and the average true-expert picks settle around ~8 thanks to a PID-style budget controller; device-level load balancing and ScMoE (shortcut-connected MoE) keep the system efficient. They present the model as a non-thinking foundation model with strong throughput/cost metrics."><meta name=twitter:site content="@sjtudenglab"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://zhijie-group.github.io/blogs/"},{"@type":"ListItem","position":2,"name":"AdaMoE: Token-Adaptive Routing with Null Experts for MoE","item":"https://zhijie-group.github.io/blogs/adamoe/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AdaMoE: Token-Adaptive Routing with Null Experts for MoE","name":"AdaMoE: Token-Adaptive Routing with Null Experts for MoE","description":" LongCat-Flash[1] just showed a clean, large-scale deployment of token-adaptive MoE with zero-computation (identity) experts—-activating 18.6–31.3B parameters per token (~27B on average) inside a 560B MoE. Each layer mixes 512 FFN experts + 256 zero-compute experts, the router selects Top-12, and the average true-expert picks settle around ~8 thanks to a PID-style budget controller; device-level load balancing and ScMoE (shortcut-connected MoE) keep the system efficient. They present the model as a non-thinking foundation model with strong throughput/cost metrics.\n","keywords":[],"articleBody":" LongCat-Flash[1] just showed a clean, large-scale deployment of token-adaptive MoE with zero-computation (identity) experts—-activating 18.6–31.3B parameters per token (~27B on average) inside a 560B MoE. Each layer mixes 512 FFN experts + 256 zero-compute experts, the router selects Top-12, and the average true-expert picks settle around ~8 thanks to a PID-style budget controller; device-level load balancing and ScMoE (shortcut-connected MoE) keep the system efficient. They present the model as a non-thinking foundation model with strong throughput/cost metrics.\nThat design—adding null experts and bumping top-k so each token uses a variable number of true experts—is precisely the idea behind AdaMoE.\nTL; DR AdaMoE adds a set of null experts (with zero compute) to the expert pool to enable token-adaptive expert choice for MoE.\nUnder the top-k routing paradigm, tokens that route to null experts effectively use fewer true experts, making the number of true experts per token adaptive under the same average budget. With a minor tweak to the load‑balancing loss (treat all nulls as one averaged bucket) and a simple annealing schedule, AdaMoE reduces FLOPs while maintaining or improving accuracy (e.g., on Mixtral‑8×7B/ARC‑C: −14.55% FLOPs with +1.69% accuracy). Why Expert Selection Should Be Adaptive at the Token Level The rationale for shifting from a fixed top-k routing mechanism to a token-adaptive one is based on a critical observation: not all tokens are computationally equal. The information content and processing complexity vary dramatically across different tokens within a text. A traditional MoE model, which compels every token to activate a fixed number of experts, allocates computation uniformly, irrespective of this variance. This static approach can lead to inefficient resource distribution. Proportions of the number of top experts with cumulative routing probabilities exceeding 50% for tokens in the SocialIQA dataset. Each bar represents the proportion of different counts of tokens at the corresponding MoE layer in Mixtral-8x7B. We provide empirical evidence for this variance. We analyzed the routing probability distributions in Mixtral-8x7B, a model with a fixed top-2 router. Our analysis revealed two key patterns:\nA large fraction of tokens had routing probabilities that were highly concentrated on a single expert, indicating that the activation of a second expert was often superfluous; A significant portion of other tokens had their probabilities distributed more evenly across multiple experts, suggesting that they required the computational capacity of two or even more experts for effective processing. This finding demonstrates that a static top-k strategy is suboptimal, leading to computationally excessive allocations for simple tokens and potentially insufficient allocations for complex ones.\n“Null Experts” for Adaptive Routing AdaMoE achieves token-adaptive expert selection by incorporating null experts, which are defined as an empty operation requiring zero FLOPs to process the token feature. In the context of LLMs, common operations satisfying this requirement include a constant zero mapping and an identity mapping (we take the zero mappings null expert as the default choice in the following just for simplicity).\nOur mechanism operates as follows:\nExtends the expert set with m null experts (besides n true experts). Slightly increases the router’s top‑k (e.g., from 2 → 3/4). Now each token’s top‑k may include some nulls. Makes compute adaptive: if top‑k includes r nulls, the token uses only k-r true experts. Balances sensibly by aggregating all nulls into a single bucket in the load‑balance loss (don’t force balance between identical nulls). Normalizes over true experts only after top‑k so the output scale matches vanilla MoE. Fixed top-2 vs. AdaMoE. Left: vanilla top‑2 where every token activates exactly 2 true experts. Right: AdaMoE where top‑4 is chosen from 4 true experts + 5 null experts; some tokens hit 3 true experts, others only 1. Main Results On Mixtral‑8×7B fine‑tuning, AdaMoE reduces FFN FLOPs while keeping or improving accuracy on multiple benchmarks. For example, on ARC‑Challenge, FLOPs drop by ~14.55% with +1.69% accuracy, and the layer‑wise average experts/token falls from 2.0 → ~1.67. From Paper to Production We are also encouraged to see that the concept of null experts is not merely theoretical, but has been implemented in state-of-the-art LLMs. The technical report for LongCat-Flash identifies zero-computation experts as a key architectural innovation and cites our paper.\nThe report explains that this mechanism enables the model to “allocate a dynamic computation budget to important tokens based on their significance,” activating a variable range of parameters for each token depending on context. This direct industrial application underscores the practicality and scalability of the adaptive routing strategy we proposed.\nIn addition, LongCat-Flash introduces several optimization techniques to address communication and load-balancing challenges associated with adaptive expert selection—further demonstrating the viability of our approach in large-scale systems.\nCite Our Work If you find our work useful, please cite our paper:\n@inproceedings{zeng-etal-2024-adamoe, title = \"AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models\", author = \"Zeng, Zihao and Miao, Yibo and Gao, Hongcheng and Zhang, Hao and Deng, Zhijie\", editor = \"Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung\", booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2024\", month = nov, year = \"2024\", address = \"Miami, Florida, USA\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2024.findings-emnlp.361/\", doi = \"10.18653/v1/2024.findings-emnlp.361\", pages = \"6223--6235\" } Reference [1] Team, Meituan LongCat, et al. “LongCat-Flash Technical Report.” arXiv preprint arXiv:2509.01322 (2025).\n","wordCount":"866","inLanguage":"en","image":"https://zhijie-group.github.io/blogs/adamoe/image/vs.png","datePublished":"2025-09-08T00:00:00Z","dateModified":"2025-09-08T00:00:00Z","author":{"@type":"Person","name":"Zihao Zeng*, Yibo Miao*, Hongcheng Gao, Hao Zhang, Zhijie Deng†"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zhijie-group.github.io/blogs/adamoe/"},"publisher":{"@type":"Organization","name":"DENG Lab @ SJTU","logo":{"@type":"ImageObject","url":"https://zhijie-group.github.io/favicon.ico"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://zhijie-group.github.io/ accesskey=h title="DENG Lab @ SJTU (Alt + H)"><img id=logo-image src=/img/logo-new.png alt="DENG Lab @ SJTU" height=40></a><div class=logo-switches><ul class=lang-switch></ul></div></div><ul id=menu><div style=margin-right:15px><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div><li><a href=https://zhijie-group.github.io/home/ title=Home><span>Home</span></a></li><li><a href=https://zhijie-group.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://zhijie-group.github.io/people/ title=People><span>People</span></a></li><li><a href=https://zhijie-group.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://zhijie-group.github.io/contact/ title=Contact><span>Contact</span></a></li><li><a href=https://x.com/sjtudenglab title=Twitter><i class="fab fa-twitter fa-lg"></i></a></li><li><a href=https://github.com/zhijie-group title=GitHub><i class="fab fa-github fa-lg"></i></a></li><li><a href=https://www.zhihu.com/people/SJTUDengLab title=Zhihu><span>Zhihu</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><script>function updateThemeAndLogo(){document.body.classList.toggle("dark");const e=document.body.classList.contains("dark"),n=document.getElementById("logo-image"),t=e?"dark":"light";localStorage.setItem("pref-theme",t)}function applyCurrentThemeAndLogo(){const e=document.body.classList.contains("dark"),t=document.getElementById("logo-image")}document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("theme-toggle");e&&e.addEventListener("click",updateThemeAndLogo);const t=localStorage.getItem("pref-theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");document.body.classList.toggle("dark",t==="dark")});const themeToggleButton=document.getElementById("theme-toggle");themeToggleButton&&(themeToggleButton.addEventListener("click",()=>{updateThemeAndLogo()}),applyCurrentThemeAndLogo())</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-T4DXGLCH1D"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-T4DXGLCH1D")</script><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">AdaMoE: Token-Adaptive Routing with Null Experts for MoE</h1><div class=post-meta><span title='2025-09-08 00:00:00 +0000 UTC'>September 8, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Zihao Zeng*, Yibo Miao*, Hongcheng Gao, Hao Zhang, Zhijie Deng†</div></header><figure class=entry-cover><img loading=eager src=https://zhijie-group.github.io/blogs/adamoe/image/vs.png alt=AdaMoE><p><strong>Left</strong>: vanilla top-2 MoE. <strong>Right</strong>: AdaMoE with null experts.</p></figure><div class=post-content><div style=padding-left:30px;display:flex!important;gap:60px!important><a href=https://arxiv.org/pdf/2406.13233 target=_blank rel="noopener noreferrer"><img src="https://img.shields.io/badge/📃 Paper Link--1abc9c.svg?style=social" style=transform:scale(1.5)>
</a><a href=https://github.com/zhijie-group/AdaMoE target=_blank rel="noopener noreferrer"><img src="https://img.shields.io/badge/💻 Code Link--1abc9c.svg?style=social" style=transform:scale(1.5)></a></div><div style=text-align:left><p>LongCat-Flash[1] just showed a clean, large-scale deployment of token-adaptive MoE with zero-computation (identity) experts—-activating 18.6–31.3B parameters per token (~27B on average) inside a 560B MoE. Each layer mixes 512 FFN experts + 256 zero-compute experts, the router selects Top-12, and the average true-expert picks settle around ~8 thanks to a PID-style budget controller; device-level load balancing and ScMoE (shortcut-connected MoE) keep the system efficient. They present the model as a non-thinking foundation model with strong throughput/cost metrics.</p><p>That design—adding null experts and bumping top-k so each token uses a variable number of true experts—is precisely the idea behind AdaMoE.</p></div><h2 id=tl-dr>TL; DR<a hidden class=anchor aria-hidden=true href=#tl-dr>#</a></h2><div style=text-align:left><p>AdaMoE adds a set of <strong>null experts</strong> (with zero compute) to the expert pool to enable token-adaptive expert choice for MoE.</p><ul><li>Under the top-k routing paradigm, tokens that route to null experts effectively use fewer true experts, making the number of <strong>true experts per token adaptive</strong> under the same average budget.</li><li>With a minor tweak to the load‑balancing loss (treat all nulls as one averaged bucket) and a simple annealing schedule, AdaMoE reduces FLOPs while maintaining or improving accuracy (e.g., on Mixtral‑8×7B/ARC‑C: −14.55% FLOPs with +1.69% accuracy).</li></ul></div><h2 id=why-expert-selection-should-be-adaptive-at-the-token-level>Why Expert Selection Should Be Adaptive at the Token Level<a hidden class=anchor aria-hidden=true href=#why-expert-selection-should-be-adaptive-at-the-token-level>#</a></h2><div style=text-align:left>The rationale for shifting from a fixed <strong>top-k</strong> routing mechanism to a token-adaptive one is based on a critical observation: <strong>not all tokens are computationally equal</strong>. The information content and processing complexity vary dramatically across different tokens within a text. A traditional MoE model, which compels every token to activate a fixed number of experts, allocates computation uniformly, irrespective of this variance. This static approach can lead to inefficient resource distribution.</div><style>.gray-text{color:gray}</style><figure><div style=display:grid;place-items:center><img src=image/pre.png alt=Pre-Experiment style=width:55%;height:auto><figcaption style=font-size:16px class=gray-text>Proportions of the number of top experts with cumulative routing probabilities exceeding 50% for tokens in the SocialIQA dataset. Each bar represents the proportion of different counts of tokens at the corresponding MoE layer in Mixtral-8x7B.</figcaption></div></figure><div style=text-align:left><p>We provide empirical evidence for this variance. We analyzed the routing probability distributions in Mixtral-8x7B, a model with a fixed top-2 router. Our analysis revealed two key patterns:</p><ol><li>A large fraction of tokens had routing probabilities that were highly concentrated on a single expert, indicating that the activation of a second expert was often superfluous;</li><li>A significant portion of other tokens had their probabilities distributed more evenly across multiple experts, suggesting that they required the computational capacity of two or even more experts for effective processing.</li></ol><p>This finding demonstrates that a static top-k strategy is suboptimal, leading to computationally excessive allocations for simple tokens and potentially insufficient allocations for complex ones.</p></div><h2 id=null-experts-for-adaptive-routing>“Null Experts” for Adaptive Routing<a hidden class=anchor aria-hidden=true href=#null-experts-for-adaptive-routing>#</a></h2><div style=text-align:left><p>AdaMoE achieves token-adaptive expert selection by incorporating <strong>null experts</strong>, which are defined as an empty operation requiring <strong>zero FLOPs</strong> to process the token feature. In the context of LLMs, common operations satisfying this requirement include a constant zero mapping and an identity mapping (we take the zero mappings null expert as the default choice in the following just for simplicity).</p><p>Our mechanism operates as follows:</p><ol><li><strong>Extends the expert set</strong> with m null experts (besides n true experts).</li><li><strong>Slightly increases</strong> the router’s <strong>top‑k</strong> (e.g., from 2 → 3/4). Now each token’s top‑k may include some nulls.</li><li><strong>Makes compute adaptive:</strong> if top‑k includes r nulls, the token uses only k-r true experts.</li><li><strong>Balances sensibly</strong> by <strong>aggregating all nulls into a single bucket</strong> in the load‑balance loss (don’t force balance between identical nulls).</li><li><strong>Normalizes over true experts only</strong> after top‑k so the output scale matches vanilla MoE.</li></ol></div><style>.gray-text{color:gray}</style><figure><div style=display:grid;place-items:center><img src=image/vs.png alt=DeepSeek style=width:85%;height:auto><figcaption style=font-size:16px class=gray-text>Fixed top-2 vs. AdaMoE. Left: vanilla top‑2 where every token activates exactly 2 true experts. Right: AdaMoE where top‑4 is chosen from 4 true experts + 5 null experts; some tokens hit 3 true experts, others only 1.</figcaption></div></figure><h2 id=main-results>Main Results<a hidden class=anchor aria-hidden=true href=#main-results>#</a></h2><div style=text-align:left>On Mixtral‑8×7B fine‑tuning, AdaMoE reduces FFN FLOPs while keeping or improving accuracy on multiple benchmarks. For example, on ARC‑Challenge, FLOPs drop by ~14.55% with +1.69% accuracy, and the layer‑wise average experts/token falls from 2.0 → ~1.67.</div><style>.gray-text{color:gray}</style><img src=image/res.png alt=venn style=width:80%;height:auto><h2 id=from-paper-to-production>From Paper to Production<a hidden class=anchor aria-hidden=true href=#from-paper-to-production>#</a></h2><div style=text-align:left><p>We are also encouraged to see that the concept of null experts is not merely theoretical, but has been implemented in state-of-the-art LLMs. The technical report for <strong>LongCat-Flash</strong> identifies <strong>zero-computation experts</strong> as a key architectural innovation and cites our paper.</p><p>The report explains that this mechanism enables the model to “allocate a dynamic computation budget to important tokens based on their significance,” activating a variable range of parameters for each token depending on context. This direct industrial application underscores the practicality and scalability of the adaptive routing strategy we proposed.</p><p>In addition, LongCat-Flash introduces several optimization techniques to address communication and load-balancing challenges associated with adaptive expert selection—further demonstrating the viability of our approach in large-scale systems.</p></div><h2 id=cite-our-work>Cite Our Work<a hidden class=anchor aria-hidden=true href=#cite-our-work>#</a></h2><p>If you find our work useful, please cite our paper:</p><pre tabindex=0><code>@inproceedings{zeng-etal-2024-adamoe,
    title = &#34;AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models&#34;,
    author = &#34;Zeng, Zihao  and
      Miao, Yibo  and
      Gao, Hongcheng  and
      Zhang, Hao  and
      Deng, Zhijie&#34;,
    editor = &#34;Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung&#34;,
    booktitle = &#34;Findings of the Association for Computational Linguistics: EMNLP 2024&#34;,
    month = nov,
    year = &#34;2024&#34;,
    address = &#34;Miami, Florida, USA&#34;,
    publisher = &#34;Association for Computational Linguistics&#34;,
    url = &#34;https://aclanthology.org/2024.findings-emnlp.361/&#34;,
    doi = &#34;10.18653/v1/2024.findings-emnlp.361&#34;,
    pages = &#34;6223--6235&#34;
}
</code></pre><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p>[1] Team, Meituan LongCat, et al. &ldquo;LongCat-Flash Technical Report.&rdquo; arXiv preprint arXiv:2509.01322 (2025).</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://zhijie-group.github.io/>DENG Lab @ SJTU</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
, Adapted by Zhijie Deng</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>