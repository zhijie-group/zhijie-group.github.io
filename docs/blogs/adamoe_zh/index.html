<!doctype html><html lang=en dir=auto><head><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css rel=stylesheet><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AdaMoE: 借助“空专家”实现Token级别的动态路由选择 | DENG Lab @ SJTU</title>
<meta name=keywords content><meta name=description content="
    
    
    
        
            
        
    
    
        
            
        
    



    LongCat-Flash[1] 近期展示了一种token级别自适应的MoE的大规模部署方案。该方案利用零计算（恒等）专家，在560B MoE模型中，每个token激活18.6B~31.3B的参数量（平均约27B）。每一层混合了512个真专家和256个零计算专家，每次选择top-12的专家，并且由于采用了PID式预算控制器，平均真专家选择数量稳定在约8个。设备级负载均衡和ScMoE保证了系统的效率。他们将该模型呈现为一个具有强大吞吐量/成本指标的非思考模型。
这种设计——添加空专家并增加top-k值，从而使每个token使用可变数量的真实专家——正是AdaMoE背后的核心思想。





太长不看版

    AdaMoE 通过向专家池中添加一组 “空专家”（null experts）（其计算开销为零），为混合专家模型（MoE）带来了Token级别的自适应专家选择能力。

在 top-k路由范式下，当一个Token被路由到“空专家”时，它实际上激活了更少的真实专家。这使得每个Token所激活的真实专家数量，在保持平均计算预算不变的前提下，实现了自适应。
我们对负载均衡损失函数（load‑balancing loss）进行了微调（将所有“空专家”视为一个聚合的计算单元），并采用了一个简单的退火策略。AdaMoE在降低了计算量（FLOPs）的同时，保持甚至提升了模型的准确率。例如，在Mixtral‑8×7B模型和ARC‑C数据集上的实验显示：计算量降低了14.55%，而准确率提升了1.69%。





为何专家选择需要Token级别的自适应？

    我们将混合专家模型的路由机制从固定的 top-k 模式转向Token级别的自适应模式，其根本原因在于一个关键的观察：并非所有Token在计算上都生而平等。在一段文本中，不同Token所蕴含的信息量和处理的复杂度存在巨大差异。传统的MoE模型强制每个Token激活固定数量的专家，这种统一的计算分配方式忽略了Token间的差异，从而导致了计算资源的低效分配。







  
	  
      
      在 SocialIQA 数据集上，不同Token的累积路由概率超过50%所需的Top专家数量分布。每一条柱状图代表Mixtral-8x7B模型中，对应MoE层内不同专家数量的Token占比。
    
  



    我们为此提供了实验性的证据。我们分析了Mixtral-8x7B（一个采用固定top-2路由的模型）的路由概率分布，并发现了两种关键模式：

大量Token的路由概率高度集中于单个专家，这表明激活第二个专家通常是多余的；
另有相当一部分Token的概率更均匀地分布在多个专家上，这意味着它们可能需要两个甚至更多专家的计算能力才能被有效处理。

这一发现有力地证明，固定的 top-k策略是次优的，它对简单的Token造成了过度计算，而对复杂的Token则可能计算不足。





借助“空专家”实现自适应路由

    AdaMoE 通过引入 “空专家”（null experts）来实现Token级别的自适应专家选择。我们将其定义为一种不执行任何操作的单元，处理Token特征所需的计算量（FLOPs）为零。在大型语言模型的实践中，常见的零计算操作包括常数零映射和恒等映射（为简化起见，我们在后续讨论中默认采用零映射作为“空专家”的实现）。
我们的机制运行如下：

在原有的 n个真实专家之外，将专家集合扩展 m个“空专家”。
略微增加路由器的 top-k 值（例如，从 2 增加到 3 或 4）。这样，每个Token选出的 top-k专家中就可能包含一部分“空专家”。
实现计算的自适应：如果 top-k专家中包含 r个“空专家”，那么该Token实际上只使用了 k-r个真实专家。
进行合理的负载均衡：在计算负载均衡损失时，我们将所有“空专家”聚合为单一的计算单元（因为没有必要在完全等价的“空专家”之间强制实现均衡）。
在 top-k选择之后，仅对真实专家进行归一化，以确保输出的尺度与标准的MoE模型保持一致。









  
	  
      
      固定的 top-2 路由与 AdaMoE 对比。左图：标准的 top-2 路由，每个Token精确激活2个真实专家。右图：AdaMoE，从4个真实专家和5个“空专家”中选择 top-4，使得某些Token激活了3个真实专家，而另一些只激活了1个。
    
  


主要实验结果

    在对 Mixtral‑8×7B 进行微调时，AdaMoE 在多个基准测试中，既降低了FFN层的计算量（FLOPs），又保持甚至提升了模型的准确率。例如，在 ARC‑Challenge 数据集上，总FLOPs下降了约14.55%，而准确率提升了1.69%。同时，每层每个Token激活的平均专家数从 2.0 降至约 1.67。







  
	  
      
      AdaMoE在多个数据集上的性能表现，展示了准确率（Acc.）、FLOPs降低百分比（%FLOPs）和平均专家负载（Load）的对比。
    
  


从论文走向实践

    我们备受鼓舞地看到，“空专家”这一概念并非仅仅停留在理论层面，它已被业界前沿的大型语言模型所采纳和实现。LongCat-Flash的技术报告中，就将 “零计算专家”（zero-computation experts）列为一项关键的架构创新，并引用了我们的论文。"><meta name=author content="Zihao Zeng*, Yibo Miao*, Hongcheng Gao, Hao Zhang, Zhijie Deng†"><link rel=canonical href=https://zhijie-group.github.io/blogs/adamoe_zh/><link crossorigin=anonymous href=/assets/css/stylesheet.c0b173c8753dc0cb5a0e3b2c0cd5a4fa0869fbd82e03d31553bf8232b3914c61.css integrity="sha256-wLFzyHU9wMtaDjssDNWk+ghp+9guA9MVU7+CMrORTGE=" rel="preload stylesheet" as=style><link rel=icon href=https://zhijie-group.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zhijie-group.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://zhijie-group.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://zhijie-group.github.io/apple-touch-icon.png><link rel=mask-icon href=https://zhijie-group.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://zhijie-group.github.io/blogs/adamoe_zh/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(255, 255, 255);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-T4DXGLCH1D"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-T4DXGLCH1D")}</script><meta property="og:title" content="AdaMoE: 借助“空专家”实现Token级别的动态路由选择"><meta property="og:description" content="
    
    
    
        
            
        
    
    
        
            
        
    



    LongCat-Flash[1] 近期展示了一种token级别自适应的MoE的大规模部署方案。该方案利用零计算（恒等）专家，在560B MoE模型中，每个token激活18.6B~31.3B的参数量（平均约27B）。每一层混合了512个真专家和256个零计算专家，每次选择top-12的专家，并且由于采用了PID式预算控制器，平均真专家选择数量稳定在约8个。设备级负载均衡和ScMoE保证了系统的效率。他们将该模型呈现为一个具有强大吞吐量/成本指标的非思考模型。
这种设计——添加空专家并增加top-k值，从而使每个token使用可变数量的真实专家——正是AdaMoE背后的核心思想。





太长不看版

    AdaMoE 通过向专家池中添加一组 “空专家”（null experts）（其计算开销为零），为混合专家模型（MoE）带来了Token级别的自适应专家选择能力。

在 top-k路由范式下，当一个Token被路由到“空专家”时，它实际上激活了更少的真实专家。这使得每个Token所激活的真实专家数量，在保持平均计算预算不变的前提下，实现了自适应。
我们对负载均衡损失函数（load‑balancing loss）进行了微调（将所有“空专家”视为一个聚合的计算单元），并采用了一个简单的退火策略。AdaMoE在降低了计算量（FLOPs）的同时，保持甚至提升了模型的准确率。例如，在Mixtral‑8×7B模型和ARC‑C数据集上的实验显示：计算量降低了14.55%，而准确率提升了1.69%。





为何专家选择需要Token级别的自适应？

    我们将混合专家模型的路由机制从固定的 top-k 模式转向Token级别的自适应模式，其根本原因在于一个关键的观察：并非所有Token在计算上都生而平等。在一段文本中，不同Token所蕴含的信息量和处理的复杂度存在巨大差异。传统的MoE模型强制每个Token激活固定数量的专家，这种统一的计算分配方式忽略了Token间的差异，从而导致了计算资源的低效分配。







  
	  
      
      在 SocialIQA 数据集上，不同Token的累积路由概率超过50%所需的Top专家数量分布。每一条柱状图代表Mixtral-8x7B模型中，对应MoE层内不同专家数量的Token占比。
    
  



    我们为此提供了实验性的证据。我们分析了Mixtral-8x7B（一个采用固定top-2路由的模型）的路由概率分布，并发现了两种关键模式：

大量Token的路由概率高度集中于单个专家，这表明激活第二个专家通常是多余的；
另有相当一部分Token的概率更均匀地分布在多个专家上，这意味着它们可能需要两个甚至更多专家的计算能力才能被有效处理。

这一发现有力地证明，固定的 top-k策略是次优的，它对简单的Token造成了过度计算，而对复杂的Token则可能计算不足。





借助“空专家”实现自适应路由

    AdaMoE 通过引入 “空专家”（null experts）来实现Token级别的自适应专家选择。我们将其定义为一种不执行任何操作的单元，处理Token特征所需的计算量（FLOPs）为零。在大型语言模型的实践中，常见的零计算操作包括常数零映射和恒等映射（为简化起见，我们在后续讨论中默认采用零映射作为“空专家”的实现）。
我们的机制运行如下：

在原有的 n个真实专家之外，将专家集合扩展 m个“空专家”。
略微增加路由器的 top-k 值（例如，从 2 增加到 3 或 4）。这样，每个Token选出的 top-k专家中就可能包含一部分“空专家”。
实现计算的自适应：如果 top-k专家中包含 r个“空专家”，那么该Token实际上只使用了 k-r个真实专家。
进行合理的负载均衡：在计算负载均衡损失时，我们将所有“空专家”聚合为单一的计算单元（因为没有必要在完全等价的“空专家”之间强制实现均衡）。
在 top-k选择之后，仅对真实专家进行归一化，以确保输出的尺度与标准的MoE模型保持一致。









  
	  
      
      固定的 top-2 路由与 AdaMoE 对比。左图：标准的 top-2 路由，每个Token精确激活2个真实专家。右图：AdaMoE，从4个真实专家和5个“空专家”中选择 top-4，使得某些Token激活了3个真实专家，而另一些只激活了1个。
    
  


主要实验结果

    在对 Mixtral‑8×7B 进行微调时，AdaMoE 在多个基准测试中，既降低了FFN层的计算量（FLOPs），又保持甚至提升了模型的准确率。例如，在 ARC‑Challenge 数据集上，总FLOPs下降了约14.55%，而准确率提升了1.69%。同时，每层每个Token激活的平均专家数从 2.0 降至约 1.67。







  
	  
      
      AdaMoE在多个数据集上的性能表现，展示了准确率（Acc.）、FLOPs降低百分比（%FLOPs）和平均专家负载（Load）的对比。
    
  


从论文走向实践

    我们备受鼓舞地看到，“空专家”这一概念并非仅仅停留在理论层面，它已被业界前沿的大型语言模型所采纳和实现。LongCat-Flash的技术报告中，就将 “零计算专家”（zero-computation experts）列为一项关键的架构创新，并引用了我们的论文。"><meta property="og:type" content="article"><meta property="og:url" content="https://zhijie-group.github.io/blogs/adamoe_zh/"><meta property="og:image" content="https://zhijie-group.github.io/blogs/adamoe/image/vs.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2025-09-08T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-08T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zhijie-group.github.io/blogs/adamoe/image/vs.png"><meta name=twitter:title content="AdaMoE: 借助“空专家”实现Token级别的动态路由选择"><meta name=twitter:description content="
    
    
    
        
            
        
    
    
        
            
        
    



    LongCat-Flash[1] 近期展示了一种token级别自适应的MoE的大规模部署方案。该方案利用零计算（恒等）专家，在560B MoE模型中，每个token激活18.6B~31.3B的参数量（平均约27B）。每一层混合了512个真专家和256个零计算专家，每次选择top-12的专家，并且由于采用了PID式预算控制器，平均真专家选择数量稳定在约8个。设备级负载均衡和ScMoE保证了系统的效率。他们将该模型呈现为一个具有强大吞吐量/成本指标的非思考模型。
这种设计——添加空专家并增加top-k值，从而使每个token使用可变数量的真实专家——正是AdaMoE背后的核心思想。





太长不看版

    AdaMoE 通过向专家池中添加一组 “空专家”（null experts）（其计算开销为零），为混合专家模型（MoE）带来了Token级别的自适应专家选择能力。

在 top-k路由范式下，当一个Token被路由到“空专家”时，它实际上激活了更少的真实专家。这使得每个Token所激活的真实专家数量，在保持平均计算预算不变的前提下，实现了自适应。
我们对负载均衡损失函数（load‑balancing loss）进行了微调（将所有“空专家”视为一个聚合的计算单元），并采用了一个简单的退火策略。AdaMoE在降低了计算量（FLOPs）的同时，保持甚至提升了模型的准确率。例如，在Mixtral‑8×7B模型和ARC‑C数据集上的实验显示：计算量降低了14.55%，而准确率提升了1.69%。





为何专家选择需要Token级别的自适应？

    我们将混合专家模型的路由机制从固定的 top-k 模式转向Token级别的自适应模式，其根本原因在于一个关键的观察：并非所有Token在计算上都生而平等。在一段文本中，不同Token所蕴含的信息量和处理的复杂度存在巨大差异。传统的MoE模型强制每个Token激活固定数量的专家，这种统一的计算分配方式忽略了Token间的差异，从而导致了计算资源的低效分配。







  
	  
      
      在 SocialIQA 数据集上，不同Token的累积路由概率超过50%所需的Top专家数量分布。每一条柱状图代表Mixtral-8x7B模型中，对应MoE层内不同专家数量的Token占比。
    
  



    我们为此提供了实验性的证据。我们分析了Mixtral-8x7B（一个采用固定top-2路由的模型）的路由概率分布，并发现了两种关键模式：

大量Token的路由概率高度集中于单个专家，这表明激活第二个专家通常是多余的；
另有相当一部分Token的概率更均匀地分布在多个专家上，这意味着它们可能需要两个甚至更多专家的计算能力才能被有效处理。

这一发现有力地证明，固定的 top-k策略是次优的，它对简单的Token造成了过度计算，而对复杂的Token则可能计算不足。





借助“空专家”实现自适应路由

    AdaMoE 通过引入 “空专家”（null experts）来实现Token级别的自适应专家选择。我们将其定义为一种不执行任何操作的单元，处理Token特征所需的计算量（FLOPs）为零。在大型语言模型的实践中，常见的零计算操作包括常数零映射和恒等映射（为简化起见，我们在后续讨论中默认采用零映射作为“空专家”的实现）。
我们的机制运行如下：

在原有的 n个真实专家之外，将专家集合扩展 m个“空专家”。
略微增加路由器的 top-k 值（例如，从 2 增加到 3 或 4）。这样，每个Token选出的 top-k专家中就可能包含一部分“空专家”。
实现计算的自适应：如果 top-k专家中包含 r个“空专家”，那么该Token实际上只使用了 k-r个真实专家。
进行合理的负载均衡：在计算负载均衡损失时，我们将所有“空专家”聚合为单一的计算单元（因为没有必要在完全等价的“空专家”之间强制实现均衡）。
在 top-k选择之后，仅对真实专家进行归一化，以确保输出的尺度与标准的MoE模型保持一致。









  
	  
      
      固定的 top-2 路由与 AdaMoE 对比。左图：标准的 top-2 路由，每个Token精确激活2个真实专家。右图：AdaMoE，从4个真实专家和5个“空专家”中选择 top-4，使得某些Token激活了3个真实专家，而另一些只激活了1个。
    
  


主要实验结果

    在对 Mixtral‑8×7B 进行微调时，AdaMoE 在多个基准测试中，既降低了FFN层的计算量（FLOPs），又保持甚至提升了模型的准确率。例如，在 ARC‑Challenge 数据集上，总FLOPs下降了约14.55%，而准确率提升了1.69%。同时，每层每个Token激活的平均专家数从 2.0 降至约 1.67。







  
	  
      
      AdaMoE在多个数据集上的性能表现，展示了准确率（Acc.）、FLOPs降低百分比（%FLOPs）和平均专家负载（Load）的对比。
    
  


从论文走向实践

    我们备受鼓舞地看到，“空专家”这一概念并非仅仅停留在理论层面，它已被业界前沿的大型语言模型所采纳和实现。LongCat-Flash的技术报告中，就将 “零计算专家”（zero-computation experts）列为一项关键的架构创新，并引用了我们的论文。"><meta name=twitter:site content="@sjtudenglab"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://zhijie-group.github.io/blogs/"},{"@type":"ListItem","position":2,"name":"AdaMoE: 借助“空专家”实现Token级别的动态路由选择","item":"https://zhijie-group.github.io/blogs/adamoe_zh/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AdaMoE: 借助“空专家”实现Token级别的动态路由选择","name":"AdaMoE: 借助“空专家”实现Token级别的动态路由选择","description":" LongCat-Flash[1] 近期展示了一种token级别自适应的MoE的大规模部署方案。该方案利用零计算（恒等）专家，在560B MoE模型中，每个token激活18.6B~31.3B的参数量（平均约27B）。每一层混合了512个真专家和256个零计算专家，每次选择top-12的专家，并且由于采用了PID式预算控制器，平均真专家选择数量稳定在约8个。设备级负载均衡和ScMoE保证了系统的效率。他们将该模型呈现为一个具有强大吞吐量/成本指标的非思考模型。\n这种设计——添加空专家并增加top-k值，从而使每个token使用可变数量的真实专家——正是AdaMoE背后的核心思想。\n太长不看版 AdaMoE 通过向专家池中添加一组 “空专家”（null experts）（其计算开销为零），为混合专家模型（MoE）带来了Token级别的自适应专家选择能力。\n在 top-k路由范式下，当一个Token被路由到“空专家”时，它实际上激活了更少的真实专家。这使得每个Token所激活的真实专家数量，在保持平均计算预算不变的前提下，实现了自适应。 我们对负载均衡损失函数（load‑balancing loss）进行了微调（将所有“空专家”视为一个聚合的计算单元），并采用了一个简单的退火策略。AdaMoE在降低了计算量（FLOPs）的同时，保持甚至提升了模型的准确率。例如，在Mixtral‑8×7B模型和ARC‑C数据集上的实验显示：计算量降低了14.55%，而准确率提升了1.69%。 为何专家选择需要Token级别的自适应？ 我们将混合专家模型的路由机制从固定的 top-k 模式转向Token级别的自适应模式，其根本原因在于一个关键的观察：并非所有Token在计算上都生而平等。在一段文本中，不同Token所蕴含的信息量和处理的复杂度存在巨大差异。传统的MoE模型强制每个Token激活固定数量的专家，这种统一的计算分配方式忽略了Token间的差异，从而导致了计算资源的低效分配。 在 SocialIQA 数据集上，不同Token的累积路由概率超过50%所需的Top专家数量分布。每一条柱状图代表Mixtral-8x7B模型中，对应MoE层内不同专家数量的Token占比。 我们为此提供了实验性的证据。我们分析了Mixtral-8x7B（一个采用固定top-2路由的模型）的路由概率分布，并发现了两种关键模式：\n大量Token的路由概率高度集中于单个专家，这表明激活第二个专家通常是多余的； 另有相当一部分Token的概率更均匀地分布在多个专家上，这意味着它们可能需要两个甚至更多专家的计算能力才能被有效处理。 这一发现有力地证明，固定的 top-k策略是次优的，它对简单的Token造成了过度计算，而对复杂的Token则可能计算不足。\n借助“空专家”实现自适应路由 AdaMoE 通过引入 “空专家”（null experts）来实现Token级别的自适应专家选择。我们将其定义为一种不执行任何操作的单元，处理Token特征所需的计算量（FLOPs）为零。在大型语言模型的实践中，常见的零计算操作包括常数零映射和恒等映射（为简化起见，我们在后续讨论中默认采用零映射作为“空专家”的实现）。\n我们的机制运行如下：\n在原有的 n个真实专家之外，将专家集合扩展 m个“空专家”。 略微增加路由器的 top-k 值（例如，从 2 增加到 3 或 4）。这样，每个Token选出的 top-k专家中就可能包含一部分“空专家”。 实现计算的自适应：如果 top-k专家中包含 r个“空专家”，那么该Token实际上只使用了 k-r个真实专家。 进行合理的负载均衡：在计算负载均衡损失时，我们将所有“空专家”聚合为单一的计算单元（因为没有必要在完全等价的“空专家”之间强制实现均衡）。 在 top-k选择之后，仅对真实专家进行归一化，以确保输出的尺度与标准的MoE模型保持一致。 固定的 top-2 路由与 AdaMoE 对比。左图：标准的 top-2 路由，每个Token精确激活2个真实专家。右图：AdaMoE，从4个真实专家和5个“空专家”中选择 top-4，使得某些Token激活了3个真实专家，而另一些只激活了1个。 主要实验结果 在对 Mixtral‑8×7B 进行微调时，AdaMoE 在多个基准测试中，既降低了FFN层的计算量（FLOPs），又保持甚至提升了模型的准确率。例如，在 ARC‑Challenge 数据集上，总FLOPs下降了约14.55%，而准确率提升了1.69%。同时，每层每个Token激活的平均专家数从 2.0 降至约 1.67。 AdaMoE在多个数据集上的性能表现，展示了准确率（Acc.）、FLOPs降低百分比（%FLOPs）和平均专家负载（Load）的对比。 从论文走向实践 我们备受鼓舞地看到，“空专家”这一概念并非仅仅停留在理论层面，它已被业界前沿的大型语言模型所采纳和实现。LongCat-Flash的技术报告中，就将 “零计算专家”（zero-computation experts）列为一项关键的架构创新，并引用了我们的论文。\n","keywords":[],"articleBody":" LongCat-Flash[1] 近期展示了一种token级别自适应的MoE的大规模部署方案。该方案利用零计算（恒等）专家，在560B MoE模型中，每个token激活18.6B~31.3B的参数量（平均约27B）。每一层混合了512个真专家和256个零计算专家，每次选择top-12的专家，并且由于采用了PID式预算控制器，平均真专家选择数量稳定在约8个。设备级负载均衡和ScMoE保证了系统的效率。他们将该模型呈现为一个具有强大吞吐量/成本指标的非思考模型。\n这种设计——添加空专家并增加top-k值，从而使每个token使用可变数量的真实专家——正是AdaMoE背后的核心思想。\n太长不看版 AdaMoE 通过向专家池中添加一组 “空专家”（null experts）（其计算开销为零），为混合专家模型（MoE）带来了Token级别的自适应专家选择能力。\n在 top-k路由范式下，当一个Token被路由到“空专家”时，它实际上激活了更少的真实专家。这使得每个Token所激活的真实专家数量，在保持平均计算预算不变的前提下，实现了自适应。 我们对负载均衡损失函数（load‑balancing loss）进行了微调（将所有“空专家”视为一个聚合的计算单元），并采用了一个简单的退火策略。AdaMoE在降低了计算量（FLOPs）的同时，保持甚至提升了模型的准确率。例如，在Mixtral‑8×7B模型和ARC‑C数据集上的实验显示：计算量降低了14.55%，而准确率提升了1.69%。 为何专家选择需要Token级别的自适应？ 我们将混合专家模型的路由机制从固定的 top-k 模式转向Token级别的自适应模式，其根本原因在于一个关键的观察：并非所有Token在计算上都生而平等。在一段文本中，不同Token所蕴含的信息量和处理的复杂度存在巨大差异。传统的MoE模型强制每个Token激活固定数量的专家，这种统一的计算分配方式忽略了Token间的差异，从而导致了计算资源的低效分配。 在 SocialIQA 数据集上，不同Token的累积路由概率超过50%所需的Top专家数量分布。每一条柱状图代表Mixtral-8x7B模型中，对应MoE层内不同专家数量的Token占比。 我们为此提供了实验性的证据。我们分析了Mixtral-8x7B（一个采用固定top-2路由的模型）的路由概率分布，并发现了两种关键模式：\n大量Token的路由概率高度集中于单个专家，这表明激活第二个专家通常是多余的； 另有相当一部分Token的概率更均匀地分布在多个专家上，这意味着它们可能需要两个甚至更多专家的计算能力才能被有效处理。 这一发现有力地证明，固定的 top-k策略是次优的，它对简单的Token造成了过度计算，而对复杂的Token则可能计算不足。\n借助“空专家”实现自适应路由 AdaMoE 通过引入 “空专家”（null experts）来实现Token级别的自适应专家选择。我们将其定义为一种不执行任何操作的单元，处理Token特征所需的计算量（FLOPs）为零。在大型语言模型的实践中，常见的零计算操作包括常数零映射和恒等映射（为简化起见，我们在后续讨论中默认采用零映射作为“空专家”的实现）。\n我们的机制运行如下：\n在原有的 n个真实专家之外，将专家集合扩展 m个“空专家”。 略微增加路由器的 top-k 值（例如，从 2 增加到 3 或 4）。这样，每个Token选出的 top-k专家中就可能包含一部分“空专家”。 实现计算的自适应：如果 top-k专家中包含 r个“空专家”，那么该Token实际上只使用了 k-r个真实专家。 进行合理的负载均衡：在计算负载均衡损失时，我们将所有“空专家”聚合为单一的计算单元（因为没有必要在完全等价的“空专家”之间强制实现均衡）。 在 top-k选择之后，仅对真实专家进行归一化，以确保输出的尺度与标准的MoE模型保持一致。 固定的 top-2 路由与 AdaMoE 对比。左图：标准的 top-2 路由，每个Token精确激活2个真实专家。右图：AdaMoE，从4个真实专家和5个“空专家”中选择 top-4，使得某些Token激活了3个真实专家，而另一些只激活了1个。 主要实验结果 在对 Mixtral‑8×7B 进行微调时，AdaMoE 在多个基准测试中，既降低了FFN层的计算量（FLOPs），又保持甚至提升了模型的准确率。例如，在 ARC‑Challenge 数据集上，总FLOPs下降了约14.55%，而准确率提升了1.69%。同时，每层每个Token激活的平均专家数从 2.0 降至约 1.67。 AdaMoE在多个数据集上的性能表现，展示了准确率（Acc.）、FLOPs降低百分比（%FLOPs）和平均专家负载（Load）的对比。 从论文走向实践 我们备受鼓舞地看到，“空专家”这一概念并非仅仅停留在理论层面，它已被业界前沿的大型语言模型所采纳和实现。LongCat-Flash的技术报告中，就将 “零计算专家”（zero-computation experts）列为一项关键的架构创新，并引用了我们的论文。\n该报告解释说，这一机制使得模型能够“根据Token的重要性为其分配动态的计算预算”，根据上下文的不同，为每个Token激活动态变化的参数。这一直接的工业应用，彰显了我们提出的自适应路由策略在现实世界中的实用性和可扩展性。\n此外，LongCat-Flash 还引入了多种优化技术，以应对自适应专家选择所带来的通信和负载均衡挑战——这进一步证明了我们的方法在大型系统中的可行性。\n引用我们的工作 如果您认为我们的工作对您有所启发，欢迎引用我们的论文：\n@inproceedings{zeng-etal-2024-adamoe, title = \"AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models\", author = \"Zeng, Zihao and Miao, Yibo and Gao, Hongcheng and Zhang, Hao and Deng, Zhijie\", editor = \"Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung\", booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2024\", month = nov, year = \"2024\", address = \"Miami, Florida, USA\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2024.findings-emnlp.361/\", doi = \"10.18653/v1/2024.findings-emnlp.361\", pages = \"6223--6235\" } 参考文献 [1] Team, Meituan LongCat, et al. “LongCat-Flash Technical Report.” arXiv preprint arXiv:2509.01322 (2025).\n","wordCount":"169","inLanguage":"en","image":"https://zhijie-group.github.io/blogs/adamoe/image/vs.png","datePublished":"2025-09-08T00:00:00Z","dateModified":"2025-09-08T00:00:00Z","author":{"@type":"Person","name":"Zihao Zeng*, Yibo Miao*, Hongcheng Gao, Hao Zhang, Zhijie Deng†"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zhijie-group.github.io/blogs/adamoe_zh/"},"publisher":{"@type":"Organization","name":"DENG Lab @ SJTU","logo":{"@type":"ImageObject","url":"https://zhijie-group.github.io/favicon.ico"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://zhijie-group.github.io/ accesskey=h title="DENG Lab @ SJTU (Alt + H)"><img id=logo-image src=/img/logo-new.png alt="DENG Lab @ SJTU" height=40></a><div class=logo-switches><ul class=lang-switch></ul></div></div><ul id=menu><div style=margin-right:15px><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div><li><a href=https://zhijie-group.github.io/home/ title=Home><span>Home</span></a></li><li><a href=https://zhijie-group.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://zhijie-group.github.io/people/ title=People><span>People</span></a></li><li><a href=https://zhijie-group.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://zhijie-group.github.io/contact/ title=Contact><span>Contact</span></a></li><li><a href=https://x.com/sjtudenglab title=Twitter><i class="fab fa-twitter fa-lg"></i></a></li><li><a href=https://github.com/zhijie-group title=GitHub><i class="fab fa-github fa-lg"></i></a></li><li><a href=https://www.zhihu.com/people/SJTUDengLab title=Zhihu><span>Zhihu</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><script>function updateThemeAndLogo(){document.body.classList.toggle("dark");const e=document.body.classList.contains("dark"),n=document.getElementById("logo-image"),t=e?"dark":"light";localStorage.setItem("pref-theme",t)}function applyCurrentThemeAndLogo(){const e=document.body.classList.contains("dark"),t=document.getElementById("logo-image")}document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("theme-toggle");e&&e.addEventListener("click",updateThemeAndLogo);const t=localStorage.getItem("pref-theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");document.body.classList.toggle("dark",t==="dark")});const themeToggleButton=document.getElementById("theme-toggle");themeToggleButton&&(themeToggleButton.addEventListener("click",()=>{updateThemeAndLogo()}),applyCurrentThemeAndLogo())</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-T4DXGLCH1D"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-T4DXGLCH1D")</script><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">AdaMoE: 借助“空专家”实现Token级别的动态路由选择</h1><div class=post-meta><span title='2025-09-08 00:00:00 +0000 UTC'>September 8, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Zihao Zeng*, Yibo Miao*, Hongcheng Gao, Hao Zhang, Zhijie Deng†</div></header><figure class=entry-cover><img loading=eager src=https://zhijie-group.github.io/blogs/adamoe/image/vs.png alt=AdaMoE><p><strong>左图</strong>: 标准的 top-2 MoE。 <strong>右图</strong>: 引入了“空专家”的AdaMoE模型。</p></figure><div class=post-content><div style=padding-left:30px;display:flex!important;gap:60px!important><a href=https://arxiv.org/pdf/2406.13233 target=_blank rel="noopener noreferrer"><img src="https://img.shields.io/badge/📃 Paper Link--1abc9c.svg?style=social" style=transform:scale(1.5)>
</a><a href=https://github.com/zhijie-group/AdaMoE target=_blank rel="noopener noreferrer"><img src="https://img.shields.io/badge/💻 Code Link--1abc9c.svg?style=social" style=transform:scale(1.5)></a></div><div style=text-align:left><p>LongCat-Flash[1] 近期展示了一种token级别自适应的MoE的大规模部署方案。该方案利用零计算（恒等）专家，在560B MoE模型中，每个token激活18.6B~31.3B的参数量（平均约27B）。每一层混合了512个真专家和256个零计算专家，每次选择top-12的专家，并且由于采用了PID式预算控制器，平均真专家选择数量稳定在约8个。设备级负载均衡和ScMoE保证了系统的效率。他们将该模型呈现为一个具有强大吞吐量/成本指标的非思考模型。</p><p>这种设计——添加空专家并增加top-k值，从而使每个token使用可变数量的真实专家——正是AdaMoE背后的核心思想。</p></div><h2 id=太长不看版>太长不看版<a hidden class=anchor aria-hidden=true href=#太长不看版>#</a></h2><div style=text-align:left><p>AdaMoE 通过向专家池中添加一组 <strong>“空专家”</strong>（null experts）（其计算开销为零），为混合专家模型（MoE）带来了Token级别的自适应专家选择能力。</p><ul><li>在 <code>top-k</code>路由范式下，当一个Token被路由到“空专家”时，它实际上激活了更少的真实专家。这使得<strong>每个Token所激活的真实专家数量，在保持平均计算预算不变的前提下，实现了自适应</strong>。</li><li>我们对负载均衡损失函数（load‑balancing loss）进行了微调（将所有“空专家”视为一个聚合的计算单元），并采用了一个简单的退火策略。AdaMoE在降低了计算量（FLOPs）的同时，保持甚至提升了模型的准确率。例如，在Mixtral‑8×7B模型和ARC‑C数据集上的实验显示：计算量降低了14.55%，而准确率提升了1.69%。</li></ul></div><h2 id=为何专家选择需要token级别的自适应>为何专家选择需要Token级别的自适应？<a hidden class=anchor aria-hidden=true href=#为何专家选择需要token级别的自适应>#</a></h2><div style=text-align:left>我们将混合专家模型的路由机制从固定的 <strong><code>top-k</code></strong> 模式转向Token级别的自适应模式，其根本原因在于一个关键的观察：<strong>并非所有Token在计算上都生而平等</strong>。在一段文本中，不同Token所蕴含的信息量和处理的复杂度存在巨大差异。传统的MoE模型强制每个Token激活固定数量的专家，这种统一的计算分配方式忽略了Token间的差异，从而导致了计算资源的低效分配。</div><style>.gray-text{color:gray}</style><figure><div style=display:grid;place-items:center><img src=image/pre.png alt=Pre-Experiment style=width:55%;height:auto><figcaption style=font-size:16px class=gray-text>在 SocialIQA 数据集上，不同Token的累积路由概率超过50%所需的Top专家数量分布。每一条柱状图代表Mixtral-8x7B模型中，对应MoE层内不同专家数量的Token占比。</figcaption></div></figure><div style=text-align:left><p>我们为此提供了实验性的证据。我们分析了Mixtral-8x7B（一个采用固定top-2路由的模型）的路由概率分布，并发现了两种关键模式：</p><ol><li>大量Token的路由概率高度集中于单个专家，这表明激活第二个专家通常是多余的；</li><li>另有相当一部分Token的概率更均匀地分布在多个专家上，这意味着它们可能需要两个甚至更多专家的计算能力才能被有效处理。</li></ol><p>这一发现有力地证明，固定的 <code>top-k</code>策略是次优的，它对简单的Token造成了过度计算，而对复杂的Token则可能计算不足。</p></div><h2 id=借助空专家实现自适应路由>借助“空专家”实现自适应路由<a hidden class=anchor aria-hidden=true href=#借助空专家实现自适应路由>#</a></h2><div style=text-align:left><p>AdaMoE 通过引入 <strong>“空专家”</strong>（null experts）来实现Token级别的自适应专家选择。我们将其定义为一种不执行任何操作的单元，处理Token特征所需的<strong>计算量（FLOPs）为零</strong>。在大型语言模型的实践中，常见的零计算操作包括常数零映射和恒等映射（为简化起见，我们在后续讨论中默认采用零映射作为“空专家”的实现）。</p><p>我们的机制运行如下：</p><ol><li>在原有的 <code>n</code>个真实专家之外，将<strong>专家集合扩展</strong> <code>m</code>个“空专家”。</li><li><strong>略微增加</strong>路由器的 <strong><code>top-k</code></strong> 值（例如，从 2 增加到 3 或 4）。这样，每个Token选出的 <code>top-k</code>专家中就可能包含一部分“空专家”。</li><li><strong>实现计算的自适应</strong>：如果 <code>top-k</code>专家中包含 <code>r</code>个“空专家”，那么该Token实际上只使用了 <code>k-r</code>个真实专家。</li><li><strong>进行合理的负载均衡</strong>：在计算负载均衡损失时，我们将<strong>所有“空专家”聚合为单一的计算单元</strong>（因为没有必要在完全等价的“空专家”之间强制实现均衡）。</li><li>在 <code>top-k</code>选择之后，<strong>仅对真实专家进行归一化</strong>，以确保输出的尺度与标准的MoE模型保持一致。</li></ol></div><style>.gray-text{color:gray}</style><figure><div style=display:grid;place-items:center><img src=image/vs.png alt=DeepSeek style=width:85%;height:auto><figcaption style=font-size:16px class=gray-text>固定的 top-2 路由与 AdaMoE 对比。左图：标准的 top-2 路由，每个Token精确激活2个真实专家。右图：AdaMoE，从4个真实专家和5个“空专家”中选择 top-4，使得某些Token激活了3个真实专家，而另一些只激活了1个。</figcaption></div></figure><h2 id=主要实验结果>主要实验结果<a hidden class=anchor aria-hidden=true href=#主要实验结果>#</a></h2><div style=text-align:left>在对 Mixtral‑8×7B 进行微调时，AdaMoE 在多个基准测试中，既降低了FFN层的计算量（FLOPs），又保持甚至提升了模型的准确率。例如，在 ARC‑Challenge 数据集上，<strong>总FLOPs下降了约14.55%，而准确率提升了1.69%</strong>。同时，每层每个Token激活的平均专家数从 2.0 降至约 1.67。</div><style>.gray-text{color:gray}</style><figure><div style=display:grid;place-items:center><img src=image/res.png alt=venn style=width:80%;height:auto><figcaption style=font-size:16px class=gray-text>AdaMoE在多个数据集上的性能表现，展示了准确率（Acc.）、FLOPs降低百分比（%FLOPs）和平均专家负载（Load）的对比。</figcaption></div></figure><h2 id=从论文走向实践>从论文走向实践<a hidden class=anchor aria-hidden=true href=#从论文走向实践>#</a></h2><div style=text-align:left><p>我们备受鼓舞地看到，“空专家”这一概念并非仅仅停留在理论层面，它已被业界前沿的大型语言模型所采纳和实现。<strong>LongCat-Flash</strong>的技术报告中，就将 <strong>“零计算专家”</strong>（zero-computation experts）列为一项关键的架构创新，并引用了我们的论文。</p><p>该报告解释说，这一机制使得模型能够“根据Token的重要性为其分配动态的计算预算”，根据上下文的不同，为每个Token激活动态变化的参数。这一直接的工业应用，彰显了我们提出的自适应路由策略在现实世界中的实用性和可扩展性。</p><p>此外，LongCat-Flash 还引入了多种优化技术，以应对自适应专家选择所带来的通信和负载均衡挑战——这进一步证明了我们的方法在大型系统中的可行性。</p></div><h2 id=引用我们的工作>引用我们的工作<a hidden class=anchor aria-hidden=true href=#引用我们的工作>#</a></h2><p>如果您认为我们的工作对您有所启发，欢迎引用我们的论文：</p><pre tabindex=0><code>@inproceedings{zeng-etal-2024-adamoe,
    title = &#34;AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models&#34;,
    author = &#34;Zeng, Zihao  and
      Miao, Yibo  and
      Gao, Hongcheng  and
      Zhang, Hao  and
      Deng, Zhijie&#34;,
    editor = &#34;Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung&#34;,
    booktitle = &#34;Findings of the Association for Computational Linguistics: EMNLP 2024&#34;,
    month = nov,
    year = &#34;2024&#34;,
    address = &#34;Miami, Florida, USA&#34;,
    publisher = &#34;Association for Computational Linguistics&#34;,
    url = &#34;https://aclanthology.org/2024.findings-emnlp.361/&#34;,
    doi = &#34;10.18653/v1/2024.findings-emnlp.361&#34;,
    pages = &#34;6223--6235&#34;
}
</code></pre><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><p>[1] Team, Meituan LongCat, et al. &ldquo;LongCat-Flash Technical Report.&rdquo; arXiv preprint arXiv:2509.01322 (2025).</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://zhijie-group.github.io/>DENG Lab @ SJTU</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
, Adapted by Zhijie Deng</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>