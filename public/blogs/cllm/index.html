<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/custom.css">
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" rel="stylesheet">
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Consistency Large Language Models: A Family of Efficient Parallel Decoders | DENG Lab @ SJTU</title>
<meta name="keywords" content="">
<meta name="description" content="
    
        
          
        
    
    
        
          
        
    
    
    



    TL;DR: LLMs have been traditionally regarded as sequential decoders, decoding one token after another. In this blog, we show pretrained LLMs can be easily taught to operate as efficient parallel decoders. We introduce Consistency Large Language Models (CLLMs), a new family of parallel decoders capable of reducing inference latency by efficiently decoding an $n$-token sequence per inference step. Our research shows this process &ndash; mimicking human cognitive process of forming complete sentences in mind before articulating word by word &ndash; can be effectively learned by simply finetuning pretrained LLMs. Specifically, CLLMs are trained to perform parallel decoding by mapping any randomly initialized $n$-token sequence to the same result yielded by autoregressive (AR) decoding in as few steps as possible. Experiment results show CLLMs obtained using our proposed method are highly effective, showing $2.4\times$ to $3.4\times$ improvements in generation speed, in par with or even beter than other fast inference techniques like Medusa2 and Eagle, yet require no additional memory cost to accomodate auxiliary model components at inference time.







  
	  
      
      Figure 1: Demo of $\sim 3 \times$ speedup by CLLM-ABEL-7B-001 in comparison with baseline ABEL-7B-001 using Jacobi decoding on GSM8K.
    
  


Background: Jacobi Decoding

    Large language models (LLMs) are transforming the landscape of human lives, from programming to offering legal and health advice. However, during inference, LLMs generate responses token by token using AR decoding as shown in Figure 1, leading to high latency for longer responses. Using AR decoding, it often necessitates architectural modifications, auxiliary components, or draft models, to speed up inference by generating more than one token at a time.







  
	  
      
      Figure 2: illustration of conventional AR decoding: one token is generated at a time.
    
  



    Jacobi decoding originates from the Jacobi and Gauss-Seidel fixed-point iteration for solving nonlinear equations, and is proven identical to AR generation using greedy decoding. Jacobi decoding reformulates the sequential generation process into a system of $n$ non-linear equations with $n$ variables solvable in parallel based on Jacobi iteration. Each iteration step might predict more than one correct token (By &ldquo;correct&rdquo;, we mean alignment with the AR decoding
result under a greedy sampling strategy), thereby accelerating AR decoding potentially.







  
	  
      
      Figure 3: illustration of Jacobi decoding: $n$-token sequence is fed into the LLM and iterates until convergence.
    
  



    To be specific, Jacobi decoding method first randomly guesses the next $n$ tokens in a sequence (referred to as $n$-token sequence hereinafter unless specified otherwise) from an input prompt. The $n$-token sequence, along with the prompt, is then fed to the LLM to iteratively update itself. This process continues until the $n$-token sequence stabilizes and no further changes occur, reaching a fixed point. Notably, Jacobi decoding requires no more queries to the LLM than auto-regressive (AR) decoding. Eventually, the $n$-token sequence converges to the output that would be generated by AR decoding under a greedy strategy. This progression from an initial random guess to the final AR generation outcome traces what is known as a Jacobi trajectory. An instance of Jacobi decoding iteration process and Jacobi trajectory is illustrated in Figure 2.




Limitations of Jacobi Decoding

    However, vanilla Jacobi decoding for LLMs shows only marginal speedup over AR decoding in practice, e.g., an average of $1.05\times$ speedup. This is because an AR-trained LLM can rarely yield a correct token when there are incorrections in its preceding tokens. Thereby, most Jacobi iterations gain only one correction for the $n$-token sequence, resulting in a longer trajectory as illustrated on the left side of Figure 3.">
<meta name="author" content="Siqi Kou*, Lanxiang Hu*, Zhezhi He, Zhijie Deng†, Hao Zhang">
<link rel="canonical" href="http://localhost:1313/blogs/cllm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.c0b173c8753dc0cb5a0e3b2c0cd5a4fa0869fbd82e03d31553bf8232b3914c61.css" integrity="sha256-wLFzyHU9wMtaDjssDNWk&#43;ghp&#43;9guA9MVU7&#43;CMrORTGE=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/cllm/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(255, 255, 255);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-T4DXGLCH1D"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-T4DXGLCH1D');
        }
      </script><meta property="og:title" content="Consistency Large Language Models: A Family of Efficient Parallel Decoders" />
<meta property="og:description" content="
    
        
          
        
    
    
        
          
        
    
    
    



    TL;DR: LLMs have been traditionally regarded as sequential decoders, decoding one token after another. In this blog, we show pretrained LLMs can be easily taught to operate as efficient parallel decoders. We introduce Consistency Large Language Models (CLLMs), a new family of parallel decoders capable of reducing inference latency by efficiently decoding an $n$-token sequence per inference step. Our research shows this process &ndash; mimicking human cognitive process of forming complete sentences in mind before articulating word by word &ndash; can be effectively learned by simply finetuning pretrained LLMs. Specifically, CLLMs are trained to perform parallel decoding by mapping any randomly initialized $n$-token sequence to the same result yielded by autoregressive (AR) decoding in as few steps as possible. Experiment results show CLLMs obtained using our proposed method are highly effective, showing $2.4\times$ to $3.4\times$ improvements in generation speed, in par with or even beter than other fast inference techniques like Medusa2 and Eagle, yet require no additional memory cost to accomodate auxiliary model components at inference time.







  
	  
      
      Figure 1: Demo of $\sim 3 \times$ speedup by CLLM-ABEL-7B-001 in comparison with baseline ABEL-7B-001 using Jacobi decoding on GSM8K.
    
  


Background: Jacobi Decoding

    Large language models (LLMs) are transforming the landscape of human lives, from programming to offering legal and health advice. However, during inference, LLMs generate responses token by token using AR decoding as shown in Figure 1, leading to high latency for longer responses. Using AR decoding, it often necessitates architectural modifications, auxiliary components, or draft models, to speed up inference by generating more than one token at a time.







  
	  
      
      Figure 2: illustration of conventional AR decoding: one token is generated at a time.
    
  



    Jacobi decoding originates from the Jacobi and Gauss-Seidel fixed-point iteration for solving nonlinear equations, and is proven identical to AR generation using greedy decoding. Jacobi decoding reformulates the sequential generation process into a system of $n$ non-linear equations with $n$ variables solvable in parallel based on Jacobi iteration. Each iteration step might predict more than one correct token (By &ldquo;correct&rdquo;, we mean alignment with the AR decoding
result under a greedy sampling strategy), thereby accelerating AR decoding potentially.







  
	  
      
      Figure 3: illustration of Jacobi decoding: $n$-token sequence is fed into the LLM and iterates until convergence.
    
  



    To be specific, Jacobi decoding method first randomly guesses the next $n$ tokens in a sequence (referred to as $n$-token sequence hereinafter unless specified otherwise) from an input prompt. The $n$-token sequence, along with the prompt, is then fed to the LLM to iteratively update itself. This process continues until the $n$-token sequence stabilizes and no further changes occur, reaching a fixed point. Notably, Jacobi decoding requires no more queries to the LLM than auto-regressive (AR) decoding. Eventually, the $n$-token sequence converges to the output that would be generated by AR decoding under a greedy strategy. This progression from an initial random guess to the final AR generation outcome traces what is known as a Jacobi trajectory. An instance of Jacobi decoding iteration process and Jacobi trajectory is illustrated in Figure 2.




Limitations of Jacobi Decoding

    However, vanilla Jacobi decoding for LLMs shows only marginal speedup over AR decoding in practice, e.g., an average of $1.05\times$ speedup. This is because an AR-trained LLM can rarely yield a correct token when there are incorrections in its preceding tokens. Thereby, most Jacobi iterations gain only one correction for the $n$-token sequence, resulting in a longer trajectory as illustrated on the left side of Figure 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/blogs/cllm/" />
<meta property="og:image" content="http://localhost:1313/img/objective_illustration_global.jpg" /><meta property="article:section" content="blogs" />
<meta property="article:published_time" content="2024-05-06T12:00:00-08:00" />
<meta property="article:modified_time" content="2024-05-06T12:00:00-08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/img/objective_illustration_global.jpg" />
<meta name="twitter:title" content="Consistency Large Language Models: A Family of Efficient Parallel Decoders"/>
<meta name="twitter:description" content="
    
        
          
        
    
    
        
          
        
    
    
    



    TL;DR: LLMs have been traditionally regarded as sequential decoders, decoding one token after another. In this blog, we show pretrained LLMs can be easily taught to operate as efficient parallel decoders. We introduce Consistency Large Language Models (CLLMs), a new family of parallel decoders capable of reducing inference latency by efficiently decoding an $n$-token sequence per inference step. Our research shows this process &ndash; mimicking human cognitive process of forming complete sentences in mind before articulating word by word &ndash; can be effectively learned by simply finetuning pretrained LLMs. Specifically, CLLMs are trained to perform parallel decoding by mapping any randomly initialized $n$-token sequence to the same result yielded by autoregressive (AR) decoding in as few steps as possible. Experiment results show CLLMs obtained using our proposed method are highly effective, showing $2.4\times$ to $3.4\times$ improvements in generation speed, in par with or even beter than other fast inference techniques like Medusa2 and Eagle, yet require no additional memory cost to accomodate auxiliary model components at inference time.







  
	  
      
      Figure 1: Demo of $\sim 3 \times$ speedup by CLLM-ABEL-7B-001 in comparison with baseline ABEL-7B-001 using Jacobi decoding on GSM8K.
    
  


Background: Jacobi Decoding

    Large language models (LLMs) are transforming the landscape of human lives, from programming to offering legal and health advice. However, during inference, LLMs generate responses token by token using AR decoding as shown in Figure 1, leading to high latency for longer responses. Using AR decoding, it often necessitates architectural modifications, auxiliary components, or draft models, to speed up inference by generating more than one token at a time.







  
	  
      
      Figure 2: illustration of conventional AR decoding: one token is generated at a time.
    
  



    Jacobi decoding originates from the Jacobi and Gauss-Seidel fixed-point iteration for solving nonlinear equations, and is proven identical to AR generation using greedy decoding. Jacobi decoding reformulates the sequential generation process into a system of $n$ non-linear equations with $n$ variables solvable in parallel based on Jacobi iteration. Each iteration step might predict more than one correct token (By &ldquo;correct&rdquo;, we mean alignment with the AR decoding
result under a greedy sampling strategy), thereby accelerating AR decoding potentially.







  
	  
      
      Figure 3: illustration of Jacobi decoding: $n$-token sequence is fed into the LLM and iterates until convergence.
    
  



    To be specific, Jacobi decoding method first randomly guesses the next $n$ tokens in a sequence (referred to as $n$-token sequence hereinafter unless specified otherwise) from an input prompt. The $n$-token sequence, along with the prompt, is then fed to the LLM to iteratively update itself. This process continues until the $n$-token sequence stabilizes and no further changes occur, reaching a fixed point. Notably, Jacobi decoding requires no more queries to the LLM than auto-regressive (AR) decoding. Eventually, the $n$-token sequence converges to the output that would be generated by AR decoding under a greedy strategy. This progression from an initial random guess to the final AR generation outcome traces what is known as a Jacobi trajectory. An instance of Jacobi decoding iteration process and Jacobi trajectory is illustrated in Figure 2.




Limitations of Jacobi Decoding

    However, vanilla Jacobi decoding for LLMs shows only marginal speedup over AR decoding in practice, e.g., an average of $1.05\times$ speedup. This is because an AR-trained LLM can rarely yield a correct token when there are incorrections in its preceding tokens. Thereby, most Jacobi iterations gain only one correction for the $n$-token sequence, resulting in a longer trajectory as illustrated on the left side of Figure 3."/>
<meta name="twitter:site" content="@sjtudenglab"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Consistency Large Language Models: A Family of Efficient Parallel Decoders",
      "item": "http://localhost:1313/blogs/cllm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Consistency Large Language Models: A Family of Efficient Parallel Decoders",
  "name": "Consistency Large Language Models: A Family of Efficient Parallel Decoders",
  "description": " TL;DR: LLMs have been traditionally regarded as sequential decoders, decoding one token after another. In this blog, we show pretrained LLMs can be easily taught to operate as efficient parallel decoders. We introduce Consistency Large Language Models (CLLMs), a new family of parallel decoders capable of reducing inference latency by efficiently decoding an $n$-token sequence per inference step. Our research shows this process \u0026ndash; mimicking human cognitive process of forming complete sentences in mind before articulating word by word \u0026ndash; can be effectively learned by simply finetuning pretrained LLMs. Specifically, CLLMs are trained to perform parallel decoding by mapping any randomly initialized $n$-token sequence to the same result yielded by autoregressive (AR) decoding in as few steps as possible. Experiment results show CLLMs obtained using our proposed method are highly effective, showing $2.4\\times$ to $3.4\\times$ improvements in generation speed, in par with or even beter than other fast inference techniques like Medusa2 and Eagle, yet require no additional memory cost to accomodate auxiliary model components at inference time. Figure 1: Demo of $\\sim 3 \\times$ speedup by CLLM-ABEL-7B-001 in comparison with baseline ABEL-7B-001 using Jacobi decoding on GSM8K. Background: Jacobi Decoding Large language models (LLMs) are transforming the landscape of human lives, from programming to offering legal and health advice. However, during inference, LLMs generate responses token by token using AR decoding as shown in Figure 1, leading to high latency for longer responses. Using AR decoding, it often necessitates architectural modifications, auxiliary components, or draft models, to speed up inference by generating more than one token at a time. Figure 2: illustration of conventional AR decoding: one token is generated at a time. Jacobi decoding originates from the Jacobi and Gauss-Seidel fixed-point iteration for solving nonlinear equations, and is proven identical to AR generation using greedy decoding. Jacobi decoding reformulates the sequential generation process into a system of $n$ non-linear equations with $n$ variables solvable in parallel based on Jacobi iteration. Each iteration step might predict more than one correct token (By \u0026ldquo;correct\u0026rdquo;, we mean alignment with the AR decoding result under a greedy sampling strategy), thereby accelerating AR decoding potentially. Figure 3: illustration of Jacobi decoding: $n$-token sequence is fed into the LLM and iterates until convergence. To be specific, Jacobi decoding method first randomly guesses the next $n$ tokens in a sequence (referred to as $n$-token sequence hereinafter unless specified otherwise) from an input prompt. The $n$-token sequence, along with the prompt, is then fed to the LLM to iteratively update itself. This process continues until the $n$-token sequence stabilizes and no further changes occur, reaching a fixed point. Notably, Jacobi decoding requires no more queries to the LLM than auto-regressive (AR) decoding. Eventually, the $n$-token sequence converges to the output that would be generated by AR decoding under a greedy strategy. This progression from an initial random guess to the final AR generation outcome traces what is known as a Jacobi trajectory. An instance of Jacobi decoding iteration process and Jacobi trajectory is illustrated in Figure 2. Limitations of Jacobi Decoding However, vanilla Jacobi decoding for LLMs shows only marginal speedup over AR decoding in practice, e.g., an average of $1.05\\times$ speedup. This is because an AR-trained LLM can rarely yield a correct token when there are incorrections in its preceding tokens. Thereby, most Jacobi iterations gain only one correction for the $n$-token sequence, resulting in a longer trajectory as illustrated on the left side of Figure 3.\n",
  "keywords": [
    
  ],
  "articleBody": " TL;DR: LLMs have been traditionally regarded as sequential decoders, decoding one token after another. In this blog, we show pretrained LLMs can be easily taught to operate as efficient parallel decoders. We introduce Consistency Large Language Models (CLLMs), a new family of parallel decoders capable of reducing inference latency by efficiently decoding an $n$-token sequence per inference step. Our research shows this process – mimicking human cognitive process of forming complete sentences in mind before articulating word by word – can be effectively learned by simply finetuning pretrained LLMs. Specifically, CLLMs are trained to perform parallel decoding by mapping any randomly initialized $n$-token sequence to the same result yielded by autoregressive (AR) decoding in as few steps as possible. Experiment results show CLLMs obtained using our proposed method are highly effective, showing $2.4\\times$ to $3.4\\times$ improvements in generation speed, in par with or even beter than other fast inference techniques like Medusa2 and Eagle, yet require no additional memory cost to accomodate auxiliary model components at inference time. Figure 1: Demo of $\\sim 3 \\times$ speedup by CLLM-ABEL-7B-001 in comparison with baseline ABEL-7B-001 using Jacobi decoding on GSM8K. Background: Jacobi Decoding Large language models (LLMs) are transforming the landscape of human lives, from programming to offering legal and health advice. However, during inference, LLMs generate responses token by token using AR decoding as shown in Figure 1, leading to high latency for longer responses. Using AR decoding, it often necessitates architectural modifications, auxiliary components, or draft models, to speed up inference by generating more than one token at a time. Figure 2: illustration of conventional AR decoding: one token is generated at a time. Jacobi decoding originates from the Jacobi and Gauss-Seidel fixed-point iteration for solving nonlinear equations, and is proven identical to AR generation using greedy decoding. Jacobi decoding reformulates the sequential generation process into a system of $n$ non-linear equations with $n$ variables solvable in parallel based on Jacobi iteration. Each iteration step might predict more than one correct token (By “correct”, we mean alignment with the AR decoding result under a greedy sampling strategy), thereby accelerating AR decoding potentially. Figure 3: illustration of Jacobi decoding: $n$-token sequence is fed into the LLM and iterates until convergence. To be specific, Jacobi decoding method first randomly guesses the next $n$ tokens in a sequence (referred to as $n$-token sequence hereinafter unless specified otherwise) from an input prompt. The $n$-token sequence, along with the prompt, is then fed to the LLM to iteratively update itself. This process continues until the $n$-token sequence stabilizes and no further changes occur, reaching a fixed point. Notably, Jacobi decoding requires no more queries to the LLM than auto-regressive (AR) decoding. Eventually, the $n$-token sequence converges to the output that would be generated by AR decoding under a greedy strategy. This progression from an initial random guess to the final AR generation outcome traces what is known as a Jacobi trajectory. An instance of Jacobi decoding iteration process and Jacobi trajectory is illustrated in Figure 2. Limitations of Jacobi Decoding However, vanilla Jacobi decoding for LLMs shows only marginal speedup over AR decoding in practice, e.g., an average of $1.05\\times$ speedup. This is because an AR-trained LLM can rarely yield a correct token when there are incorrections in its preceding tokens. Thereby, most Jacobi iterations gain only one correction for the $n$-token sequence, resulting in a longer trajectory as illustrated on the left side of Figure 3.\nLookahead decoding and speculative decoding methods try to mitigate inefficiency in Jacobi decoding and conventional AR decoding, but incurs extra memory cost during inference time. While CLLMs require none.\nConsistency LLMs (CLLMs) Jacobi Decoding Preliminary Given a prompt $\\mathbf x$ and a pre-trained LLM $p(\\cdot| \\mathbf x)$, LLM typically generates with the standard AR decoding method under the greedy strategy, i.e. $$ \\begin{align} y_i = \\underset{y}{\\text{arg max }} p(y | \\mathbf {y}_{:i}, \\mathbf x) \\ \\ \\text{for}\\,\\, i = 1,\\dots,n \\end{align} $$ Jacobi decoding re-frames the LLM inference process as solving a system of nonlinear equations to transform the decoding process into a parallelizable computation. Consider, $f(y_i, \\mathbf y_{:i}, \\mathbf x):= y_i- \\underset{y}{\\text{arg max }} p(y | \\mathbf y_{:i}, \\mathbf x)$, we can rewrite the above equation as a system of nonlinear equations: $$ \\begin{align} f(y_i, \\mathbf y_{:i}, \\mathbf x) = 0 \\ \\ \\text{for} \\quad i = 1,\\dots,n \\Longrightarrow \\begin{cases} y_{1}^{(j+1)} \u0026= \\underset{y}{\\text{arg max}} \\ \\ p(y | \\mathbf x) \\\\ y_{2}^{(j+1)} \u0026= \\underset{y}{\\text{arg max}} \\ \\ p(y | \\mathbf y_{1}^{(j)}, \\mathbf x) \\\\ \u0026 \\vdots \\\\ y_{n}^{(j+1)} \u0026= \\underset{y}{\\text{arg max}} \\ \\ p(y | \\mathbf y_{:n}^{(j)}, \\mathbf x) \\end{cases} \\end{align} $$ Note that the process exits at some k such that $\\mathbf y^{(k)} = \\mathbf y^{(k−1)}$ and we define $\\mathbf y^{∗} := \\mathbf y^{(k)}$ as the fixed point, and $\\mathcal J := \\set{ \\mathbf y^{(1)}, \\dots, \\mathbf y^{(k)} }$ as the Jacobi trajectory. Training with Jacobi Trajectories To address this, we propose adapting pre-trained LLMs so that they can consistently map any point $\\mathbf y$ on the Jacobi trajectory $\\mathcal{J}$ to the fixed point $\\mathbf y^*$. Surprisingly, we find such an objective is analogous to that of consistency models, a leading acceleration approach for diffusion models. In our proposed method, we use Jacobi trajectories collected from a target model to train the model with a loss that encourages single-step convergence during Jacobi iterations. For each target model $p$ to be adapted as a CLLM, the training consists of two parts: Jacobi trajectory preparation: for each prompt, we sequentially perform Jacobi decoding for every truncation of $n$ tokens until the entire response sequence $\\mathbf l$ has been generated, which amounts to a concatenation of all consecutive fixed points. Each sequence generated along a trajectory counts as one data entry. Note that for a lengthy response $\\mathbf l$ of $N$ ($N ≫ n$) tokens, such truncation avoids slow model evaluation on lengthy input. Training with consistency and AR loss: we jointly optimize two losses for tuning CLLMs, the consistency loss guarantees the prediction of multiple tokens at once and the AR loss prevents the CLLM from deviating from the target LLM so as to maintain generation quality. Figure 4: an illustration of consistency training for one-step convergence: refining the target LLM to consistently predict the fixed point given any state along Jacobi trajectory as input. Consistency and AR Loss Consistency Loss Let $p$ denote the target LLM. Let $q_\\theta(\\cdot| \\mathbf x)$ denote the CLLM with parameters $\\theta$ initialized with those of $p$. For a prompt $\\mathbf x$ and the corresponding Jacobi trajectory $\\mathcal{J}$, let $\\mathbf y$ and $\\mathbf y^*$ denote a random state and the fixed point on the trajectory, respectively.\nWe can encourage CLLM to output $\\mathbf y^*$ with $\\mathbf y$ as the input by minimizing the following loss, termed as the global consistency (GC) loss:\n$$ \\begin{align} \\mathcal L_{\\text{GC}} =\\underset{(\\mathbf x, \\mathcal{J}) \\sim \\mathcal{D}, \\mathbf y \\sim \\mathcal{J}}{\\mathbb E} \\Big[ \\sum_{i=1}^n D( q_{\\theta^{-}}(\\cdot|\\mathbf y_{:i}^{*}, \\mathbf x)) || q_{\\theta}(\\cdot|\\mathbf y_{:i}, \\mathbf x)\\Big] \\end{align} $$ where $\\theta^{-} = \\text{stopgrad}(\\theta)$ and we abuse notations to represent uniform sampling from the dataset, and we abuse notations to represent uniform sampling from the dataset. $D(\\cdot||\\cdot)$ denotes the distance between two distributions, choices are discussed in the GKD method and in this paper we primarily experiment with the forward KL.\nAlternatively, local consistency (LC) loss following the formulation in consistency models, where the adjacent states $(\\mathbf y^{(j)}, \\mathbf y^{(j+1)})$ in a Jacobi trajectory $\\mathcal{J}$ are driven to yield the same outputs:\n$$ \\begin{align} \\mathcal L_{\\text{LC}} =\\underset{(\\mathbf x, \\mathcal{J}) \\sim \\mathcal{D}, (\\mathbf y^{(j)}, \\mathbf y^{(j+1)} )\\sim \\mathcal{J}}{\\mathbb E} \\Big[ \\sum_{i=1}^n D( q_{\\theta^{-}}(\\cdot|\\mathbf y_{:i}^{(j+1)}, \\mathbf x)) || q_{\\theta}(\\cdot|\\mathbf y_{:i}^{(j)}, \\mathbf x) \\Big] \\end{align} $$AR Loss To avoid deviating from the distribution of the target LLM, we incorporate the traditional AR loss based on the generation $\\mathbf l$ of the target LLM $p$: $$ \\begin{align} \\mathcal L_{\\text{AR}} = \\underset{ (\\mathbf x, \\mathbf l) \\sim \\mathcal D }{\\mathbb E} \\Big[ - \\sum_{i=1}^N \\log q_{\\theta}(l_i | \\mathbf l_{:i}, \\mathbf x) \\Big] \\end{align} $$ Putting the two loss together, with some weight $w$, the total loss for training a CLLM is: $$ \\mathcal{L}(\\theta) = \\mathcal L_{\\text{consistency}} + w\\mathcal{L}_{\\text{AR}} $$Experiments Results Our experiments contain three domain-specific tasks, including Spider (text-to-SQL), Human-Eval (Python code completion), and GSM8k (math), and the broader open-domain conversational challenge, MT-bench. Reported experiments were conducted using either fine-tuned coder LLM, Deepseek-coder-7B-instruct, LLaMA-2-7B or ABEL-7B-001 as the target model depending on the task. Both training and evaluation are carried out on NVIDIA A100 40GB servers. Figure 5: CLLM speedup on different downstream tasks. CLLMs are significantly faster than pre-trained models and achieve comparable speedups in comparison with Medusa, yet with no extra cost at inference time. Figure 6: illustration of CLLM vs. other baselines on domain-specific tasks (Spider, CSN-Python, GSM8k), as well as on MT-bench. CLLMs achieve similar or even better speedup in comoparison with Medusa2 while introducing no extra inference cost (in terms FLOPS and memory consumption). Specialized domains: From Figure 5, we can see that in comparison with other baselines including the original target model, Medusa2, and speculative decoding, CLLMs achieve the most significant speedup.\nOpen-domain conversational Challenge (MT-bench): CLLM trained from LLaMA2-7B using ShareGPT dataset can achieve roughly the same speedup as Medusa2 when combined with lookahead decoding, with comparable scores on MT-bench. However, CLLM offers higher adaptability and memory efficiency as it requires no modifications to the target model’s original architecture and no auxiliary components.\nTraining Cost The fine-tuning cost of CLLMs is moderate, e.g., passing only around 1M tokens for LLaMA-7B to achieve a $3.4\\times$ speedup on the Spider dataset. In the cases where the dataset size is large, for example, for CodeSearchNet-Python, only 10% of the dataset is required to generate Jacobi trajectories in training CLLMs to obtain around $2.5\\times$ speedup. The total number of tokens can be estimated by taking:\n$N = $ avg # of trajectories per prompt $ \\times $ avg trajectory length $ \\times $ # of prompts.\ndataset estimated training cost (tokens) $\\%$ of pre-training cost Spider 2M $\u003c 0.01\\%$ CodeSearchNet-Python 100M $\\sim 0.1\\%$ GSM8K 10M $\\sim 0.01\\%$ ShareGPT 200M $\\sim 0.2\\%$ Fast Forwarding and Stationary Tokens Figure 7: Comparison of Jacobi trajectory between a target LLM and CLLMs on Spider. Each point along the Jacobi trajectory is a color-coded sequence: blue for correct tokens matching with AR results, and red for inaccurate ones. CLLM demonstrates enhanced efficiency, converging to the fixed point $2\\times$ faster the Target LLM. This increased efficiency in the CLLM can be attributed to the consistency loss which facilitates the learning of the structure of each $n$-token sequence given a prefix. The left side of Figure 6 shows target LLMs typically generate only one correct token in one iteration. In contrast, in CLLMs, we identify fast forwarding phenomenon where multiple consecutive tokens are correctly predicted in a single Jacobi iteration.\nMoreover, tokens correctly generated in advance (e.g. “country” and “H” at index 6 and 7 on the left side of Figure 6), are often replaced inaccurately in subsequent iterations in target LLMs. On the other hand, CLLMs exhibit the capability of predicting correct tokens preemptively, even with preceding incorrect tokens, while ensuring the tokens remain unchanged. We term such tokens as stationary tokens. Both phenomena contribute to the fast convergence in Jacobi decoding of CLLMs, thereby leading to a considerable generation speedup.\nWe observe that CLLMs acquire a crucial linguistic concept through training – collocations: a series of words or terms that co-occur more frequently than one would expect by random chance. Language is not solely composed of isolated words but also relies heavily on specific word pairings. Examples of collocations are abundant in both natural and coding languages. They include verb + preposition combinations (e.g., ‘’talk to’’, ‘‘remind … of …’’), verb + noun structures (e.g., ‘‘make a decision’’, ‘‘catch a cold’’), and many more domain-specific syntactical structures (e.g., ‘‘SELECT … FROM …’’, ‘‘if … else’’ for programming). The consistency generation objective allows CLLMs to infer such structures from any point in the Jacobi trajectory, encouraging CLLMs to acquire proficiency in numerous collocations and thereby predict multiple words simultaneously to minimize iteration steps.\nGet started Please see our paper for more details. We also invite you to try out our codebase and CLLM checkpoints! Acknowledgement We would like to thank Yang Song, Canwen Xu, Yonghao Zhuang, Dacheng Li and Yichao Fu for providing insightful feedback.\nCitation @misc{kou2024cllms, title={CLLMs: Consistency Large Language Models}, author={Siqi Kou and Lanxiang Hu and Zhezhi He and Zhijie Deng and Hao Zhang}, year={2024}, eprint={2403.00835}, archivePrefix={arXiv}, primaryClass={cs.CL} } ",
  "wordCount" : "2080",
  "inLanguage": "en",
  "image":"http://localhost:1313/img/objective_illustration_global.jpg","datePublished": "2024-05-06T12:00:00-08:00",
  "dateModified": "2024-05-06T12:00:00-08:00",
  "author":{
    "@type": "Person",
    "name": "Siqi Kou*, Lanxiang Hu*, Zhezhi He, Zhijie Deng†, Hao Zhang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/cllm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "DENG Lab @ SJTU",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
    
    	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]      
    }
  };
</script>

    
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="DENG Lab @ SJTU (Alt + H)">
                <img id="logo-image" src="/img/logo-new.png" alt="DENG Lab @ SJTU" height="40">
                
            </a>
            <div class="logo-switches">
                <ul class="lang-switch">
                </ul>
            </div>
        </div>
        <ul id="menu">
            <div style="margin-right: 15px;">
              <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                  <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                      stroke-linejoin="round">
                      <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                  </svg>
                  <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                      stroke-linejoin="round">
                      <circle cx="12" cy="12" r="5"></circle>
                      <line x1="12" y1="1" x2="12" y2="3"></line>
                      <line x1="12" y1="21" x2="12" y2="23"></line>
                      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                      <line x1="1" y1="12" x2="3" y2="12"></line>
                      <line x1="21" y1="12" x2="23" y2="12"></line>
                      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                  </svg>
              </button>
            </div>
            <li>
                  <a href="http://localhost:1313/home/" title="Home">
                      <span>Home</span>
                  </a>
            </li>
            <li>
                  <a href="http://localhost:1313/blogs/" title="Blogs">
                      <span>Blogs</span>
                  </a>
            </li>
            <li>
                  <a href="http://localhost:1313/people/" title="People">
                      <span>People</span>
                  </a>
            </li>
            <li>
                  <a href="http://localhost:1313/publications/" title="Publications">
                      <span>Publications</span>
                  </a>
            </li>
            <li>
                  <a href="http://localhost:1313/contact/" title="Contact">
                      <span>Contact</span>
                  </a>
            </li>
            <li>
                  
                  <a href="https://x.com/sjtudenglab" title="Twitter">
                    <i class="fab fa-twitter fa-lg"></i>
                  </a>
            </li>
            <li>
                  
                  <a href="https://github.com/zhijie-group" title="GitHub">
                    <i class="fab fa-github fa-lg"></i>
                  </a>
            </li>
            <li>
                  <a href="https://www.zhihu.com/people/SJTUDengLab" title="Zhihu">
                      <span>Zhihu</span>&nbsp;
                      <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                      </svg>
                  </a>
            </li>
        </ul>
    </nav>
</header>

<script>
    function updateThemeAndLogo() {
        
        document.body.classList.toggle('dark');

        
        const isDarkTheme = document.body.classList.contains('dark');
        const logoImage = document.getElementById('logo-image');
        

        
        const themePreference = isDarkTheme ? 'dark' : 'light';
        localStorage.setItem('pref-theme', themePreference);
    }

    function applyCurrentThemeAndLogo() {
        
        const isDarkTheme = document.body.classList.contains('dark');

        
        const logoImage = document.getElementById('logo-image');
        
    }

    
    document.addEventListener('DOMContentLoaded', function() {
        const themeToggleButton = document.getElementById('theme-toggle');
        if (themeToggleButton) {
            themeToggleButton.addEventListener('click', updateThemeAndLogo);
        }

        
        
        const savedTheme = localStorage.getItem('pref-theme') || (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light');
        document.body.classList.toggle('dark', savedTheme === 'dark');
    });

    
    const themeToggleButton = document.getElementById('theme-toggle');
    if (themeToggleButton) {
        themeToggleButton.addEventListener('click', () => {
            updateThemeAndLogo(); 
        });
        
        applyCurrentThemeAndLogo();
    }
</script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-T4DXGLCH1D"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-T4DXGLCH1D');
</script>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Consistency Large Language Models: A Family of Efficient Parallel Decoders
    </h1>
    <div class="post-meta"><span title='2024-05-06 12:00:00 -0800 -0800'>May 6, 2024</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Siqi Kou*, Lanxiang Hu*, Zhezhi He, Zhijie Deng†, Hao Zhang

</div>
  </header> 
<figure class="entry-cover"><img loading="eager" src="http://localhost:1313/img/objective_illustration_global.jpg" alt="jacobi trajectory">
        <p>An instance of Jacobi trajectory and an illustration of the global consistency loss learning objective.</p>
</figure>
  <div class="post-content"><div style="padding-left: 30px; display: flex !important; gap: 60px !important;">
    
        <a href="https://arxiv.org/abs/2403.00835" target="_blank" rel="noopener noreferrer">
          <img src="https://img.shields.io/badge/arXiv-2403.00835-white.svg?style=social" style="transform: scale(1.5);">
        </a>
    
    
        <a href="https://github.com/hao-ai-lab/Consistency_LLM" target="_blank" rel="noopener noreferrer">
          <img src="https://img.shields.io/github/stars/hao-ai-lab/Consistency_LLM?style=social" style="transform: scale(1.5);">
        </a>
    
    
    
</div>

<div style="text-align: left;">
    <strong>TL;DR:</strong> LLMs have been traditionally regarded as sequential decoders, decoding one token after another. In this blog, we show pretrained LLMs can be easily taught to operate as efficient parallel decoders. We introduce <strong>Consistency Large Language Models (CLLMs)</strong>, a new family of parallel decoders capable of reducing inference latency by efficiently decoding an $n$-token sequence per inference step. Our research shows this process &ndash; mimicking human cognitive process of forming complete sentences in mind before articulating word by word &ndash; can be effectively learned by simply finetuning pretrained LLMs. Specifically, CLLMs are trained to perform parallel decoding by mapping any randomly initialized $n$-token sequence to the same result yielded by autoregressive (AR) decoding in as few steps as possible. Experiment results show CLLMs obtained using our proposed method are highly effective, showing $2.4\times$ to $3.4\times$ improvements in generation speed, in par with or even beter than other fast inference techniques like Medusa2 and Eagle, yet require no additional memory cost to accomodate auxiliary model components at inference time.
</div>



<style>
  .gray-text {
    color: #808080;
  }
</style>


  <figure>
	  <div style="display: grid; place-items: center;">
      <img src="img/baseline_vs_cllm_gsm8k_best_acc_demo.gif" alt="cllm-gsm8k-acc-demo" style="width: 120%; height: auto;">
      <figcaption style="font-size: 16px;" class="gray-text">Figure 1: Demo of $\sim 3 \times$ speedup by CLLM-ABEL-7B-001 in comparison with baseline <a href="https://github.com/GAIR-NLP/abel">ABEL-7B-001</a> using Jacobi decoding on GSM8K.</figcaption>
    </div>
  </figure>


<h2 id="background-jacobi-decoding">Background: Jacobi Decoding<a hidden class="anchor" aria-hidden="true" href="#background-jacobi-decoding">#</a></h2>
<div style="text-align: left;">
    Large language models (LLMs) are transforming the landscape of human lives, from programming to offering legal and health advice. However, during inference, LLMs generate responses token by token using AR decoding as shown in Figure 1, leading to high latency for longer responses. Using AR decoding, it often necessitates architectural modifications, auxiliary components, or draft models, to speed up inference by generating more than one token at a time.
</div>



<style>
  .gray-text {
    color: #808080;
  }
</style>


  <figure>
	  <div style="display: grid; place-items: center;">
      <img src="img/clm_objective.png" alt="autoregressive" style="width: 60%; height: auto;">
      <figcaption style="font-size: 16px;" class="gray-text">Figure 2: illustration of conventional AR decoding: one token is generated at a time.</figcaption>
    </div>
  </figure>


<div style="text-align: left;">
    <a href="https://arxiv.org/abs/2305.10427">Jacobi decoding</a> originates from the Jacobi and Gauss-Seidel fixed-point iteration for solving nonlinear equations, and <a href="https://proceedings.mlr.press/v139/song21a.html">is proven identical to AR generation using greedy decoding</a>. Jacobi decoding reformulates the sequential generation process into a system of $n$ non-linear equations with $n$ variables solvable in parallel based on Jacobi iteration. Each iteration step might predict more than one correct token (By &ldquo;correct&rdquo;, we mean alignment with the AR decoding
result under a greedy sampling strategy), thereby accelerating AR decoding potentially.
</div>



<style>
  .gray-text {
    color: #808080;
  }
</style>


  <figure>
	  <div style="display: grid; place-items: center;">
      <img src="img/jacobi_objective.png" alt="jacobi" style="width: 60%; height: auto;">
      <figcaption style="font-size: 16px;" class="gray-text">Figure 3: illustration of Jacobi decoding: $n$-token sequence is fed into the LLM and iterates until convergence.</figcaption>
    </div>
  </figure>


<div style="text-align: left;">
    To be specific, Jacobi decoding method first randomly guesses the next $n$ tokens in a sequence (referred to as $n$-token sequence hereinafter unless specified otherwise) from an input prompt. The $n$-token sequence, along with the prompt, is then fed to the LLM to iteratively update itself. This process continues until the $n$-token sequence stabilizes and no further changes occur, reaching a fixed point. Notably, Jacobi decoding requires no more queries to the LLM than auto-regressive (AR) decoding. Eventually, the $n$-token sequence converges to the output that would be generated by AR decoding under a greedy strategy. This progression from an initial random guess to the final AR generation outcome traces what is known as a Jacobi trajectory. An instance of Jacobi decoding iteration process and <strong>Jacobi trajectory</strong> is illustrated in Figure 2.
</div>



<h3 id="limitations-of-jacobi-decoding">Limitations of Jacobi Decoding<a hidden class="anchor" aria-hidden="true" href="#limitations-of-jacobi-decoding">#</a></h3>
<div style="text-align: left;">
    <p>However, vanilla Jacobi decoding for LLMs shows only marginal speedup over AR decoding in practice, e.g., <a href="https://arxiv.org/abs/2305.10427">an average of $1.05\times$ speedup</a>. This is because an AR-trained LLM can rarely yield a correct token when there are incorrections in its preceding tokens. Thereby, most Jacobi iterations gain only one correction for the $n$-token sequence, resulting in a longer trajectory as illustrated on the left side of Figure 3.</p>
<p><a href="https://lmsys.org/blog/2023-11-21-lookahead-decoding/">Lookahead decoding</a> and speculative decoding methods try to mitigate inefficiency in Jacobi decoding and conventional AR decoding, but incurs extra memory cost during inference time. While CLLMs require none.</p>

</div>



<h2 id="consistency-llms-cllms">Consistency LLMs (CLLMs)<a hidden class="anchor" aria-hidden="true" href="#consistency-llms-cllms">#</a></h2>
<h3 id="jacobi-decoding-preliminary">Jacobi Decoding Preliminary<a hidden class="anchor" aria-hidden="true" href="#jacobi-decoding-preliminary">#</a></h3>
<div style="text-align: left;">
    Given a prompt $\mathbf x$ and a pre-trained LLM $p(\cdot| \mathbf x)$, LLM typically generates with the standard AR decoding method under the greedy strategy, i.e.
</div>



$$
\begin{align}
y_i = \underset{y}{\text{arg max }} p(y | \mathbf {y}_{:i}, \mathbf x) \ \ \text{for}\,\, i = 1,\dots,n
\end{align}
$$<div style="text-align: left;">
    Jacobi decoding re-frames the LLM inference process as solving a system of nonlinear equations to transform the decoding process into a parallelizable computation. Consider, $f(y_i, \mathbf y_{:i}, \mathbf x):= y_i- \underset{y}{\text{arg max }} p(y | \mathbf y_{:i}, \mathbf x)$, we can rewrite the above equation as a system of nonlinear equations:
</div>



$$
\begin{align}
f(y_i, \mathbf y_{:i}, \mathbf x) = 0 \ \ \text{for} \quad i = 1,\dots,n 
\Longrightarrow 
\begin{cases}
y_{1}^{(j+1)} &= \underset{y}{\text{arg max}} \ \ p(y | \mathbf x) \\
y_{2}^{(j+1)} &= \underset{y}{\text{arg max}} \ \ p(y | \mathbf y_{1}^{(j)}, \mathbf x) \\
& \vdots \\
y_{n}^{(j+1)} &= \underset{y}{\text{arg max}} \ \ p(y | \mathbf y_{:n}^{(j)}, \mathbf x)
\end{cases}
\end{align}
$$<div style="text-align: left;">
    Note that the process exits at some k such that $\mathbf y^{(k)} = \mathbf y^{(k−1)}$ and we define $\mathbf y^{∗} := \mathbf y^{(k)}$ as the <strong>fixed point</strong>, and $\mathcal J := \set{  \mathbf y^{(1)}, \dots, \mathbf y^{(k)} }$ as the <strong>Jacobi trajectory</strong>.
</div>



<h3 id="training-with-jacobi-trajectories">Training with Jacobi Trajectories<a hidden class="anchor" aria-hidden="true" href="#training-with-jacobi-trajectories">#</a></h3>
<div style="text-align: left;">
    To address this, we propose adapting pre-trained LLMs so that they can consistently map any point $\mathbf y$ on the Jacobi trajectory $\mathcal{J}$ to the fixed point $\mathbf y^*$. Surprisingly, we find such an objective is analogous to that of <a href="https://arxiv.org/abs/2303.01469">consistency models</a>, a leading acceleration approach for diffusion models. In our proposed method, we use Jacobi trajectories collected from a target model to train the model with a loss that encourages single-step convergence during Jacobi iterations. For each target model $p$ to be adapted as a CLLM, the training consists of two parts:
</div>



<div style="text-align: left;">
    <ul>
<li><strong>Jacobi trajectory preparation:</strong> for each prompt, we sequentially perform Jacobi decoding for every truncation of $n$ tokens until the entire response sequence $\mathbf l$ has been generated, which amounts to a concatenation of all consecutive fixed points. Each sequence generated along a trajectory counts as one data entry. Note that for a lengthy response $\mathbf l$ of $N$ ($N ≫ n$) tokens, such truncation avoids slow model evaluation on lengthy input.</li>
</ul>

</div>



<div style="text-align: left;">
    <ul>
<li><strong>Training with consistency and AR loss:</strong> we jointly optimize two losses for tuning CLLMs, the consistency loss guarantees the prediction of multiple tokens at once and the AR loss prevents the CLLM from deviating from the target LLM so as to maintain generation quality.</li>
</ul>

</div>



<style>
  .gray-text {
    color: #808080;
  }
</style>


  <figure>
	  <div style="display: grid; place-items: center;">
      <img src="img/cllm_objective.png" alt="training_objective" style="width: 130%; height: auto;">
      <figcaption style="font-size: 16px;" class="gray-text">Figure 4: an illustration of consistency training for one-step convergence: refining the target LLM to consistently predict the fixed point given any state along Jacobi trajectory as input.</figcaption>
    </div>
  </figure>


<h3 id="consistency-and-ar-loss">Consistency and AR Loss<a hidden class="anchor" aria-hidden="true" href="#consistency-and-ar-loss">#</a></h3>
<h4 id="consistency-loss">Consistency Loss<a hidden class="anchor" aria-hidden="true" href="#consistency-loss">#</a></h4>
<div style="text-align: left;">
    <p>Let $p$ denote the target LLM. Let $q_\theta(\cdot| \mathbf x)$ denote the CLLM with parameters $\theta$ initialized with those of $p$. For a prompt $\mathbf x$ and the corresponding Jacobi trajectory $\mathcal{J}$, let $\mathbf y$ and $\mathbf y^*$ denote a random state and the fixed point on the trajectory, respectively.</p>
<p>We can encourage CLLM to output $\mathbf y^*$ with $\mathbf y$ as the input by minimizing the following loss, termed as the <strong>global consistency (GC) loss</strong>:</p>

</div>



$$
\begin{align}
   \mathcal L_{\text{GC}} =\underset{(\mathbf x, \mathcal{J}) \sim \mathcal{D}, \mathbf y \sim \mathcal{J}}{\mathbb E} \Big[ \sum_{i=1}^n  D( q_{\theta^{-}}(\cdot|\mathbf y_{:i}^{*}, \mathbf x))  || q_{\theta}(\cdot|\mathbf y_{:i}, \mathbf x)\Big] 
\end{align}
$$<div style="text-align: left;">
    <p>where $\theta^{-} = \text{stopgrad}(\theta)$ and we abuse notations to represent uniform sampling from the dataset, and we abuse notations to represent uniform sampling from the dataset.  $D(\cdot||\cdot)$ denotes the distance between two distributions, choices are discussed in <a href="https://arxiv.org/abs/2306.13649">the GKD method</a> and in this paper we primarily experiment with the forward KL.</p>
<p>Alternatively, local consistency (LC) loss following the formulation in consistency models, where the adjacent states $(\mathbf y^{(j)}, \mathbf y^{(j+1)})$ in a Jacobi trajectory $\mathcal{J}$ are driven to yield the same outputs:</p>

</div>



$$
\begin{align}
   \mathcal L_{\text{LC}} =\underset{(\mathbf x, \mathcal{J}) \sim \mathcal{D}, (\mathbf y^{(j)}, \mathbf y^{(j+1)} )\sim \mathcal{J}}{\mathbb E} \Big[ \sum_{i=1}^n  D( q_{\theta^{-}}(\cdot|\mathbf y_{:i}^{(j+1)}, \mathbf x)) || q_{\theta}(\cdot|\mathbf y_{:i}^{(j)}, \mathbf x) \Big] 
\end{align}
$$<h4 id="ar-loss">AR Loss<a hidden class="anchor" aria-hidden="true" href="#ar-loss">#</a></h4>
<div style="text-align: left;">
    To avoid deviating from the distribution of the target LLM, we incorporate the traditional AR loss based on the generation $\mathbf l$ of the target LLM $p$:
</div>



$$
\begin{align}
    \mathcal L_{\text{AR}} = \underset{ (\mathbf x, \mathbf l) \sim \mathcal D }{\mathbb E} \Big[ - \sum_{i=1}^N \log q_{\theta}(l_i | \mathbf l_{:i}, \mathbf x) \Big]
\end{align}
$$<p>
<div style="text-align: left;">
    Putting the two loss together, with some weight $w$, the total loss for training a CLLM is:
</div>


</p>
$$
\mathcal{L}(\theta) = \mathcal L_{\text{consistency}} + w\mathcal{L}_{\text{AR}}
$$<h2 id="experiments">Experiments<a hidden class="anchor" aria-hidden="true" href="#experiments">#</a></h2>
<h3 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h3>
<div style="text-align: left;">
    Our experiments contain three domain-specific tasks, including Spider (text-to-SQL), Human-Eval (Python code completion), and GSM8k (math), and the broader open-domain conversational challenge, MT-bench. Reported experiments were conducted using either fine-tuned coder LLM, Deepseek-coder-7B-instruct, LLaMA-2-7B or ABEL-7B-001 as the target model depending on the task. Both training and evaluation are carried out on NVIDIA A100 40GB servers.
</div>



<style>
  .gray-text {
    color: #808080;
  }
</style>


  <figure>
	  <div style="display: grid; place-items: center;">
      <img src="img/cllm_speedup.png" alt="speedup" style="width: 70%; height: auto;">
      <figcaption style="font-size: 16px;" class="gray-text">Figure 5: CLLM speedup on different downstream tasks. CLLMs are significantly faster than pre-trained models and achieve comparable speedups in comparison with Medusa, yet with no extra cost at inference time.</figcaption>
    </div>
  </figure>


<style>
  .gray-text {
    color: #808080;
  }
</style>


    <figure>
	<div style="display: flex; justify-content: center; align-items: center; gap: 20px;">
                <img src="img/mt-bench.png" alt="specialized" style="width: 50%; height: auto;"> 
		<img src="img/specialized_domains.png" alt="mt_bench" style="width: 50%; height: auto;">
	</div>
	<div style="display: grid; place-items: center;">
		<figcaption style="font-size: 16px;" class="gray-text">Figure 6: illustration of CLLM vs. other baselines on domain-specific tasks (Spider, CSN-Python, GSM8k), as well as on MT-bench. CLLMs achieve similar or even better speedup in comoparison with Medusa2 while introducing no extra inference cost (in terms FLOPS and memory consumption).</figcaption>
	</div>
    </figure>


<div style="text-align: left;">
    <p><strong>Specialized domains:</strong> From Figure 5, we can see that in comparison with other baselines including the original target model, Medusa2, and speculative decoding, CLLMs achieve the most significant speedup.</p>
<p><strong>Open-domain conversational Challenge (MT-bench):</strong> CLLM trained from LLaMA2-7B using ShareGPT dataset can achieve roughly the same speedup as Medusa2 when combined with lookahead decoding, with comparable scores on MT-bench. However, CLLM offers higher adaptability and memory efficiency as it requires no modifications to the target model&rsquo;s original architecture and no auxiliary components.</p>

</div>



<h3 id="training-cost">Training Cost<a hidden class="anchor" aria-hidden="true" href="#training-cost">#</a></h3>
<div style="text-align: left;">
    <p>The fine-tuning cost of CLLMs is moderate, e.g., passing only around 1M tokens for LLaMA-7B to achieve a $3.4\times$ speedup on the Spider dataset. In the cases where the dataset size is large, for example, for CodeSearchNet-Python, only 10% of the dataset is required to generate Jacobi trajectories in training CLLMs to obtain around $2.5\times$ speedup. The total number of tokens can be estimated by taking:</p>
<p>$N = $ avg # of trajectories per prompt $ \times $ avg trajectory length $ \times $ # of prompts.</p>

</div>



<div style="text-align: center;">
    <table>
  <thead>
      <tr>
          <th style="text-align: center">dataset</th>
          <th style="text-align: center">estimated training cost (tokens)</th>
          <th style="text-align: center">$\%$ of pre-training cost</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">Spider</td>
          <td style="text-align: center">2M</td>
          <td style="text-align: center">$< 0.01\%$</td>
      </tr>
      <tr>
          <td style="text-align: center">CodeSearchNet-Python</td>
          <td style="text-align: center">100M</td>
          <td style="text-align: center">$\sim 0.1\%$</td>
      </tr>
      <tr>
          <td style="text-align: center">GSM8K</td>
          <td style="text-align: center">10M</td>
          <td style="text-align: center">$\sim 0.01\%$</td>
      </tr>
      <tr>
          <td style="text-align: center">ShareGPT</td>
          <td style="text-align: center">200M</td>
          <td style="text-align: center">$\sim 0.2\%$</td>
      </tr>
  </tbody>
</table>

</div>

<h3 id="fast-forwarding-and-stationary-tokens">Fast Forwarding and Stationary Tokens<a hidden class="anchor" aria-hidden="true" href="#fast-forwarding-and-stationary-tokens">#</a></h3>
<style>
  .gray-text {
    color: #808080;
  }
</style>


  <figure>
	  <div style="display: grid; place-items: center;">
      <img src="img/trajectory_compare_aligned.png" alt="trajectory_compare" style="width: 120%; height: auto;">
      <figcaption style="font-size: 16px;" class="gray-text">Figure 7: Comparison of Jacobi trajectory between a target LLM and CLLMs on Spider. Each point along the Jacobi trajectory is a color-coded sequence: blue for correct tokens matching with AR results, and red for inaccurate ones. CLLM demonstrates enhanced efficiency, converging to the fixed point $2\times$ faster the Target LLM. This increased efficiency in the CLLM can be attributed to the consistency loss which facilitates the learning of the structure of each $n$-token sequence given a prefix.</figcaption>
    </div>
  </figure>


<div style="text-align: left;">
    <p>The left side of Figure 6 shows target LLMs typically generate only one correct token in one iteration. In contrast, in CLLMs, we identify <strong>fast forwarding phenomenon</strong> where multiple consecutive tokens are correctly predicted in a single Jacobi iteration.</p>
<p>Moreover, tokens correctly generated in advance (e.g. “country” and “H” at index 6 and 7 on the left side of Figure 6), are often replaced inaccurately in subsequent iterations in target LLMs. On the other hand, CLLMs exhibit the capability of predicting correct tokens preemptively, even with preceding incorrect tokens, while ensuring the tokens remain unchanged. We term such tokens as <strong>stationary tokens</strong>. Both phenomena contribute to the fast convergence in Jacobi decoding of CLLMs, thereby leading to a considerable generation speedup.</p>
<p>We observe that CLLMs acquire a crucial linguistic concept through training – <strong>collocations</strong>: a series of words or terms that <a href="https://aclanthology.org/P91-1036.pdf">co-occur more frequently than one would expect by random chance</a>. Language is not solely composed of isolated words but also relies heavily on specific word pairings. Examples of collocations are abundant in both natural and coding languages. They include verb + preposition combinations (e.g., &lsquo;&rsquo;talk to&rsquo;&rsquo;, &lsquo;&lsquo;remind &hellip; of &hellip;&rsquo;&rsquo;), verb + noun structures (e.g., &lsquo;&lsquo;make a decision&rsquo;&rsquo;, &lsquo;&lsquo;catch a cold&rsquo;&rsquo;), and many more domain-specific syntactical structures (e.g., &lsquo;&lsquo;SELECT &hellip; FROM &hellip;&rsquo;&rsquo;, &lsquo;&lsquo;if &hellip; else&rsquo;&rsquo; for programming). The consistency generation objective allows CLLMs to infer such structures from any point in the Jacobi trajectory, encouraging CLLMs to acquire proficiency in numerous collocations and thereby predict multiple words simultaneously to minimize iteration steps.</p>

</div>



<h2 id="get-started">Get started<a hidden class="anchor" aria-hidden="true" href="#get-started">#</a></h2>
<div style="text-align: left;">
    Please see <a href="http://arxiv.org/abs/2403.00835">our paper</a> for more details. We also invite you to try out <a href="https://github.com/hao-ai-lab/Consistency_LLM">our codebase</a> and <a href="https://huggingface.co/cllm">CLLM checkpoints</a>!
</div>



<h2 id="acknowledgement">Acknowledgement<a hidden class="anchor" aria-hidden="true" href="#acknowledgement">#</a></h2>
<p>We would like to thank Yang Song, Canwen Xu, Yonghao Zhuang, Dacheng Li and Yichao Fu for providing insightful feedback.</p>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<pre tabindex="0"><code>@misc{kou2024cllms,
      title={CLLMs: Consistency Large Language Models}, 
      author={Siqi Kou and Lanxiang Hu and Zhezhi He and Zhijie Deng and Hao Zhang},
      year={2024},
      eprint={2403.00835},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
</code></pre>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">DENG Lab @ SJTU</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
        , Adapted by Zhijie Deng
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
