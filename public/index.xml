<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DENG Lab @ SJTU</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on DENG Lab @ SJTU</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Sep 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AdaMoE: Token-Adaptive Routing with Null Experts for MoE</title>
      <link>http://localhost:1313/blogs/adamoe/</link>
      <pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/adamoe/</guid>
      <description>&lt;div style=&#34;padding-left: 30px; display: flex !important; gap: 60px !important;&#34;&gt;
    
    
    
        &lt;a href=&#34;https://arxiv.org/pdf/2406.13233&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
            &lt;img src=&#34;https://img.shields.io/badge/ğŸ“ƒ Paper Link--1abc9c.svg?style=social&#34; style=&#34;transform: scale(1.5);&#34;&gt;
        &lt;/a&gt;
    
    
        &lt;a href=&#34;https://github.com/zhijie-group/AdaMoE&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
            &lt;img src=&#34;https://img.shields.io/badge/ğŸ’» Code Link--1abc9c.svg?style=social&#34; style=&#34;transform: scale(1.5);&#34;&gt;
        &lt;/a&gt;
    
&lt;/div&gt;

&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;p&gt;LongCat-Flash[1] just showed a clean, large-scale deployment of token-adaptive MoE with zero-computation (identity) expertsâ€”-activating 18.6â€“31.3B parameters per token (~27B on average) inside a 560B MoE. Each layer mixes 512 FFN experts + 256 zero-compute experts, the router selects Top-12, and the average true-expert picks settle around ~8 thanks to a PID-style budget controller; device-level load balancing and ScMoE (shortcut-connected MoE) keep the system efficient. They present the model as a non-thinking foundation model with strong throughput/cost metrics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdaMoE: å€ŸåŠ©â€œç©ºä¸“å®¶â€å®ç°Tokençº§åˆ«çš„åŠ¨æ€è·¯ç”±é€‰æ‹©</title>
      <link>http://localhost:1313/blogs/adamoe_zh/</link>
      <pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/adamoe_zh/</guid>
      <description>&lt;div style=&#34;padding-left: 30px; display: flex !important; gap: 60px !important;&#34;&gt;
    
    
    
        &lt;a href=&#34;https://arxiv.org/pdf/2406.13233&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
            &lt;img src=&#34;https://img.shields.io/badge/ğŸ“ƒ Paper Link--1abc9c.svg?style=social&#34; style=&#34;transform: scale(1.5);&#34;&gt;
        &lt;/a&gt;
    
    
        &lt;a href=&#34;https://github.com/zhijie-group/AdaMoE&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
            &lt;img src=&#34;https://img.shields.io/badge/ğŸ’» Code Link--1abc9c.svg?style=social&#34; style=&#34;transform: scale(1.5);&#34;&gt;
        &lt;/a&gt;
    
&lt;/div&gt;

&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;p&gt;LongCat-Flash[1] è¿‘æœŸå±•ç¤ºäº†ä¸€ç§tokençº§åˆ«è‡ªé€‚åº”çš„MoEçš„å¤§è§„æ¨¡éƒ¨ç½²æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨é›¶è®¡ç®—ï¼ˆæ’ç­‰ï¼‰ä¸“å®¶ï¼Œåœ¨560B MoEæ¨¡å‹ä¸­ï¼Œæ¯ä¸ªtokenæ¿€æ´»18.6B~31.3Bçš„å‚æ•°é‡ï¼ˆå¹³å‡çº¦27Bï¼‰ã€‚æ¯ä¸€å±‚æ··åˆäº†512ä¸ªçœŸä¸“å®¶å’Œ256ä¸ªé›¶è®¡ç®—ä¸“å®¶ï¼Œæ¯æ¬¡é€‰æ‹©top-12çš„ä¸“å®¶ï¼Œå¹¶ä¸”ç”±äºé‡‡ç”¨äº†PIDå¼é¢„ç®—æ§åˆ¶å™¨ï¼Œå¹³å‡çœŸä¸“å®¶é€‰æ‹©æ•°é‡ç¨³å®šåœ¨çº¦8ä¸ªã€‚è®¾å¤‡çº§è´Ÿè½½å‡è¡¡å’ŒScMoEä¿è¯äº†ç³»ç»Ÿçš„æ•ˆç‡ã€‚ä»–ä»¬å°†è¯¥æ¨¡å‹å‘ˆç°ä¸ºä¸€ä¸ªå…·æœ‰å¼ºå¤§ååé‡/æˆæœ¬æŒ‡æ ‡çš„éæ€è€ƒæ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;è¿™ç§è®¾è®¡â€”â€”æ·»åŠ ç©ºä¸“å®¶å¹¶å¢åŠ top-kå€¼ï¼Œä»è€Œä½¿æ¯ä¸ªtokenä½¿ç”¨å¯å˜æ•°é‡çš„çœŸå®ä¸“å®¶â€”â€”æ­£æ˜¯AdaMoEèƒŒåçš„æ ¸å¿ƒæ€æƒ³ã€‚&lt;/p&gt;

&lt;/div&gt;



&lt;h2 id=&#34;å¤ªé•¿ä¸çœ‹ç‰ˆ&#34;&gt;å¤ªé•¿ä¸çœ‹ç‰ˆ&lt;/h2&gt;
&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;p&gt;AdaMoE é€šè¿‡å‘ä¸“å®¶æ± ä¸­æ·»åŠ ä¸€ç»„ &lt;strong&gt;â€œç©ºä¸“å®¶â€&lt;/strong&gt;ï¼ˆnull expertsï¼‰ï¼ˆå…¶è®¡ç®—å¼€é”€ä¸ºé›¶ï¼‰ï¼Œä¸ºæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¸¦æ¥äº†Tokençº§åˆ«çš„è‡ªé€‚åº”ä¸“å®¶é€‰æ‹©èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨ &lt;code&gt;top-k&lt;/code&gt;è·¯ç”±èŒƒå¼ä¸‹ï¼Œå½“ä¸€ä¸ªTokenè¢«è·¯ç”±åˆ°â€œç©ºä¸“å®¶â€æ—¶ï¼Œå®ƒå®é™…ä¸Šæ¿€æ´»äº†æ›´å°‘çš„çœŸå®ä¸“å®¶ã€‚è¿™ä½¿å¾—&lt;strong&gt;æ¯ä¸ªTokenæ‰€æ¿€æ´»çš„çœŸå®ä¸“å®¶æ•°é‡ï¼Œåœ¨ä¿æŒå¹³å‡è®¡ç®—é¢„ç®—ä¸å˜çš„å‰æä¸‹ï¼Œå®ç°äº†è‡ªé€‚åº”&lt;/strong&gt;ã€‚&lt;/li&gt;
&lt;li&gt;æˆ‘ä»¬å¯¹è´Ÿè½½å‡è¡¡æŸå¤±å‡½æ•°ï¼ˆloadâ€‘balancing lossï¼‰è¿›è¡Œäº†å¾®è°ƒï¼ˆå°†æ‰€æœ‰â€œç©ºä¸“å®¶â€è§†ä¸ºä¸€ä¸ªèšåˆçš„è®¡ç®—å•å…ƒï¼‰ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ä¸ªç®€å•çš„é€€ç«ç­–ç•¥ã€‚AdaMoEåœ¨é™ä½äº†è®¡ç®—é‡ï¼ˆFLOPsï¼‰çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æå‡äº†æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨Mixtralâ€‘8Ã—7Bæ¨¡å‹å’ŒARCâ€‘Cæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼šè®¡ç®—é‡é™ä½äº†14.55%ï¼Œè€Œå‡†ç¡®ç‡æå‡äº†1.69%ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;



&lt;h2 id=&#34;ä¸ºä½•ä¸“å®¶é€‰æ‹©éœ€è¦tokençº§åˆ«çš„è‡ªé€‚åº”&#34;&gt;ä¸ºä½•ä¸“å®¶é€‰æ‹©éœ€è¦Tokençº§åˆ«çš„è‡ªé€‚åº”ï¼Ÿ&lt;/h2&gt;
&lt;div style=&#34;text-align: left;&#34;&gt;
    æˆ‘ä»¬å°†æ··åˆä¸“å®¶æ¨¡å‹çš„è·¯ç”±æœºåˆ¶ä»å›ºå®šçš„ &lt;strong&gt;&lt;code&gt;top-k&lt;/code&gt;&lt;/strong&gt; æ¨¡å¼è½¬å‘Tokençº§åˆ«çš„è‡ªé€‚åº”æ¨¡å¼ï¼Œå…¶æ ¹æœ¬åŸå› åœ¨äºä¸€ä¸ªå…³é”®çš„è§‚å¯Ÿï¼š&lt;strong&gt;å¹¶éæ‰€æœ‰Tokenåœ¨è®¡ç®—ä¸Šéƒ½ç”Ÿè€Œå¹³ç­‰&lt;/strong&gt;ã€‚åœ¨ä¸€æ®µæ–‡æœ¬ä¸­ï¼Œä¸åŒTokenæ‰€è•´å«çš„ä¿¡æ¯é‡å’Œå¤„ç†çš„å¤æ‚åº¦å­˜åœ¨å·¨å¤§å·®å¼‚ã€‚ä¼ ç»Ÿçš„MoEæ¨¡å‹å¼ºåˆ¶æ¯ä¸ªTokenæ¿€æ´»å›ºå®šæ•°é‡çš„ä¸“å®¶ï¼Œè¿™ç§ç»Ÿä¸€çš„è®¡ç®—åˆ†é…æ–¹å¼å¿½ç•¥äº†Tokené—´çš„å·®å¼‚ï¼Œä»è€Œå¯¼è‡´äº†è®¡ç®—èµ„æºçš„ä½æ•ˆåˆ†é…ã€‚
&lt;/div&gt;



&lt;style&gt;
  .gray-text {
    color: #808080;
  }
&lt;/style&gt;


  &lt;figure&gt;
	  &lt;div style=&#34;display: grid; place-items: center;&#34;&gt;
      &lt;img src=&#34;image/pre.png&#34; alt=&#34;Pre-Experiment&#34; style=&#34;width: 55%; height: auto;&#34;&gt;
      &lt;figcaption style=&#34;font-size: 16px;&#34; class=&#34;gray-text&#34;&gt;åœ¨ SocialIQA æ•°æ®é›†ä¸Šï¼Œä¸åŒTokençš„ç´¯ç§¯è·¯ç”±æ¦‚ç‡è¶…è¿‡50%æ‰€éœ€çš„Topä¸“å®¶æ•°é‡åˆ†å¸ƒã€‚æ¯ä¸€æ¡æŸ±çŠ¶å›¾ä»£è¡¨Mixtral-8x7Bæ¨¡å‹ä¸­ï¼Œå¯¹åº”MoEå±‚å†…ä¸åŒä¸“å®¶æ•°é‡çš„Tokenå æ¯”ã€‚&lt;/figcaption&gt;
    &lt;/div&gt;
  &lt;/figure&gt;


&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;p&gt;æˆ‘ä»¬ä¸ºæ­¤æä¾›äº†å®éªŒæ€§çš„è¯æ®ã€‚æˆ‘ä»¬åˆ†æäº†Mixtral-8x7Bï¼ˆä¸€ä¸ªé‡‡ç”¨å›ºå®štop-2è·¯ç”±çš„æ¨¡å‹ï¼‰çš„è·¯ç”±æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶å‘ç°äº†ä¸¤ç§å…³é”®æ¨¡å¼ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å¤§é‡Tokençš„è·¯ç”±æ¦‚ç‡é«˜åº¦é›†ä¸­äºå•ä¸ªä¸“å®¶ï¼Œè¿™è¡¨æ˜æ¿€æ´»ç¬¬äºŒä¸ªä¸“å®¶é€šå¸¸æ˜¯å¤šä½™çš„ï¼›&lt;/li&gt;
&lt;li&gt;å¦æœ‰ç›¸å½“ä¸€éƒ¨åˆ†Tokençš„æ¦‚ç‡æ›´å‡åŒ€åœ°åˆ†å¸ƒåœ¨å¤šä¸ªä¸“å®¶ä¸Šï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯èƒ½éœ€è¦ä¸¤ä¸ªç”šè‡³æ›´å¤šä¸“å®¶çš„è®¡ç®—èƒ½åŠ›æ‰èƒ½è¢«æœ‰æ•ˆå¤„ç†ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;è¿™ä¸€å‘ç°æœ‰åŠ›åœ°è¯æ˜ï¼Œå›ºå®šçš„ &lt;code&gt;top-k&lt;/code&gt;ç­–ç•¥æ˜¯æ¬¡ä¼˜çš„ï¼Œå®ƒå¯¹ç®€å•çš„Tokené€ æˆäº†è¿‡åº¦è®¡ç®—ï¼Œè€Œå¯¹å¤æ‚çš„Tokenåˆ™å¯èƒ½è®¡ç®—ä¸è¶³ã€‚&lt;/p&gt;

&lt;/div&gt;



&lt;h2 id=&#34;å€ŸåŠ©ç©ºä¸“å®¶å®ç°è‡ªé€‚åº”è·¯ç”±&#34;&gt;å€ŸåŠ©â€œç©ºä¸“å®¶â€å®ç°è‡ªé€‚åº”è·¯ç”±&lt;/h2&gt;
&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;p&gt;AdaMoE é€šè¿‡å¼•å…¥ &lt;strong&gt;â€œç©ºä¸“å®¶â€&lt;/strong&gt;ï¼ˆnull expertsï¼‰æ¥å®ç°Tokençº§åˆ«çš„è‡ªé€‚åº”ä¸“å®¶é€‰æ‹©ã€‚æˆ‘ä»¬å°†å…¶å®šä¹‰ä¸ºä¸€ç§ä¸æ‰§è¡Œä»»ä½•æ“ä½œçš„å•å…ƒï¼Œå¤„ç†Tokenç‰¹å¾æ‰€éœ€çš„&lt;strong&gt;è®¡ç®—é‡ï¼ˆFLOPsï¼‰ä¸ºé›¶&lt;/strong&gt;ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å®è·µä¸­ï¼Œå¸¸è§çš„é›¶è®¡ç®—æ“ä½œåŒ…æ‹¬å¸¸æ•°é›¶æ˜ å°„å’Œæ’ç­‰æ˜ å°„ï¼ˆä¸ºç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬åœ¨åç»­è®¨è®ºä¸­é»˜è®¤é‡‡ç”¨é›¶æ˜ å°„ä½œä¸ºâ€œç©ºä¸“å®¶â€çš„å®ç°ï¼‰ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘ä»¬çš„æœºåˆ¶è¿è¡Œå¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;åœ¨åŸæœ‰çš„ &lt;code&gt;n&lt;/code&gt;ä¸ªçœŸå®ä¸“å®¶ä¹‹å¤–ï¼Œå°†&lt;strong&gt;ä¸“å®¶é›†åˆæ‰©å±•&lt;/strong&gt; &lt;code&gt;m&lt;/code&gt;ä¸ªâ€œç©ºä¸“å®¶â€ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ç•¥å¾®å¢åŠ &lt;/strong&gt;è·¯ç”±å™¨çš„ &lt;strong&gt;&lt;code&gt;top-k&lt;/code&gt;&lt;/strong&gt; å€¼ï¼ˆä¾‹å¦‚ï¼Œä» 2 å¢åŠ åˆ° 3 æˆ– 4ï¼‰ã€‚è¿™æ ·ï¼Œæ¯ä¸ªTokené€‰å‡ºçš„ &lt;code&gt;top-k&lt;/code&gt;ä¸“å®¶ä¸­å°±å¯èƒ½åŒ…å«ä¸€éƒ¨åˆ†â€œç©ºä¸“å®¶â€ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;å®ç°è®¡ç®—çš„è‡ªé€‚åº”&lt;/strong&gt;ï¼šå¦‚æœ &lt;code&gt;top-k&lt;/code&gt;ä¸“å®¶ä¸­åŒ…å« &lt;code&gt;r&lt;/code&gt;ä¸ªâ€œç©ºä¸“å®¶â€ï¼Œé‚£ä¹ˆè¯¥Tokenå®é™…ä¸Šåªä½¿ç”¨äº† &lt;code&gt;k-r&lt;/code&gt;ä¸ªçœŸå®ä¸“å®¶ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;è¿›è¡Œåˆç†çš„è´Ÿè½½å‡è¡¡&lt;/strong&gt;ï¼šåœ¨è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±æ—¶ï¼Œæˆ‘ä»¬å°†&lt;strong&gt;æ‰€æœ‰â€œç©ºä¸“å®¶â€èšåˆä¸ºå•ä¸€çš„è®¡ç®—å•å…ƒ&lt;/strong&gt;ï¼ˆå› ä¸ºæ²¡æœ‰å¿…è¦åœ¨å®Œå…¨ç­‰ä»·çš„â€œç©ºä¸“å®¶â€ä¹‹é—´å¼ºåˆ¶å®ç°å‡è¡¡ï¼‰ã€‚&lt;/li&gt;
&lt;li&gt;åœ¨ &lt;code&gt;top-k&lt;/code&gt;é€‰æ‹©ä¹‹åï¼Œ&lt;strong&gt;ä»…å¯¹çœŸå®ä¸“å®¶è¿›è¡Œå½’ä¸€åŒ–&lt;/strong&gt;ï¼Œä»¥ç¡®ä¿è¾“å‡ºçš„å°ºåº¦ä¸æ ‡å‡†çš„MoEæ¨¡å‹ä¿æŒä¸€è‡´ã€‚&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;



&lt;style&gt;
  .gray-text {
    color: #808080;
  }
&lt;/style&gt;


  &lt;figure&gt;
	  &lt;div style=&#34;display: grid; place-items: center;&#34;&gt;
      &lt;img src=&#34;image/vs.png&#34; alt=&#34;DeepSeek&#34; style=&#34;width: 85%; height: auto;&#34;&gt;
      &lt;figcaption style=&#34;font-size: 16px;&#34; class=&#34;gray-text&#34;&gt;å›ºå®šçš„ top-2 è·¯ç”±ä¸ AdaMoE å¯¹æ¯”ã€‚å·¦å›¾ï¼šæ ‡å‡†çš„ top-2 è·¯ç”±ï¼Œæ¯ä¸ªTokenç²¾ç¡®æ¿€æ´»2ä¸ªçœŸå®ä¸“å®¶ã€‚å³å›¾ï¼šAdaMoEï¼Œä»4ä¸ªçœŸå®ä¸“å®¶å’Œ5ä¸ªâ€œç©ºä¸“å®¶â€ä¸­é€‰æ‹© top-4ï¼Œä½¿å¾—æŸäº›Tokenæ¿€æ´»äº†3ä¸ªçœŸå®ä¸“å®¶ï¼Œè€Œå¦ä¸€äº›åªæ¿€æ´»äº†1ä¸ªã€‚&lt;/figcaption&gt;
    &lt;/div&gt;
  &lt;/figure&gt;


&lt;h2 id=&#34;ä¸»è¦å®éªŒç»“æœ&#34;&gt;ä¸»è¦å®éªŒç»“æœ&lt;/h2&gt;
&lt;div style=&#34;text-align: left;&#34;&gt;
    åœ¨å¯¹ Mixtralâ€‘8Ã—7B è¿›è¡Œå¾®è°ƒæ—¶ï¼ŒAdaMoE åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ—¢é™ä½äº†FFNå±‚çš„è®¡ç®—é‡ï¼ˆFLOPsï¼‰ï¼Œåˆä¿æŒç”šè‡³æå‡äº†æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ ARCâ€‘Challenge æ•°æ®é›†ä¸Šï¼Œ&lt;strong&gt;æ€»FLOPsä¸‹é™äº†çº¦14.55%ï¼Œè€Œå‡†ç¡®ç‡æå‡äº†1.69%&lt;/strong&gt;ã€‚åŒæ—¶ï¼Œæ¯å±‚æ¯ä¸ªTokenæ¿€æ´»çš„å¹³å‡ä¸“å®¶æ•°ä» 2.0 é™è‡³çº¦ 1.67ã€‚
&lt;/div&gt;



&lt;style&gt;
  .gray-text {
    color: #808080;
  }
&lt;/style&gt;


  &lt;figure&gt;
	  &lt;div style=&#34;display: grid; place-items: center;&#34;&gt;
      &lt;img src=&#34;image/res.png&#34; alt=&#34;venn&#34; style=&#34;width: 80%; height: auto;&#34;&gt;
      &lt;figcaption style=&#34;font-size: 16px;&#34; class=&#34;gray-text&#34;&gt;AdaMoEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œå±•ç¤ºäº†å‡†ç¡®ç‡ï¼ˆAcc.ï¼‰ã€FLOPsé™ä½ç™¾åˆ†æ¯”ï¼ˆ%FLOPsï¼‰å’Œå¹³å‡ä¸“å®¶è´Ÿè½½ï¼ˆLoadï¼‰çš„å¯¹æ¯”ã€‚&lt;/figcaption&gt;
    &lt;/div&gt;
  &lt;/figure&gt;


&lt;h2 id=&#34;ä»è®ºæ–‡èµ°å‘å®è·µ&#34;&gt;ä»è®ºæ–‡èµ°å‘å®è·µ&lt;/h2&gt;
&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;p&gt;æˆ‘ä»¬å¤‡å—é¼“èˆåœ°çœ‹åˆ°ï¼Œâ€œç©ºä¸“å®¶â€è¿™ä¸€æ¦‚å¿µå¹¶éä»…ä»…åœç•™åœ¨ç†è®ºå±‚é¢ï¼Œå®ƒå·²è¢«ä¸šç•Œå‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹æ‰€é‡‡çº³å’Œå®ç°ã€‚&lt;strong&gt;LongCat-Flash&lt;/strong&gt;çš„æŠ€æœ¯æŠ¥å‘Šä¸­ï¼Œå°±å°† &lt;strong&gt;â€œé›¶è®¡ç®—ä¸“å®¶â€&lt;/strong&gt;ï¼ˆzero-computation expertsï¼‰åˆ—ä¸ºä¸€é¡¹å…³é”®çš„æ¶æ„åˆ›æ–°ï¼Œå¹¶å¼•ç”¨äº†æˆ‘ä»¬çš„è®ºæ–‡ã€‚&lt;/p&gt;</description>
    </item>
    <item>
      <title>SIFT: Grounding LLM Reasoning in Contexts via Stickers</title>
      <link>http://localhost:1313/blogs/sift/</link>
      <pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/sift/</guid>
      <description>&lt;div style=&#34;padding-left: 30px; display: flex !important; gap: 60px !important;&#34;&gt;
    
    
    
        &lt;a href=&#34;https://github.com/zhijie-group/SIFT/blob/main/paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
            &lt;img src=&#34;https://img.shields.io/badge/ğŸ“ƒ Paper Link--1abc9c.svg?style=social&#34; style=&#34;transform: scale(1.5);&#34;&gt;
        &lt;/a&gt;
    
    
        &lt;a href=&#34;https://github.com/zhijie-group/SIFT&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
            &lt;img src=&#34;https://img.shields.io/badge/ğŸ’» Code Link--1abc9c.svg?style=social&#34; style=&#34;transform: scale(1.5);&#34;&gt;
        &lt;/a&gt;
    
&lt;/div&gt;

&lt;h2 id=&#34;introduction--motivation&#34;&gt;Introduction &amp;amp; Motivation&lt;/h2&gt;
&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;p&gt;Techniques including Chain-of-Thought (CoT) Prompting [1,2] and Self-Consistency [3], as well as reasoning-enhanced models, e.g., OpenAI-o1, DeepSeek-R1, and KIMI-k1.5 [4-6], have all contributed to improvements in multi-step reasoning for solving hard problems.&lt;/p&gt;
&lt;p&gt;Discussions in the community suggest that advanced reasoning capabilities in LLMs mainly stem from two factors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;foundational knowledge acquisition through massive pretraining on diverse data;&lt;/li&gt;
&lt;li&gt;strategic refinement via post-training interventions like supervised fine-tuning (SFT) or reinforcement learning (RL).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In recent weeks, much attention has been focused on reproducing R1, i.e., improving the reasoning abilities of LLMs of varying scales. However, our investigations reveal a critical lacuna of this paradigm: we observe that diverse models, from the small Llama3.2-3B-Instruct to the state-of-the-art  DeepSeek-R1, suffer from systematical &lt;strong&gt;misinterpretation of the original problem&lt;/strong&gt; during the reasoning process. The errors in foundational comprehension, rather than faulty logical sequencing, constitute a substantial proportion of inaccuracies in final outcomes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Consistency Large Language Models: A Family of Efficient Parallel Decoders</title>
      <link>http://localhost:1313/blogs/cllm/</link>
      <pubDate>Mon, 06 May 2024 12:00:00 -0800</pubDate>
      <guid>http://localhost:1313/blogs/cllm/</guid>
      <description>&lt;div style=&#34;padding-left: 30px; display: flex !important; gap: 60px !important;&#34;&gt;
    
        &lt;a href=&#34;https://arxiv.org/abs/2403.00835&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
          &lt;img src=&#34;https://img.shields.io/badge/arXiv-2403.00835-white.svg?style=social&#34; style=&#34;transform: scale(1.5);&#34;&gt;
        &lt;/a&gt;
    
    
        &lt;a href=&#34;https://github.com/hao-ai-lab/Consistency_LLM&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
          &lt;img src=&#34;https://img.shields.io/github/stars/hao-ai-lab/Consistency_LLM?style=social&#34; style=&#34;transform: scale(1.5);&#34;&gt;
        &lt;/a&gt;
    
    
    
&lt;/div&gt;

&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;strong&gt;TL;DR:&lt;/strong&gt; LLMs have been traditionally regarded as sequential decoders, decoding one token after another. In this blog, we show pretrained LLMs can be easily taught to operate as efficient parallel decoders. We introduce &lt;strong&gt;Consistency Large Language Models (CLLMs)&lt;/strong&gt;, a new family of parallel decoders capable of reducing inference latency by efficiently decoding an $n$-token sequence per inference step. Our research shows this process &amp;ndash; mimicking human cognitive process of forming complete sentences in mind before articulating word by word &amp;ndash; can be effectively learned by simply finetuning pretrained LLMs. Specifically, CLLMs are trained to perform parallel decoding by mapping any randomly initialized $n$-token sequence to the same result yielded by autoregressive (AR) decoding in as few steps as possible. Experiment results show CLLMs obtained using our proposed method are highly effective, showing $2.4\times$ to $3.4\times$ improvements in generation speed, in par with or even beter than other fast inference techniques like Medusa2 and Eagle, yet require no additional memory cost to accomodate auxiliary model components at inference time.
&lt;/div&gt;



&lt;style&gt;
  .gray-text {
    color: #808080;
  }
&lt;/style&gt;


  &lt;figure&gt;
	  &lt;div style=&#34;display: grid; place-items: center;&#34;&gt;
      &lt;img src=&#34;img/baseline_vs_cllm_gsm8k_best_acc_demo.gif&#34; alt=&#34;cllm-gsm8k-acc-demo&#34; style=&#34;width: 120%; height: auto;&#34;&gt;
      &lt;figcaption style=&#34;font-size: 16px;&#34; class=&#34;gray-text&#34;&gt;Figure 1: Demo of $\sim 3 \times$ speedup by CLLM-ABEL-7B-001 in comparison with baseline &lt;a href=&#34;https://github.com/GAIR-NLP/abel&#34;&gt;ABEL-7B-001&lt;/a&gt; using Jacobi decoding on GSM8K.&lt;/figcaption&gt;
    &lt;/div&gt;
  &lt;/figure&gt;


&lt;h2 id=&#34;background-jacobi-decoding&#34;&gt;Background: Jacobi Decoding&lt;/h2&gt;
&lt;div style=&#34;text-align: left;&#34;&gt;
    Large language models (LLMs) are transforming the landscape of human lives, from programming to offering legal and health advice. However, during inference, LLMs generate responses token by token using AR decoding as shown in Figure 1, leading to high latency for longer responses. Using AR decoding, it often necessitates architectural modifications, auxiliary components, or draft models, to speed up inference by generating more than one token at a time.
&lt;/div&gt;



&lt;style&gt;
  .gray-text {
    color: #808080;
  }
&lt;/style&gt;


  &lt;figure&gt;
	  &lt;div style=&#34;display: grid; place-items: center;&#34;&gt;
      &lt;img src=&#34;img/clm_objective.png&#34; alt=&#34;autoregressive&#34; style=&#34;width: 60%; height: auto;&#34;&gt;
      &lt;figcaption style=&#34;font-size: 16px;&#34; class=&#34;gray-text&#34;&gt;Figure 2: illustration of conventional AR decoding: one token is generated at a time.&lt;/figcaption&gt;
    &lt;/div&gt;
  &lt;/figure&gt;


&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;a href=&#34;https://arxiv.org/abs/2305.10427&#34;&gt;Jacobi decoding&lt;/a&gt; originates from the Jacobi and Gauss-Seidel fixed-point iteration for solving nonlinear equations, and &lt;a href=&#34;https://proceedings.mlr.press/v139/song21a.html&#34;&gt;is proven identical to AR generation using greedy decoding&lt;/a&gt;. Jacobi decoding reformulates the sequential generation process into a system of $n$ non-linear equations with $n$ variables solvable in parallel based on Jacobi iteration. Each iteration step might predict more than one correct token (By &amp;ldquo;correct&amp;rdquo;, we mean alignment with the AR decoding
result under a greedy sampling strategy), thereby accelerating AR decoding potentially.
&lt;/div&gt;



&lt;style&gt;
  .gray-text {
    color: #808080;
  }
&lt;/style&gt;


  &lt;figure&gt;
	  &lt;div style=&#34;display: grid; place-items: center;&#34;&gt;
      &lt;img src=&#34;img/jacobi_objective.png&#34; alt=&#34;jacobi&#34; style=&#34;width: 60%; height: auto;&#34;&gt;
      &lt;figcaption style=&#34;font-size: 16px;&#34; class=&#34;gray-text&#34;&gt;Figure 3: illustration of Jacobi decoding: $n$-token sequence is fed into the LLM and iterates until convergence.&lt;/figcaption&gt;
    &lt;/div&gt;
  &lt;/figure&gt;


&lt;div style=&#34;text-align: left;&#34;&gt;
    To be specific, Jacobi decoding method first randomly guesses the next $n$ tokens in a sequence (referred to as $n$-token sequence hereinafter unless specified otherwise) from an input prompt. The $n$-token sequence, along with the prompt, is then fed to the LLM to iteratively update itself. This process continues until the $n$-token sequence stabilizes and no further changes occur, reaching a fixed point. Notably, Jacobi decoding requires no more queries to the LLM than auto-regressive (AR) decoding. Eventually, the $n$-token sequence converges to the output that would be generated by AR decoding under a greedy strategy. This progression from an initial random guess to the final AR generation outcome traces what is known as a Jacobi trajectory. An instance of Jacobi decoding iteration process and &lt;strong&gt;Jacobi trajectory&lt;/strong&gt; is illustrated in Figure 2.
&lt;/div&gt;



&lt;h3 id=&#34;limitations-of-jacobi-decoding&#34;&gt;Limitations of Jacobi Decoding&lt;/h3&gt;
&lt;div style=&#34;text-align: left;&#34;&gt;
    &lt;p&gt;However, vanilla Jacobi decoding for LLMs shows only marginal speedup over AR decoding in practice, e.g., &lt;a href=&#34;https://arxiv.org/abs/2305.10427&#34;&gt;an average of $1.05\times$ speedup&lt;/a&gt;. This is because an AR-trained LLM can rarely yield a correct token when there are incorrections in its preceding tokens. Thereby, most Jacobi iterations gain only one correction for the $n$-token sequence, resulting in a longer trajectory as illustrated on the left side of Figure 3.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/contact/</guid>
      <description>contact</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/gpu-stats/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/gpu-stats/</guid>
      <description>GPU statistics and usage information</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/home/</guid>
      <description>home page for DENG Lab @ SJTU</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/math-examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/math-examples/</guid>
      <description>&lt;p&gt;This is an inline (a^&lt;em&gt;=x-b^&lt;/em&gt;) equation.&lt;/p&gt;
&lt;p&gt;This is an inline $a^*=x-b^*$ equation.&lt;/p&gt;
&lt;p&gt;These are block equations:&lt;/p&gt;
\[a^*=x-b^*\]\[ a^*=x-b^* \]\[
a^*=x-b^*
\]&lt;p&gt;These are block equations using alternate delimiters:&lt;/p&gt;
$$a^*=x-b^*$$$$ a^*=x-b^* $$$$
a^*=x-b^*
$$</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/people/</guid>
      <description>people</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/</guid>
      <description>publications</description>
    </item>
  </channel>
</rss>
